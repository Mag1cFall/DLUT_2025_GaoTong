并非所有公式都需要死记硬背，但理解其构成、含义和应用场景至关重要。不同公式的重要性级别不同，可分为三类：
1.  **核心必背类**：定义了模型基础、评估标准或核心思想的公式。例如线性回归模型、RSS、MSE、R²、偏差-方差分解、岭回归/Lasso的目标函数等。这些是理解和交流的基础，必须熟练掌握。
2.  **理解概念类**：公式本身复杂，但其背后概念极其重要。例如贝叶斯公式、光滑样条的惩罚项、LDA/QDA的判别函数等。重点是理解公式中各部分的作用（如先验、似然、惩罚项），而非精确默写。
3.  **了解即可类**：主要是计算推导或软件自动完成的公式。例如简单线性回归系数的解析解、LOOCV的计算捷径等。知道它们的存在和用途即可。

### 一、公式整理

#### 第二章 统计学习

*   **基本模型**
    $$
    Y = f(X) + \epsilon
    $$
    *   $Y$: 因变量或响应变量。
    *   $X$: 自变量、预测变量或特征。
    *   $f(X)$: $X$ 对 $Y$ 的系统性信息，是需要估计的未知函数。
    *   $\epsilon$: 随机误差项，与 $X$ 独立，均值为0。

*   **均方误差 (Mean Squared Error, MSE)**
    $$
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2
    $$
    *   $n$: 样本数量。
    *   $y_i$: 第 $i$ 个观测的真实响应值。
    *   $\hat{f}(x_i)$: 模型对第 $i$ 个观测的预测值。

*   **期望测试MSE的分解 (偏差-方差权衡)**
    $$
    E(y_0 - \hat{f}(x_0))^2 = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\epsilon)
    $$
    *   $E(\cdot)$: 期望值。
    *   $y_0, x_0$: 一个新的、未见过的测试数据点。
    *   $\text{Var}(\hat{f}(x_0))$: 模型预测值的**方差**，衡量模型在不同训练集上预测结果的波动性。
    *   $[\text{Bias}(\hat{f}(x_0))]^2$: 模型预测值偏差的**平方**，衡量模型预测的平均值与真实值之间的差距。
    *   $\text{Var}(\epsilon)$: **不可约误差**的方差，由数据自身噪声决定。

*   **分类错误率 (Error Rate)**
    $$
    \text{Error Rate} = \frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)
    $$
    *   $I(\cdot)$: 指示函数，当条件为真时取1，否则取0。
    *   $y_i$: 第 $i$ 个观测的真实类别。
    *   $\hat{y}_i$: 模型对第 $i$ 个观测的预测类别。

---

#### 第三章 线性回归

*   **简单线性回归模型 (SLR)**
    $$
    Y = \beta_0 + \beta_1 X + \epsilon
    $$
    *   $\beta_0$: 截距项 (Intercept)。
    *   $\beta_1$: 斜率项 (Slope)。

*   **残差平方和 (Residual Sum of Squares, RSS)**
    $$
    \text{RSS} = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2
    $$
    *   $y_i$: 第 $i$ 个观测的真实值。
    *   $\hat{y}_i$: 第 $i$ 个观测的预测值。
    *   $\hat{\beta}_0, \hat{\beta}_1$: 最小二乘法估计出的系数。

*   **SLR系数的最小二乘估计**
    $$
    \hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
    $$
    $$
    \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
    $$
    *   $\bar{x}, \bar{y}$: 预测变量 $X$ 和响应变量 $Y$ 的样本均值。

*   **系数估计的标准误差 (SE)**
    $$
    \text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum(x_i - \bar{x})^2} \quad , \quad \text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum(x_i - \bar{x})^2} \right]
    $$
    *   $\sigma^2$: 误差项 $\epsilon$ 的方差，通常用 RSE² 来估计。

*   **t-统计量**
    $$
    t = \frac{\hat{\beta}_j - 0}{\text{SE}(\hat{\beta}_j)}
    $$
    *   $\hat{\beta}_j$: 第 $j$ 个系数的估计值。
    *   $\text{SE}(\hat{\beta}_j)$: 该系数估计值的标准误差。

*   **残差标准误 (Residual Standard Error, RSE)**
    $$
    \text{RSE} = \sqrt{\frac{1}{n-p-1} \text{RSS}}
    $$
    *   $n$: 样本数量。
    *   $p$: 预测变量的数量。

*   **R² 统计量**
    $$
    R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
    $$
    *   RSS: 残差平方和。
    *   TSS: 总平方和 (Total Sum of Squares)，代表响应变量 $Y$ 的总变异。

*   **F-统计量**
    $$
    F = \frac{(\text{TSS} - \text{RSS})/p}{\text{RSS}/(n-p-1)}
    $$
    *   TSS: 总平方和。
    *   RSS: 残差平方和。
    *   $p$: 预测变量的数量。
    *   $n$: 样本数量。

*   **方差膨胀因子 (Variance Inflation Factor, VIF)**
    $$
    \text{VIF}(\hat{\beta}_j) = \frac{1}{1 - R^2_{X_j|X_{-j}}}
    $$
    *   $R^2_{X_j|X_{-j}}$: 将预测变量 $X_j$ 作为响应变量，其他所有预测变量作为自变量进行回归得到的 R² 值。

---

#### 第四章 分类

*   **逻辑斯蒂函数 (Logistic Function)**
    $$
    P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}
    $$
    *   $P(Y=1|X)$: 给定预测变量 $X$ 时，事件发生（类别为1）的概率。

*   **对数发生比 (Log-odds or Logit)**
    $$
    \log\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
    $$
    *   $P$: 事件发生的概率，即 $P(Y=1|X)$。
    *   $P/(1-P)$: 发生比 (Odds)。

*   **贝叶斯定理 (用于分类)**
    $$
    p_k(x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K}\pi_l f_l(x)}
    $$
    *   $p_k(x)$: **后验概率** (Posterior Probability) - 给定观测值 $X=x$，其属于类别 $k$ 的概率。
    *   $\pi_k$: **先验概率** (Prior Probability) - 一个随机样本属于类别 $k$ 的概率。
    *   $f_k(x)$: **类条件密度** (Class-conditional Density) - 在已知样本属于类别 $k$ 的前提下，观测到 $X=x$ 的概率密度。

*   **线性判别分析 (LDA) 的判别函数**
    $$
    \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log(\pi_k)
    $$
    *   $\delta_k(x)$: 类别 $k$ 的判别得分。
    *   $x$: 观测向量。
    *   $\mu_k$: 类别 $k$ 的均值向量。
    *   $\Sigma$: 所有类别共享的协方差矩阵。
    *   $\pi_k$: 类别 $k$ 的先验概率。

*   **二次判别分析 (QDA) 的判别函数**
    $$
    \delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) - \frac{1}{2}\log|\Sigma_k| + \log(\pi_k)
    $$
    *   $\Sigma_k$: 类别 $k$ 各自的协方差矩阵。
    *   $|\Sigma_k|$: 协方差矩阵 $\Sigma_k$ 的行列式。

---

#### 第五章 重抽样方法

*   **留一交叉验证 (LOOCV) 误差**
    $$
    CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_i
    $$
    *   $n$: 样本总数。
    *   $\text{MSE}_i$: 在移除第 $i$ 个点后训练的模型，对第 $i$ 个点进行预测所产生的均方误差。

*   **K-折交叉验证 (K-Fold CV) 误差**
    $$
    CV_{(k)} = \frac{1}{k} \sum_{j=1}^{k} \text{MSE}_j
    $$
    *   $k$: 折数。
    *   $\text{MSE}_j$: 以第 $j$ 折作为验证集时计算出的均方误差。

---

#### 第六章 线性模型选择与正则化

*   **Cp / AIC (赤池信息量准则)**
    $$
    C_p = \frac{1}{n}(\text{RSS} + 2d\hat{\sigma}^2)
    $$
    *   $n$: 样本数量。
    *   RSS: 包含 $d$ 个变量的模型的残差平方和。
    *   $d$: 模型中预测变量的数量。
    *   $\hat{\sigma}^2$: 全模型（包含所有变量）的误差方差估计值。

*   **BIC (贝叶斯信息准则)**
    $$
    \text{BIC} = \frac{1}{n}(\text{RSS} + \log(n)d\hat{\sigma}^2)
    $$
    *   $\log(n)$: 样本量 $n$ 的自然对数。

*   **调整 R² (Adjusted R²)**
    $$
    \text{Adjusted } R^2 = 1 - \frac{\text{RSS}/(n-d-1)}{\text{TSS}/(n-1)}
    $$
    *   $d$: 模型中预测变量的数量。

*   **岭回归 (Ridge Regression) 目标函数**
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
    $$
    *   $\lambda$: 调节参数，控制惩罚强度。
    *   $\sum \beta_j^2$: L2惩罚项。

*   **Lasso 目标函数**
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
    $$
    *   $\sum |\beta_j|$: L1惩罚项。

---

#### 第七章 非线性模型

*   **多项式回归 (Polynomial Regression)**
    $$
    y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_d x_i^d + \epsilon_i
    $$
    *   $d$: 多项式的阶数。

*   **光滑样条 (Smoothing Spline) 目标函数**
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - g(x_i))^2 + \lambda \int (g''(t))^2 dt \right\}
    $$
    *   $g(x)$: 待寻找的光滑函数。
    *   $\lambda$: 调节参数，控制光滑度。
    *   $\int (g''(t))^2 dt$: 粗糙度惩罚项，衡量函数的总弯曲度。

*   **广义可加模型 (Generalized Additive Model, GAM)**
    $$
    y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i
    $$
    *   $f_j(\cdot)$: 对第 $j$ 个预测变量的非线性平滑函数。

---

#### 第八章 基于树的方法

*   **分类树纯度度量：基尼指数 (Gini Index)**
    $$
    G = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})
    $$
    *   $K$: 类别总数。
    *   $\hat{p}_{mk}$: 第 $m$ 个节点中，属于类别 $k$ 的训练样本所占的比例。

*   **分类树纯度度量：交叉熵 (Cross-Entropy)**
    $$
    D = -\sum_{k=1}^{K} \hat{p}_{mk} \log(\hat{p}_{mk})
    $$
    *   $\log$: 通常指自然对数。

*   **代价复杂性剪枝 (Cost Complexity Pruning)**
    $$
    \sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|
    $$
    *   $T$: 某个子树。
    *   $|T|$: 子树 $T$ 的终端节点数量。
    *   $R_m$: 第 $m$ 个终端节点对应的区域。
    *   $\alpha$: 调节参数，惩罚树的复杂度。

---

### **第九章 支持向量机 (SVM)**

#### **9.1 核心概念**

*   **最大间隔分类器**:
    *   **适用场景**: 数据线性可分。
    *   **思想**: 找到一个能将数据完美分开，且离两侧数据点**间隔 (margin)** 最大的超平面。
    *   **支持向量**: 位于间隔边界上的点，它们决定了超平面的位置。
*   **支持向量分类器 (SVC, 软间隔)**:
    *   **适用场景**: 数据近似线性可分。
    *   **思想**: 允许一些点违反间隔（进入间隔或被错分），以换取更宽、更鲁棒的间隔。
    *   **成本参数 C**: 控制对违反间隔的容忍度，是偏差-方差权衡的关键。**小C**对应宽间隔、高偏差、低方差；**大C**对应窄间隔、低偏差、高方差。
*   **支持向量机 (SVM)**:
    *   **适用场景**: 非线性决策边界。
    *   **思想**: 利用**核技巧 (Kernel Trick)**，将数据隐式地映射到高维空间，使其线性可分，然后在这个高维空间应用支持向量分类器。
    *   **常用核函数**:
        *   **多项式核**: 适用于多项式边界。
        *   **径向基函数 (RBF) 核**: 适用范围广，能处理复杂的非线性边界。其参数 $\gamma$ 控制单个样本的影响范围，也是偏差-方差权衡的关键。

#### **本章核心公式**

*   **支持向量分类器优化问题 (软间隔)**
    $$ \underset{\beta_0, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n, M}{\text{maximize}} \quad M $$
    **约束条件**:
    1.  $\sum_{j=1}^p \beta_j^2 = 1$
    2.  $y_i(\beta_0 + \dots + \beta_p x_{ip}) \ge M(1 - \epsilon_i)$
    3.  $\epsilon_i \ge 0, \quad \sum_{i=1}^n \epsilon_i \le C$
    *   $M$: 间隔宽度。
    *   $\epsilon_i$: 松弛变量，表示第 $i$ 个点对间隔的违反程度。
    *   $C$: 成本参数，控制对违反间隔的总容忍度。

*   **多项式核 (Polynomial Kernel)**
    $$ K(x_i, x_{i'}) = (1 + \sum_{j=1}^p x_{ij}x_{i'j})^d $$
    *   $d$: 多项式的次数。

*   **径向基函数 (RBF) 核**
    $$ K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2) = \exp(-\gamma \|x_i - x_{i'}\|^2) $$
    *   $\gamma$: 调节参数，控制核的宽度或单个样本的影响范围。

---

### **第十章 无监督学习**

#### **10.1 核心概念**

*   **主成分分析 (PCA)**:
    *   **目标**: **降维**。找到一个新的坐标系（主成分），使得数据在新坐标系下的方差得到最大程度的保留。
    *   **主成分**: 原始变量的线性组合，且彼此正交。第一主成分是方差最大的方向。
    *   **关键实践**: **进行 PCA 前必须对变量进行标准化**。
    *   **选择主成分数量**: 常通过观察**碎石图 (Scree Plot)** 的“肘点”来决定。
*   **聚类分析**:
    *   **目标**: 将数据划分为若干个子群（簇），使得簇内相似、簇间相异。
    *   **K-均值聚类 (K-Means)**:
        *   **算法**: 迭代地进行“分配”和“更新”两个步骤，直到簇分配不再变化。
        *   **特点**: 需要预先指定簇数 K，且对初始中心敏感。
    *   **层次聚类 (Hierarchical Clustering)**:
        *   **算法**: 从每个点自成一簇开始，逐步合并最相似的簇，直到所有点合并为一个簇。
        *   **可视化**: 结果可用**谱系图 (Dendrogram)** 展示。
        *   **连接方法 (Linkage)**: 定义簇间距离的方式，如最长距离法 (Complete)、最短距离法 (Single)、类平均法 (Average)等。

#### **本章核心公式**

*   **主成分 (Principal Component)**
    $$ Z_m = \sum_{j=1}^{p} \phi_{jm} X_j $$
    *   $Z_m$: 第 m 个主成分的得分。
    *   $X_j$: 第 j 个原始变量。
    *   $\phi_{jm}$: 第 j 个变量在第 m 个主成分上的载荷 (loading)。

*   **方差解释比率 (Proportion of Variance Explained, PVE)**
    $$ \text{PVE}_m = \frac{\sum_{i=1}^{n} (\sum_{j=1}^{p} \phi_{jm} x_{ij})^2}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^2} $$
    *   分子是第 m 个主成分的方差，分母是原始数据的总方差。

*   **K-均值聚类 (K-Means) 目标函数**
    $$ \underset{C_1, \dots, C_K}{\text{minimize}} \left\{ \sum_{k=1}^{K} W(C_k) \right\} = \underset{C_1, \dots, C_K}{\text{minimize}} \left\{ \sum_{k=1}^{K} \sum_{i \in C_k} \|x_i - \mu_k\|^2 \right\} $$
    *   $C_k$: 第 k 个簇的样本集合。
    *   $W(C_k)$: 第 k 个簇的簇内变异（通常是簇内平方和）。
    *   $\mu_k$: 第 k 个簇的中心点。

***

### 專有名詞對照總表

| 中文 | 簡稱 | 翻译 | 英文全稱 |
| :--- | :--- | :--- | :--- |
| R平方 | R² | R² 統計量 | R-squared Statistic |
| F統計量 | F-stat | F 統計量 | F-statistic |
| K-均值聚類 | K-Means | K-均值分群 | K-Means Clustering |
| K-最近鄰 | KNN | K-最近鄰 | K-Nearest Neighbors |
| 人工神經網絡 | ANN | 人工神經網絡 | Artificial Neural Network |
| 代价复杂性剪枝 | - | 代價複雜度剪枝 | Cost Complexity Pruning |
| 偏差 | - | 偏差 | Bias |
| 偏差-方差权衡 | - | 偏差-方差權衡 | Bias-Variance Trade-Off |
| 标准误差 | SE | 標準誤 | Standard Error |
| 参数方法 | - | 參數方法 | Parametric Methods |
| 残差平方和 | RSS | 殘差平方和 | Residual Sum of Squares |
| 残差标准误 | RSE | 殘差標準誤 | Residual Standard Error |
| 成本参数 (SVM) | C | 成本參數 | Cost Parameter (SVM) |
| 惩罚因子 | - | 懲罰因子 | Penalty Factor |
| 抽样 | - | 抽樣 | Sampling |
| 交叉熵 | - | 交叉熵 | Cross-Entropy |
| 交叉验证 | CV | 交叉驗證 | Cross-Validation |
| 次数 (多项式) | d | 次數 (多項式) | Degree (Polynomial) |
| 刀切法 | - | 刀切法 | Jackknife |
| 倒置现象 | - | 倒置現象 | Inversion (in Clustering) |
| 迭代 | - | 迭代 | Iteration |
| 调节参数 | - | 調節參數 | Tuning Parameter / Hyperparameter |
| 定性变量 | - | 定性變數 | Qualitative Variable / Categorical Variable |
| 多分类 | - | 多分類 | Multi-class Classification |
| 多项式回归 | - | 多項式迴歸 | Polynomial Regression |
| 多元线性回归 | MLR | 多元線性迴歸 | Multiple Linear Regression |
| 多元正态分布 | - | 多元常態分佈 | Multivariate Normal Distribution |
| 多重共线性 | - | 多重共線性 | Multicollinearity |
| 发生比 | Odds | 發生比 | Odds |
| 方差 | - | 變異數/方差 | Variance |
| 方差膨胀因子 | VIF | 變異數膨脹因子 | Variance Inflation Factor |
| 非参数方法 | - | 非參數方法 | Non-parametric Methods |
| 非线性 | - | 非線性 | Non-linear |
| 分类 | - | 分類 | Classification |
| 分类错误率 | - | 分類錯誤率 | Classification Error Rate |
| 分位数 | - | 分位數 | Quantile |
| 杠杆值 | - | 槓桿值 | Leverage |
| 高杠杆点 | - | 高槓桿點 | High Leverage Point |
| 高斯核 | - | 高斯核 | Gaussian Kernel |
| 高维 | - | 高維 | High-dimensional |
| 根节点 | - | 根節點 | Root Node |
| 共线性 | - | 共線性 | Collinearity |
| 广义可加模型 | GAM | 廣義可加模型 | Generalized Additive Model |
| 回归 | - | 迴歸 | Regression |
| 回归样条 | - | 迴歸樣條 | Regression Spline |
| 回归树 | - | 迴歸樹 | Regression Tree |
| 混淆矩阵 | - | 混淆矩陣 | Confusion Matrix |
| 激活函数 | - | 激活函數 | Activation Function |
| 基尼指数 | - | 基尼指數 | Gini Index |
| 基准水平 | - | 基準水平 | Baseline Level / Reference Level |
| 极大似然估计 | MLE | 極大似然估計 | Maximum Likelihood Estimation |
| 集成方法 | - | 集成方法 | Ensemble Method |
| 计算复杂度 | - | 計算複雜度 | Computational Complexity |
| 假设检验 | - | 假設檢定 | Hypothesis Testing |
| 假阳性率 | FPR | 假陽性率 | False Positive Rate |
| 简单线性回归 | SLR | 簡單線性迴歸 | Simple Linear Regression |
| 剪枝 | - | 剪枝 | Pruning |
| 交互作用 | - | 交互作用 | Interaction |
| 阶梯函数 | - | 階梯函數 | Step Function |
| 结点 (样条) | - | 節點 (樣條) | Knot (Spline) |
| 解释性 | - | 解釋性 | Interpretability |
| 径向基函数核 | RBF Kernel | 徑向基函數核 | Radial Basis Function (RBF) Kernel |
| 聚类分析 | - | 分群分析 | Cluster Analysis |
| 决策边界 | - | 決策邊界 | Decision Boundary |
| 决策树 | - | 決策樹 | Decision Tree |
| 均方误差 | MSE | 均方誤差 | Mean Squared Error |
| 可加性 | - | 可加性 | Additivity |
| 可解释性 | - | 可解釋性 | Interpretability |
| 可约误差 | - | 可約誤差 | Reducible Error |
| 拉索回归 | Lasso | 套索迴歸 | Lasso (Least Absolute Shrinkage and Selection Operator) |
| 类别不平衡 | - | 類別不平衡 | Class Imbalance |
| 类条件密度 | - | 類條件密度 | Class-conditional Density |
| 离群点 | - | 離群點 | Outlier |
| 灵敏度 | - | 敏感度 | Sensitivity |
| 岭回归 | - | 嶺迴歸 | Ridge Regression |
| 留一交叉验证 | LOOCV | 留一交叉驗證 | Leave-One-Out Cross-Validation |
| 逻辑斯蒂回归 | - | 羅吉斯迴歸 | Logistic Regression |
| 逻辑函数 | - | 羅吉斯函數 | Logistic Function / Sigmoid Function |
| 模型评估 | - | 模型評估 | Model Assessment |
| 模型选择 | - | 模型選擇 | Model Selection |
| 判别分析 | - | 判別分析 | Discriminant Analysis |
| 判别函数 | - | 判別函數 | Discriminant Function |
| 偏最小二乘 | PLS | 偏最小二乘 | Partial Least Squares |
| 谱系图 | - | 譜系圖/樹狀圖 | Dendrogram |
| 期望最大化 | EM | 期望最大化 | Expectation-Maximization |
| 奇异值分解 | SVD | 奇異值分解 | Singular Value Decomposition |
| 前向逐步选择 | - | 前向逐步選擇 | Forward Stepwise Selection |
| 欠拟合 | - | 欠擬合 | Underfitting |
| 权衡 | - | 權衡 | Trade-off |
| 权重 | - | 權重 | Weight |
| 权重衰减 | - | 權重衰減 | Weight Decay |
| 热图 | - | 熱圖 | Heatmap |
| 软间隔 | - | 軟間隔 | Soft Margin |
| 弱学习器 | - | 弱學習器 | Weak Learner |
| 散点图 | - | 散點圖 | Scatter Plot |
| 神经网络 | NN | 神經網絡 | Neural Network |
| 生成模型 | - | 生成模型 | Generative Model |
| 时间序列 | - | 時間序列 | Time Series |
| 收敛 | - | 收斂 | Convergence |
| 树桩 | - | 樹樁 | Stump (Decision Stump) |
| 数据框 | - | 數據框 | Data Frame |
| 数据挖掘 | - | 數據挖掘 | Data Mining |
| 随机森林 | RF | 隨機森林 | Random Forest |
| 随机梯度下降 | SGD | 隨機梯度下降 | Stochastic Gradient Descent |
| 损失函数 | - | 損失函數 | Loss Function |
| 特征 | - | 特徵 | Feature |
| 特征工程 | - | 特徵工程 | Feature Engineering |
| 特征选择 | - | 特徵選擇 | Feature Selection |
| 特征缩放 | - | 特徵縮放 | Feature Scaling |
| 特征值 | - | 特徵值 | Eigenvalue |
| 特征向量 | - | 特徵向量 | Eigenvector |
| 特异度 | - | 特異度 | Specificity |
| 提升法 | Boosting | 提升法 | Boosting |
| 梯度 | - | 梯度 | Gradient |
| 梯度下降 | - | 梯度下降 | Gradient Descent |
| 统计学习 | - | 統計學習 | Statistical Learning |
| 投影 | - | 投影 | Projection |
| 推断 | - | 推論 | Inference |
| 拖尾 | - | 拖尾 | Tailing (in distributions) |
| 维度 | - | 維度 | Dimension |
| 维度灾难 | - | 維度災難 | Curse of Dimensionality |
| 网格搜索 | - | 網格搜索 | Grid Search |
| 无监督学习 | - | 無監督學習 | Unsupervised Learning |
| 误差 | - | 誤差 | Error |
| 误差项 | - | 誤差項 | Error Term |
| 系数 | - | 係數 | Coefficient |
| 系统聚类 | - | 系統聚類/階層式分群 | Hierarchical Clustering |
| 稀疏 | - | 稀疏 | Sparse |
| 响应变量 | - | 響應變數 | Response Variable |
| 向后逐步选择 | - | 向後逐步選擇 | Backward Stepwise Selection |
| 向量 | - | 向量 | Vector |
| 线性 | - | 線性 | Linear |
| 线性代数 | - | 線性代數 | Linear Algebra |
| 线性判别分析 | LDA | 線性判別分析 | Linear Discriminant Analysis |
| 协方差 | - | 共變異數/協方差 | Covariance |
| 协方差矩阵 | - | 共變異數矩陣 | Covariance Matrix |
| 斜率 | - | 斜率 | Slope |
| 信息增益 | - | 資訊增益 | Information Gain |
| 训练集 | - | 訓練集 | Training Set |
| 哑变量 | - | 虛擬變數 | Dummy Variable |
| 验证集 | - | 驗證集 | Validation Set |
| 样本 | - | 樣本 | Sample |
| 样条 | - | 樣條 | Spline |
| 异方差性 | - | 異質變異 | Heteroscedasticity |
| 因子 | - | 因子 | Factor |
| 因子分析 | - | 因子分析 | Factor Analysis |
| 硬间隔 | - | 硬間隔 | Hard Margin |
| 有监督学习 | - | 監督式學習 | Supervised Learning |
| 预测 | - | 預測 | Prediction |
| 预测变量 | - | 預測變數 | Predictor Variable |
| 预测区间 | - | 預測區間 | Prediction Interval |
| 阈值 | - | 閾值 | Threshold |
| 原假设 | - | 虛無假設 | Null Hypothesis |
| 约束 | - | 約束 | Constraint |
| 召回率 | - | 召回率 | Recall |
| 折 | - | 折 | Fold (in CV) |
| 真阳性率 | TPR | 真陽性率 | True Positive Rate |
| 正则化 | - | 正規化/正則化 | Regularization |
| 正态分布 | - | 常態分佈 | Normal Distribution / Gaussian Distribution |
| 支持向量 | SV | 支持向量 | Support Vector |
| 支持向量机 | SVM | 支持向量機 | Support Vector Machine |
| 置信区间 | CI | 信賴區間 | Confidence Interval |
| 主成分 | PC | 主成分 | Principal Component |
| 主成分分析 | PCA | 主成分分析 | Principal Component Analysis |
| 主成分回归 | PCR | 主成分迴歸 | Principal Component Regression |
| 准确率 | - | 準確率 | Accuracy |
| 自举法/自助法 | - | 自助法 | Bootstrap |
| 自相关 | - | 自我相關 | Autocorrelation |
| 自变量 | - | 自變數 | Independent Variable |
| 最小二乘法 | - | 最小二乘法 | Least Squares |
| 最优子集选择 | - | 最優子集選擇 | Best Subset Selection |
| 最大间隔分类器 | - | 最大間隔分類器 | Maximal Margin Classifier |