## 第二章 统计学习

### 2.1 核心概念

#### 2.1.1 基本模型

统计学习旨在理解自变量 **X** (预测变量/特征) 与因变量 **Y** (响应变量) 之间的关系。该关系可表示为：
$$
Y = f(X) + \epsilon
$$

-   **f(X)**: X 对 Y 的系统性信息，是我们要估计的核心函数。
-   **ε**: 随机误差项，与X独立，均值为0，代表了模型无法解释的、不可约的误差。

#### 2.1.2 统计学习的目标

1.  **预测 (Prediction)**:
    *   目标：准确预测未来或未知观测的 Y 值。
    *   关注点：预测结果的准确性，可以将 f 视为一个“黑箱”。
    *   误差来源：
        *   **可约误差 (Reducible Error)**: 通过选择更好的统计学习方法可以减小的误差。
        *   **不可约误差 (Irreducible Error)**: 由 ε 引入的误差，是任何模型都无法消除的误差下限。

2.  **推断 (Inference)**:
    *   目标：理解 X 的变化如何影响 Y。
    *   关注点：f 的具体形式，不能是“黑箱”。需要回答如“哪些预测变量与响应变量相关？”或“某个预测变量与响应变量的关系是正还是负？”等问题。

#### 2.1.3 估计 f 的方法

1.  **参数方法 (Parametric Methods)**
    *   **步骤**:
        1.  **假设模型形式**: 预先假设 f 的函数形式，例如，假设 f 是线性的：$f(X) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p$。
        2.  **拟合模型**: 使用训练数据估计模型参数 (如 $\beta_0, \beta_1, \dots, \beta_p$)。
    *   **优点**: 将估计 f 的复杂问题简化为估计少量参数，计算上更简单。
    *   **缺点**: 如果假设的模型形式与真实的 f 相差甚远，则模型表现会很差。容易**过拟合**（模型过于复杂，拟合了噪声）。

2.  **非参数方法 (Non-parametric Methods)**
    *   **步骤**: 不对 f 的函数形式做任何显式假设，力求使模型尽可能地贴近数据点。
    *   **例子**: 自然样条曲线、决策树。
    *   **优点**: 具有高度灵活性，能拟合多种形状的 f。
    *   **缺点**: 需要大量的观测数据才能获得对 f 的准确估计。

#### 2.1.4 预测精度与模型解释性的权衡

模型的光滑度（或灵活性/复杂度）与可解释性之间存在权衡关系。

*   **低光滑度/灵活性**: 模型简单（如线性回归），可解释性强，但可能无法捕捉复杂的非线性关系，导致偏差较高。
*   **高光滑度/灵活性**: 模型复杂（如支持向量机、Boosting），可解释性差，但能拟合复杂关系，偏差较低。



#### 2.1.5 有监督学习 vs. 无监督学习

*   **有监督学习 (Supervised Learning)**: 每个观测数据 $x_i$ 都有一个对应的响应变量 $y_i$。目标是学习一个从输入到输出的映射。
    *   **回归 (Regression)**: 响应变量 Y 是定量的。
    *   **分类 (Classification)**: 响应变量 Y 是定性的（类别）。
*   **无监督学习 (Unsupervised Learning)**: 只有输入数据 X，没有对应的响应变量 Y。目标是发现数据中的结构或关系。
    *   **例子**: 聚类分析。

### 2.2 模型精度评估

#### 2.2.1 回归模型的评估：均方误差 (MSE)

在回归问题中，我们使用**均方误差 (MSE)** 来衡量模型的拟合效果。
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2 = \frac{1}{n} \text{RSS}
$$
其中 RSS (Residual Sum of Squares) 是残差平方和。

*   **训练 MSE (Training MSE)**: 在训练数据上计算的 MSE。
*   **测试 MSE (Test MSE)**: 在未见过的测试数据上计算的 MSE。

我们的目标是使**测试 MSE** 最小化。模型光滑度增加时，训练 MSE 会单调下降，但测试 MSE 会先下降后上升，呈现 **U 形**，这是由偏差-方差权衡导致的。

#### 2.2.2 偏差-方差权衡 (Bias-Variance Trade-Off)

一个新数据点 $x_0$ 的期望测试 MSE 可以分解为三部分：
$$
E(y_0 - \hat{f}(x_0))^2 = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\epsilon)
$$

| 特性 | **偏差 (Bias)** | **方差 (Variance)** |
| :--- | :--- | :--- |
| **定义** | 模型预测值的平均值与真实值之间的差距。 | 模型在不同训练集上预测值的波动程度。 |
| **来源** | 模型的假设过于简单，无法捕捉真实关系（**欠拟合**）。 | 模型对训练数据过于敏感，拟合了噪声（**过拟合**）。 |
| **与光滑度的关系** | 光滑度**低**，偏差**高**。 | 光滑度**高**，方差**高**。 |


*   **黑色实线**: 测试 MSE。
*   **橙色曲线**: 偏差的平方。
*   **蓝色曲线**: 方差。
*   **水平虚线**: 不可约误差 $\text{Var}(\epsilon)$。

随着模型光滑度（Flexibility）增加：
1.  **偏差**（橙色）持续下降。
2.  **方差**（蓝色）持续上升。
3.  **测试 MSE**（黑色）先下降后上升，在中间点达到最小值。

#### 2.2.3 分类模型的评估

*   **错误率 (Error Rate)**: 被错误分类的样本比例。
    *   训练错误率: $\frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i)$
    *   测试错误率: $\text{Ave}(I(y_0 \neq \hat{y}_0))$
*   **贝叶斯分类器 (Bayes Classifier)**: 理论上错误率最低的分类器。它将每个观测分到后验概率最大的那一类。在实际中无法计算，因为需要知道真实的条件概率分布。
*   **K-最近邻 (KNN)**: 一种非参数分类方法。
    *   **原理**: 对于一个测试点，找出训练集中离它最近的 K 个点，然后根据这 K 个点的类别进行“投票”，决定测试点的类别。
    *   **K 的选择**: K 的选择是偏差-方差权衡的体现。
        *   **小 K (如 K=1)**: 模型光滑度高，决策边界非常“曲折”。**低偏差，高方差**。
        *   **大 K (如 K=100)**: 模型光滑度低，决策边界趋于平滑。**高偏差，低方差**。

#### 核心问题：预测 (Prediction) 与推断 (Inference) 的区别

统计学习的目标可以分为两大类：预测和推断。理解二者的区别至关重要。

*   **1. 预测 (Prediction)**
    *   **目标**: 准确地预测响应变量 Y。
    *   **核心**: 将模型 $f$ 视为一个“黑箱”。我们不关心 $f$ 的具体形式，只关心对于给定的输入 X，能否得到一个尽可能接近真实 Y 的预测值 $\hat{Y}$。
    *   **例子**: 根据病人的临床指标预测其患病风险有多高；根据市场数据预测某支股票明天的价格。
    *   **评估**: 重点是评估预测的**准确性**，核心指标是**预测误差**，如均方误差 (MSE)。

*   **2. 推断 (Inference)**
    *   **目标**: 理解输入 X 和输出 Y **之间**的关系。
    *   **核心**: 模型 $f$ 不能是“黑箱”。我们需要打开这个“盒子”，去理解变量之间的作用机制。
    *   **典型问题**:
        *   哪些预测变量与响应变量显著相关？
        *   某个预测变量与响应变量是正相关还是负相关？
        *   响应变量与每个预测变量之间的关系是简单的线性关系，还是更复杂的非线性关系？
    *   **例子**: 研究广告投入（电视、广播、报纸）分别对产品销量的影响有多大，以便制定营销策略；研究药物剂量与病人血压之间的关系。
    *   **评估**: 重点是评估模型的**可解释性**和**不确定性**，核心指标包括系数的置信区间、p值等。

#### 模型质量的核心度量

根据目标是预测还是推断，我们使用不同的指标来衡量模型的质量。

*   **1. 评价模型预测精度：均方误差 (MSE) 与误分类率**
    *   **均方误差 (Mean Squared Error, MSE)** (用于回归问题)
        *   **定义**: 预测值与真实值之差的平方的均值。它是衡量模型预测准确性的黄金标准。
        *   **公式**:
            $$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2 $$
        *   **区分训练MSE与测试MSE**:
            *   **训练MSE**: 在**训练数据**上计算的MSE。它衡量模型对已知数据的拟合程度。
            *   **测试MSE**: 在**未见过的新数据（测试数据）**上计算的MSE。它衡量模型的**泛化能力**。
            *   **核心目标**: 统计学习的最终目标是获得**最低的测试MSE**，而非最低的训练MSE。过度追求低训练MSE会导致过拟合。

    *   **误分类率 (Misclassification Rate)** (用于分类问题)
        *   **定义**: 被模型错误分类的样本占总样本的比例。
        *   **公式**:
            $$ \text{Error Rate} = \frac{1}{n} \sum_{i=1}^{n} I(y_i \neq \hat{y}_i) $$
            其中 $I(\cdot)$ 是指示函数，当条件为真时取1，否则取0。

*   **2. 衡量估计值的不确定性：标准误差 (SE)**
    *   **定义**: 标准误差 (Standard Error) 衡量的是一个**统计估计量**（例如，通过最小二乘法得到的回归系数 $\hat{\beta_1}$）与其真实值（未知的 $\beta_1$）之间的平均偏差。它反映了如果我们用不同的训练数据集去重复估计这个量，估计值的波动程度有多大。
    *   **作用**: SE越小，说明我们的估计越可靠。它常用于计算**置信区间**和进行**假设检验**（如t检验），主要服务于**推断**的目标。

*   **3. 衡量模型对数据的拟合优度：残差标准误 (RSE) 与 R²**
    *   **残差标准误 (Residual Standard Error, RSE)**
        *   **定义**: RSE可以看作是模型残差（$y_i - \hat{f}(x_i)$）的典型大小。它衡量了响应变量偏离真实回归线的平均程度。
        *   **公式**:
            $$ \text{RSE} = \sqrt{\frac{1}{n-p-1} \text{RSS}} = \sqrt{\frac{1}{n-p-1} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2} $$
        *   **解释**: RSE的单位与响应变量Y相同。RSE越小，说明模型对训练数据的拟合越好。

    *   **R² 统计量 (R-squared)**
        *   **定义**: R² 衡量的是响应变量Y的总变异中，有多少比例可以被模型中的预测变量X所解释。
        *   **公式**:
            $$ R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} $$
            其中 TSS (Total Sum of Squares) 是总平方和，代表了Y的总体方差。
        *   **解释**: R² 的取值范围是 [0, 1]。R² 越接近1，说明模型的解释能力越强，对数据的拟合越好。它是一个相对指标，不受Y的量纲影响。

#### 核心权衡：偏差-方差权衡 (Bias-Variance Trade-Off)

这是理解模型泛化能力的关键，解释了为什么过于简单的模型和过于复杂的模型都无法取得好的预测效果。

*   **期望测试MSE的分解**
    对于一个新的数据点 $x_0$，其期望测试均方误差可以被精确地分解为三个部分：
    $$ E\left[ (y_0 - \hat{f}(x_0))^2 \right] = \text{Bias}(\hat{f}(x_0))^2 + \text{Var}(\hat{f}(x_0)) + \text{Var}(\epsilon) $$

    *   **偏差的平方 (Bias²)**:
        *   **定义**: $\text{Bias}(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$。它衡量的是，如果我们用无数个不同的训练集去训练模型，所有模型对 $x_0$ 预测值的**平均值**与**真实函数值** $f(x_0)$ 之间的差距。
        *   **来源**: 源于模型假设的**不正确性或过于简单**。例如，用一条直线去拟合一个U形的数据关系，无论如何调整直线，其平均预测值都会系统性地偏离真实曲线。这导致了高偏差，即**欠拟合**。
    *   **方差 (Variance)**:
        *   **定义**: $\text{Var}(\hat{f}(x_0)) = E\left[ (\hat{f}(x_0) - E[\hat{f}(x_0)])^2 \right]$。它衡量的是，当我们用不同的训练集去训练模型时，模型对 $x_0$ 的**预测值自身的波动性**有多大。
        *   **来源**: 源于模型**对训练数据的微小变化过于敏感**。一个高度灵活（复杂）的模型会试图去拟合训练数据中的每一个点，包括噪声。当训练数据稍有变动，模型的拟合结果就会发生剧烈变化。这导致了高方差，即**过拟合**。
    *   **不可约误差 (Irreducible Error)**:
        *   **定义**: $\text{Var}(\epsilon)$。这是数据本身固有的噪声所带来的误差，是任何模型都无法消除的误差下限。

*   **图形化理解**:

    

    *   **横轴**: 模型复杂度/灵活性 (Model Flexibility)。
    *   **蓝色曲线 (偏差平方)**: 随着模型灵活性增加，模型能更好地拟合真实函数，偏差**单调递减**。
    *   **橙色曲线 (方差)**: 随着模型灵活性增加，模型开始拟合数据中的噪声，对数据变动更敏感，方差**单调递增**。
    *   **红色曲线 (测试MSE)**: 作为偏差平方和方差的和，它呈现出一条**U形曲线**。
    *   **结论**: 我们的目标是找到位于U形曲线谷底的模型，这个模型在偏差和方差之间取得了最佳的平衡。

*   **实例：K-最近邻 (KNN) 中的偏差-方差权衡**
    KNN算法是理解这一权衡的绝佳例子，其中调节参数是邻居数 K。

    *   **小 K (例如 K=1)**:
        *   **模型行为**: 预测仅依赖于最近的1个邻居，决策边界会变得非常曲折，紧紧地包围着每个训练数据点。
        *   **结果**: 模型**灵活性极高**。它能完美拟合训练数据，因此**偏差很低**。但如果训练数据稍有变动，预测结果就会大相径庭，因此**方差很高**。
    *   **大 K (例如 K=100)**:
        *   **模型行为**: 预测依赖于大量邻居的平均/投票结果，这会使得决策边界变得非常平滑，忽略了数据的局部细节。
        *   **结果**: 模型**灵活性很低**。它无法捕捉数据的复杂结构，因此**偏差很高**。但由于预测是大量点的平均，它对单个数据点的变动不敏感，因此**方差很低**。

通过调整 K 值，我们就是在偏差-方差曲线上移动，寻找能使测试错误率最小的那个 K。


---

## 第三章 线性回归

### 3.1 简单线性回归

研究一个自变量 X 和一个因变量 Y 之间的线性关系。
$$
Y = \beta_0 + \beta_1 X + \epsilon
$$
*   **参数估计**: 通过**最小二乘法**，即最小化**残差平方和 (RSS)** 来估计 $\beta_0$ 和 $\beta_1$。
    $$
    \text{RSS} = \sum_{i=1}^n (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2
    $$
*   **置信区间**: 95% 置信区间可以解释为：如果我们用不同的数据集重复构建模型 100 次，大约有 95 个区间会包含真实的参数值。
    $$
    \hat{\beta}_1 \pm 2 \cdot \text{SE}(\hat{\beta}_1)
    $$

### 3.2 多元线性回归

研究多个自变量 $X_1, \dots, X_p$ 和因变量 Y 之间的关系。
$$
Y = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p + \epsilon
$$

### 3.3 模型评估

1.  **残差标准误 (RSE)**: 衡量模型预测值偏离真实值的平均程度。RSE 越小，模型拟合越好。
    $$
    \text{RSE} = \sqrt{\frac{1}{n-p-1} \text{RSS}}
    $$
2.  **R² 统计量**: 衡量响应变量的变异中能被模型解释的比例，取值在 0 到 1 之间。R² 越大，模型拟合越好。
    $$
    R^2 = 1 - \frac{\text{RSS}}{\text{TSS}} = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
    $$
3.  **F 统计量**: 检验模型中至少有一个预测变量与响应变量相关的假设。F 值远大于 1，则可以拒绝零假设（即所有系数都为 0）。

### 3.4 常见问题与诊断

1.  **非线性关系**:
    *   **诊断**: **残差图** (残差 vs. 拟合值)。如果图中出现明显的模式（如 U 形），则表明可能存在非线性关系。
    *   **解决**: 对预测变量进行变换（如 $\log(X), \sqrt{X}, X^2$）或使用非线性模型。
    
    (左图：线性拟合残差图有 U 形模式，说明线性模型不合适。右图：二次拟合后模式消失。)

2.  **误差项方差非恒定 (异方差性)**:
    *   **诊断**: 残差图中出现“漏斗形”，即残差的大小随拟合值的变化而变化。
    *   **解决**: 对响应变量进行变换（如 $\log(Y)$）。
    
    (左图：残差呈漏斗形。右图：对 Y 取对数后，方差变得稳定。)

3.  **离群点 (Outliers)**:
    *   **定义**: 响应值 $y_i$ 远离模型预测值的点。
    *   **诊断**: 学生化残差图 (Studentized Residuals Plot)。绝对值大于 3 的点通常被视为离群点。

4.  **高杠杆点 (High Leverage Points)**:
    *   **定义**: 预测变量 $x_i$ 异常的点。
    *   **诊断**: **杠杆图** (Residuals vs. Leverage Plot)。杠杆值高的点位于图的右侧。高杠杆且残差大的点（影响力大的点）需要特别关注。
    

5.  **共线性 (Collinearity)**:
    *   **定义**: 两个或多个预测变量高度相关。
    *   **后果**: 难以分离单个变量对响应的影响，导致系数估计不稳定，标准误增大。
    *   **诊断**: 查看相关系数矩阵；计算**方差膨胀因子 (VIF)**。VIF > 5 或 10 表示存在共线性问题。

### R 语言代码

```R
model <- lm(Y ~ X1 + X2, data = mydata)
summary(model)
confint(model, level = 0.95)
plot(predict(model), residuals(model))
abline(h = 0, col = "red")
```

### 第三章 线性回归 (补充)

本章内容是统计学习的基石。核心在于理解模型、评估模型、诊断问题。

#### 一、简单线性回归 (SLR)

##### 1. 模型与系数估计

*   **模型定义**:
    $$ Y = \beta_0 + \beta_1 X + \epsilon $$
    *   **目标**: 找到最佳的截距 $\beta_0$ 和斜率 $\beta_1$，使得直线 $y = \beta_0 + \beta_1 x$ 能最好地拟合数据。
*   **最小二乘估计**:
    *   **原理**: “最好”的拟合是指所有数据点到回归直线的**残差平方和 (RSS)** 最小。
    *   **公式**:
        $$ \text{RSS} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} (y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i))^2 $$
        通过微积分求导并令其为0，可以得到系数的估计值：
        $$ \hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} $$
        $$ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} $$
    *   **核心记忆**: 最小二乘直线**必定**经过样本均值点 $(\bar{x}, \bar{y})$。

##### 2. 评估系数估计的准确性：假设检验

*   **标准误差 (Standard Error, SE)**:
    *   **概念**: SE衡量的是系数估计值 $\hat{\beta}_1$ 的不确定性。如果用不同的样本子集多次估计，$\hat{\beta}_1$ 会变化，SE就是这些估计值分布的标准差。
    *   **公式**:
        $$ \text{SE}(\hat{\beta}_1)^2 = \frac{\sigma^2}{\sum(x_i - \bar{x})^2} \quad , \quad \text{SE}(\hat{\beta}_0)^2 = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}^2}{\sum(x_i - \bar{x})^2} \right] $$
        其中 $\sigma^2 = \text{Var}(\epsilon)$，通常用 RSE² 来估计。
    *   **解读**: 预测变量 X 的散布范围越大（$\sum(x_i - \bar{x})^2$ 越大），我们对斜率 $\beta_1$ 的估计就越稳定，其SE就越小。
*   **t检验**:
    *   **目的**: 检验单个预测变量 X 与响应变量 Y 之间是否存在显著的线性关系。
    *   **假设**:
        *   零假设 $H_0$: X 和 Y 之间没有关系，即 $\beta_1 = 0$。
        *   备择假设 $H_1$: X 和 Y 之间有关系，即 $\beta_1 \neq 0$。
    *   **t统计量**:
        $$ t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)} $$
        *   **解读**: t值衡量了估计系数 $\hat{\beta}_1$ 偏离0的程度，以其标准误差为单位。t值的**绝对值**越大，说明 $\hat{\beta}_1$ 越不可能为0。
    *   **p值**:
        *   **定义**: 在零假设 $H_0$ 为真的前提下，观测到任意大于等于当前t统计量绝对值的概率。
        *   **判据**: 一个小的p值（通常 < 0.05）意味着，如果X和Y真的没有关系，我们观测到如此强的关联性的可能性非常小。因此，我们有理由**拒绝零假设**，认为该变量是统计显著的。

#### 二、评估模型整体的准确性

*   **RSE (残差标准误)**:
    *   **解读**: 衡量模型预测的典型误差大小。例如，如果RSE=3.26，意味着模型的预测值平均会偏离真实值约3.26个单位（单位与Y相同）。它是一个**绝对**度量。
*   **R² 统计量**:
    *   **解读**: R²=0.61表示响应变量Y的变异中有61%可以被预测变量X解释。它是一个**相对**度量，取值在0到1之间，更易于跨模型比较。
    *   **注意**: R² 高不代表模型一定好。它只衡量线性关系的强度，且容易受异常值影响。

#### 三、多元线性回归 (MLR)

##### 1. 模型与系数解释

*   **模型**: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon$
*   **系数解释 (核心考点)**: $\beta_j$ 的含义是，在**保持所有其他预测变量 ($X_k, k \neq j$) 固定不变**的情况下， $X_j$ 每增加一个单位，Y的**平均**变化量。
    *   **例子**: `sales = β₀ + β₁*TV + β₂*radio`。$\beta_2$ 表示在**TV广告预算不变**的情况下，radio广告预算每增加1000美元，平均带来的销量变化。

##### 2. F检验：检验模型整体显著性

*   **目的**: 回答“在所有 p 个预测变量中，是否至少有一个与响应变量相关？”
*   **假设**:
    *   零假设 $H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0$ (所有变量都无效)。
    *   备择假设 $H_1$: 至少有一个 $\beta_j \neq 0$。
*   **F统计量**:
    $$ F = \frac{(\text{TSS} - \text{RSS})/p}{\text{RSS}/(n-p-1)} $$
    *   **解读**: F统计量衡量的是模型解释的方差与未解释的方差之比。如果 $H_0$ 为真，F值应接近1。如果 $H_1$ 为真，F值应大于1。
*   **为何必须用F检验**:
    *   **多重比较陷阱**: 如果有大量预测变量（如p=100），即使它们都与Y无关，在α=0.05的显著性水平下，单独做t检验也可能碰巧有5个变量的p值小于0.05。F检验通过一次“打包”检验来避免这个问题，控制了整体犯第一类错误的概率。

#### 四、线性回归的重要扩展

##### 1. 定性预测变量 (哑变量)

*   **问题**: 如何在线性模型中加入“性别”、“地区”这类定性变量？
*   **方法**: 创建**哑变量 (Dummy Variables)**。
    *   **规则**: 如果一个定性变量有 k 个水平，需要创建 k-1 个哑变量。
    *   **例子**: 变量“地区”有三个水平：东部、西部、南部。
        1.  选择一个水平作为**基准水平**，例如“南部”。
        2.  创建两个哑变量：
            $$ x_1 = \begin{cases} 1 & \text{如果 地区=东部} \\ 0 & \text{否则} \end{cases} $$
            $$ x_2 = \begin{cases} 1 & \text{如果 地区=西部} \\ 0 & \text{否则} \end{cases} $$
        3.  模型为: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$
    *   **系数解释**:
        *   $\beta_0$: 基准水平（南部）的平均响应。
        *   $\beta_1$: 东部地区相对于南部地区的平均响应差异。
        *   $\beta_2$: 西部地区相对于南部地区的平均响应差异。

##### 2. 交互作用 (Interaction)

*   **概念**: 打破了线性模型的“可加性”假设。交互作用意味着一个变量对响应的影响，会随着另一个变量取值的不同而改变。
*   **模型**:
    $$ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 X_2) + \epsilon $$
*   **系数解释**:
    *   现在 $X_1$ 对 Y 的影响是 $(\beta_1 + \beta_3 X_2)$。这个影响不再是常数，而是依赖于 $X_2$ 的值。
    *   $\beta_3$ 衡量了交互作用的强度。
*   **实验分层原则**: 如果模型中包含交互项 $\beta_3 (X_1 X_2)$，那么即使主效应项 $\beta_1 X_1$ 或 $\beta_2 X_2$ 的p值不显著，也应该将它们保留在模型中，因为交互项的存在使得主效应的解释依赖于它。

#### 五、潜在问题诊断与处理

| 问题 | 诊断方法 | 解决方案 |
| :--- | :--- | :--- |
| **1. 非线性关系** | **残差图**（残差 vs. 拟合值）出现明显模式（如U形）。 | 使用预测变量的非线性变换（如`log(X)`, `X²`）或改用非线性模型。 |
| **2. 误差项自相关** | **残差图**（残差 vs. 时间/顺序）出现“跟踪”现象。 | 使用时序模型（如ARIMA）或在模型中包含滞后项。 |
| **3. 异方差性** | **残差图**出现“漏斗形”（残差的散布程度随拟合值变化）。 | 对响应变量Y进行变换（如`log(Y)`, `sqrt(Y)`）或使用加权最小二乘。 |
| **4. 离群点 (Outliers)** | **学生化残差图**，寻找绝对值大于3的点。 | 检查是否为数据录入错误。若不是，可尝试移除该点并比较模型变化，或使用稳健回归方法。 |
| **5. 高杠杆点 (High Leverage)** | **杠杆图**（残差 vs. 杠杆值），寻找杠杆值大的点。 | 检查该点是否对模型结果有不成比例的影响（影响力大的点）。若有，处理方法同离群点。 |
| **6. 共线性 (Collinearity)** | 查看变量间的**相关系数矩阵**；计算**方差膨胀因子 (VIF)**，VIF>5或10表示有问题。 | 移除一个高度相关的变量；将相关变量组合成一个新变量；使用岭回归或Lasso。 |

#### 六、置信区间 vs. 预测区间

这是一个非常重要的概念区分，常用于评估模型的不确定性。

*   **1. 置信区间 (Confidence Interval)**
    *   **目的**: 估计给定一组预测变量值 $x_0$ 时，响应变量的**平均值** $E(Y|X=x_0)$ 的可能范围。
    *   **问题**: “对于所有电视广告花费为10万美元的公司，其**平均**销量的95%置信区间是多少？”
    *   **来源**: 不确定性仅来自于我们对回归系数 $\beta_0, \beta_1, \dots$ 的估计不准。
    *   **宽度**: **较窄**。

*   **2. 预测区间 (Prediction Interval)**
    *   **目的**: 预测给定一组预测变量值 $x_0$ 时，**单个**响应值 $y_0$ 的可能范围。
    *   **问题**: “对于**某一个**电视广告花费为10万美元的公司，其**自身**销量的95%预测区间是多少？”
    *   **来源**: 不确定性不仅来自于系数估计的不准，还**额外**包括了**不可约误差 $\epsilon$** 的随机性。
    *   **宽度**: **更宽**。

**结论**: 对于同一个预测点 $x_0$，预测区间总是比置信区间更宽，因为它需要考虑单个观测的随机波动。

---

## 第四章 分类

### 4.1 逻辑斯蒂回归 (Logistic Regression)

用于二分类问题，预测一个事件发生的概率。它使用 **logistic 函数** 将线性模型的输出映射到 (0, 1) 区间。
$$
P(Y=1|X) = \frac{e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}{1 + e^{\beta_0 + \beta_1 X_1 + \dots + \beta_p X_p}}
$$
通过变换，可以得到对数发生比 (log-odds) 的线性模型：
$$
\log\left(\frac{P}{1-P}\right) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p
$$
参数通过**极大似然法**估计。

### 4.2 线性判别分析 (LDA)

*   **核心思想**: 找到一个能使类别之间**分离度最大**的投影方向。具体来说，是最大化**类间方差**与**类内方差**之比。
*   **假设**:
    1.  每个类别的数据都服从多元高斯分布。
    2.  所有类别的**协方差矩阵相同**。
*   **决策边界**: 线性。


### 4.3 二次判别分析 (QDA)

*   **与 LDA 的区别**: QDA 放宽了 LDA 的第二个假设，允许每个类别拥有**各自的协方差矩阵**。
*   **决策边界**: 二次（曲线）。
*   **权衡**:
    *   **LDA**: 更不灵活，偏差高，方差低。当类别协方差矩阵确实相近时表现更好。
    *   **QDA**: 更灵活，偏差低，方差高。需要更多数据来估计协方差矩阵，但能捕捉更复杂的关系。

(左图：真实边界是线性的，LDA表现好。右图：真实边界是曲线，QDA表现好。)

### 4.4 分类模型评估

#### 4.4.1 混淆矩阵 (Confusion Matrix)

| | 预测为 - | 预测为 + | 总计 |
| :--- | :--- | :--- | :--- |
| **真实为 -** | 真阴性 (TN) | 假阳性 (FP) | N |
| **真实为 +** | 假阴性 (FN) | 真阳性 (TP) | P |
| **总计** | N* | P* | |

#### 4.4.2 关键指标

| 名称 | 定义 | 解释 |
| :--- | :--- | :--- |
| **真阳性率 (TPR) / 灵敏度 / 召回率** | TP / P | 真实为阳性的样本中，被正确预测为阳性的比例。 |
| **假阳性率 (FPR)** | FP / N | 真实为阴性的样本中，被错误预测为阳性的比例。 |
| **真阴性率 (TNR) / 特异度** | TN / N | 真实为阴性的样本中，被正确预测为阴性的比例。 |
| **预测阳性率 / 精确度 (Precision)** | TP / P* | 预测为阳性的样本中，实际为阳性的比例。 |

#### 4.4.3 ROC 曲线

*   **定义**: 以 **假阳性率 (FPR)** 为横轴，**真阳性率 (TPR)** 为纵轴绘制的曲线。
*   **作用**: 展示分类器在所有可能阈值下的性能。
*   **评估**:
    *   曲线越靠近左上角，模型性能越好。
    *   **AUC (Area Under the Curve)**: ROC 曲线下的面积。AUC 值为 1 表示完美分类器，0.5 表示随机猜测。


### R 语言代码

```R
glm.fit <- glm(Y ~ X1 + X2, data = mydata, family = binomial)
lda.fit <- lda(Y ~ X1 + X2, data = mydata)
qda.fit <- qda(Y ~ X1 + X2, data = mydata)
knn.pred <- knn(train.X, test.X, train.Y, k = 3)
```


### 第四章 分类 (补充)

#### 核心问题：为什么不能用线性回归做分类？

在处理响应变量 Y 是定性（分类）的问题时，直接使用线性回归是不可行的，主要有以下两个原因：

1.  **对定性变量的数值编码是任意且会误导模型的**
    *   **问题**: 线性回归要求响应变量 Y 是数值型的。对于一个有 K 个类别的定性变量，我们需要将其编码为数值。
    *   **二分类问题**: 如果类别是“是”和“否”，我们可以编码为 1 和 0。线性回归在技术上可以运行，但有下一个问题。
    *   **多分类问题 (K>2)**: 如果类别是“中风”、“药物过量”、“癫痫发作”，我们可能会编码为 1, 2, 3。这种编码方式强加了一种不存在的**序数关系**和**等距关系**，即模型会认为“药物过量”(2) 和“中风”(1) 的差距与“癫痫发作”(3) 和“药物过量”(2) 的差距是相等的，并且“癫痫发作”是“中风”的三倍。这显然是不合理的，并且更换编码方式（如 1, 3, 2）会得到完全不同的模型。

2.  **线性回归的预测值会越界**
    *   **问题**: 分类问题我们通常希望得到的是一个**概率**，即“属于某个类别的可能性有多大”。概率的取值范围必须在 [0, 1] 之内。
    *   **线性回归的缺陷**: 线性回归模型 $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1X$ 的输出范围是 $(-\infty, +\infty)$。这意味着它很容易预测出小于0或大于1的值，这些值无法被解释为概率。

    

    *   **上图说明**: 左图是用线性回归拟合二分类数据，拟合线（蓝色）的预测值在两端明显超出了[0, 1]的范围。而右图的逻辑斯蒂回归曲线（蓝色）则始终保持在[0, 1]区间内。

#### 逻辑斯蒂回归 (Logistic Regression)

逻辑斯蒂回归通过引入一个非线性的 **Sigmoid (或 Logistic) 函数**来解决上述问题，将线性回归的输出压缩到 [0, 1] 区间。

*   **模型定义**
    *   **核心思想**: 我们不对 Y 直接建模，而是对 Y 属于某个特定类别（通常记为“1”）的**概率** $p(X) = Pr(Y=1|X)$ 进行建模。
    *   **公式**:
        $$ p(X) = \frac{e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p}} $$
        这个 S 形的函数确保了无论线性部分的输出值是多少，最终的概率 $p(X)$ 始终在 (0, 1) 之间。

*   **模型解释：对数发生比 (Log-odds)**
    *   直接解释上述公式中 $\beta_j$ 的含义很困难。为了得到一个可解释的模型，我们对公式进行数学变换。
    *   **发生比 (Odds)**: 定义为一个事件发生的概率与不发生的概率之比。
        $$ \text{Odds} = \frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1X_1 + \dots + \beta_pX_p} $$
        发生比的取值范围是 $[0, \infty)$。
    *   **对数发生比 (Log-odds / Logit)**: 对发生比取自然对数。
        $$ \log\left(\frac{p(X)}{1-p(X)}\right) = \beta_0 + \beta_1X_1 + \dots + \beta_pX_p $$
    *   **解释**: 这个变换的意义在于，我们得到了一个关于 X 的**线性模型**。现在，系数 $\beta_j$ 的解释变得清晰了：**在其他所有预测变量保持不变的情况下，Xⱼ 每增加一个单位，Y 的对数发生比会增加 $\beta_j$**。如果 $\beta_j$ 为正，说明 Xⱼ 增大会增加 Y=1 的概率；如果为负，则会减小该概率。

*   **系数估计**
    *   逻辑斯蒂回归的系数 $(\beta_0, \beta_1, \dots, \beta_p)$ 是通过**最大似然估计 (Maximum Likelihood Estimation, MLE)** 来求解的。其基本思想是，找到一组最优的系数，使得在这组系数下，我们观测到的这组训练数据（即每个样本的真实类别）出现的总概率最大。

*   **混淆现象 (Confounding)**
    *   **现象**: 在多元逻辑斯蒂回归中，一个变量的系数符号可能与其在简单逻辑斯蒂回归中的系数符号相反。
    *   **例子 (学生-信用卡余额-违约)**:
        1.  **简单回归**: 单独用“是否学生”来预测违约，发现学生的违约率更高（系数为正）。
        2.  **多元回归**: 同时用“是否学生”和“信用卡余额”来预测违约，发现学生的系数变为负。
    *   **原因**: 存在一个**混淆变量**“信用卡余额”。学生群体平均持有更高的信用卡余额，而高余额是导致违约的主要原因。在简单回归中，学生身份“吸收”了高余额带来的风险，表现为正相关。但在多元回归中，当我们**控制了信用卡余额**这个变量后（即比较相同余额的学生和非学生），学生本身的违约风险实际上是更低的（系数为负）。

#### 线性判别分析 (LDA)

LDA 是另一种经典的分类方法，它属于**生成模型**的范畴。

*   **核心原理：贝叶斯定理**
    *   **思路**: LDA 不像逻辑斯蒂回归那样直接对后验概率 $p_k(x) = Pr(Y=k|X=x)$ 建模，而是通过对**先验概率** $\pi_k$ 和**类条件密度** $f_k(x)$ 建模，然后利用贝叶斯公式来计算后验概率。
    *   **贝叶斯公式**:
        $$ p_k(x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K}\pi_l f_l(x)} $$
        *   $p_k(x)$: **后验概率** - 给定观测值 X=x，其属于类别 k 的概率。这是我们最终想要的。
        *   $\pi_k = Pr(Y=k)$: **先验概率** - 在不知道 X 的情况下，一个随机样本属于类别 k 的概率。通常用训练集中类别 k 的样本比例来估计。
        *   $f_k(x) = Pr(X=x|Y=k)$: **类条件密度** - 在已知样本属于类别 k 的前提下，观测到 X=x 的概率密度。

*   **LDA的核心假设**
    1.  **高斯分布**: 假设每个类别的类条件密度 $f_k(x)$ 都服从一个**多元高斯（正态）分布**。
    2.  **同方差**: 假设所有 K 个类别**共享同一个协方差矩阵** $\Sigma$。即不同类别的数据分布形状和方向是相同的，只是中心位置（均值 $\mu_k$）不同。

*   **判别函数 $\delta_k(x)$ 与线性边界**
    *   为了进行分类，我们需要找到使后验概率 $p_k(x)$ 最大的类别 k。对贝叶斯公式取对数并舍弃与 k 无关的项，可以得到**判别函数 $\delta_k(x)$**：
        $$ \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log(\pi_k) $$
    *   **关键**: 这个函数是关于 $x$ 的一个**线性函数**。两个类别 k 和 l 之间的决策边界由 $\delta_k(x) = \delta_l(x)$ 决定，这个方程也是线性的。这就是为什么LDA被称为**线性**判别分析。

#### 二次判别分析 (QDA)

*   **与LDA的区别**
    *   QDA 放宽了LDA的同方差假设。它假设每个类别 k 都有其**各自的协方差矩阵 $\Sigma_k$**。
    *   这允许不同类别的分布可以有不同的形状和方向，使得模型更加灵活。

*   **判别函数 $\delta_k(x)$ 与二次边界**
    *   在不同协方差矩阵的假设下，QDA的判别函数变为：
        $$ \delta_k(x) = -\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) - \frac{1}{2}\log|\Sigma_k| + \log(\pi_k) $$
    *   **关键**: 这个函数中包含了 $x$ 的二次项（$x^T\Sigma_k^{-1}x$），因此它是一个关于 $x$ 的**二次函数**。决策边界 $\delta_k(x) = \delta_l(x)$ 也就成了一条二次曲线（如抛物线、双曲线或椭圆）。

*   **LDA vs. QDA 的偏差-方差权衡**
    *   **LDA**: 限制性更强（假设同方差），模型更简单，不灵活。**偏差较高，方差较低**。
    *   **QDA**: 限制性更弱，模型更复杂，更灵活。**偏差较低，方差较高**。
    *   **如何选择**:
        *   当训练样本较少时，降低方差更为重要，**LDA** 通常表现更好。
        *   当训练样本非常多时，高方差的负面影响减弱，**QDA** 的灵活性优势得以体现，通常表现更好。
        *   如果类别间的协方差矩阵确实非常接近，**LDA** 会是更好的选择。

#### 分类模型评估指标详解

*   **混淆矩阵**
    这是一个 $2 \times 2$ 的表格，是所有分类评估指标的基础。

| | **预测为阴性(-)** | **预测为阳性(+)** |
| :--- | :--- | :--- |
| **真实为阴性(-)** | 真阴性 (TN) | 假阳性 (FP) |
| **真实为阳性(+)** | 假阴性 (FN) | 真阳性 (TP) |

*   **指标定义与解释**
    *   **第一类错误 (Type I Error)**: 即 **假阳性 (FP)**。错误地拒绝了零假设（这里通常指“样本为阴性”）。
    *   **第二类错误 (Type II Error)**: 即 **假阴性 (FN)**。错误地未能拒绝一个错误的零假设。
    *   **真阳性率 (TPR) / 灵敏度 (Sensitivity) / 召回率 (Recall)**: 在所有真实为阳性的样本中，被模型成功预测为阳性的比例。
        $$ \text{TPR} = \frac{TP}{TP + FN} $$
        *   **关注点**: 衡量模型**查全**正样本的能力。在疾病诊断、故障检测等场景中至关重要，因为漏掉一个阳性样本（假阴性）的代价很高。
    *   **特异度 (Specificity) / 真阴性率 (TNR)**: 在所有真实为阴性的样本中，被模型成功预测为阴性的比例。
        $$ \text{TNR} = \frac{TN}{TN + FP} $$
        *   **关注点**: 衡量模型**识别**负样本的能力。
    *   **假阳性率 (FPR)**: 在所有真实为阴性的样本中，被模型错误预测为阳性的比例。
        $$ \text{FPR} = \frac{FP}{TN + FP} = 1 - \text{Specificity} $$
    *   **精确度 (Precision) / 预测阳性率 (PPV)**: 在所有被模型预测为阳性的样本中，实际也为阳性的比例。
        $$ \text{Precision} = \frac{TP}{TP + FP} $$
        *   **关注点**: 衡量模型预测结果的**准确性**。在垃圾邮件检测、推荐系统等场景中很重要，因为我们不希望把好的内容错判（假阳性）。

---

## 第五章 重抽样方法

### 5.1 核心思想

通过从原始数据集中重复抽样来生成多个数据集，在这些数据集上拟合模型，以获得关于模型性能的更多信息，主要用于**估计测试误差**。

### 5.2 交叉验证 (Cross-Validation)

#### 5.2.1 验证集方法

1.  **步骤**: 将数据随机分为**训练集**和**验证集**。
2.  **优点**: 简单，计算成本低。
3.  **缺点**:
    *   结果对数据划分敏感，波动性大。
    *   只用了一部分数据训练，可能高估测试误差。

#### 5.2.2 K-折交叉验证 (K-Fold CV)

1.  **步骤**:
    *   将数据随机分为 K 个大小相等的子集（“折”）。
    *   进行 K 次循环：每次用 1 个折作为验证集，其余 K-1 个折作为训练集。
    *   将 K 次的测试误差取平均，得到最终的 CV 误差。
2.  **常用 K 值**: 5 或 10。
3.  **优点**:
    *   比验证集方法更稳定，方差更小。
    *   利用了所有数据进行训练和验证。
4.  **特例：留一交叉验证 (LOOCV)**: 当 K=n 时，即为 LOOCV。
    *   **优点**: 偏差非常小，因为几乎用了所有数据来训练。
    *   **缺点**: 计算量巨大，且方差可能较高。

### 5.3 自助法 (Bootstrap)

1.  **步骤**:
    *   从原始 n 个样本的数据集中，进行 **n 次有放回的抽样**，形成一个自助样本集。
    *   重复此过程 B 次，得到 B 个自助样本集。
    *   在每个自助样本集上计算所需的统计量（如回归系数）。
2.  **用途**: 主要用于估计统计量的**不确定性**（如标准误），而不是直接估计测试误差。因为自助样本与原始样本有大量重叠（约 2/3），直接用它评估测试误差会严重低估。

### 5.4 方法比较

| 方法 | 数据利用率 | 计算成本 | 稳定性 | 适用场景 |
| :--- | :--- | :--- | :--- | :--- |
| **验证集法** | 低 | 低 | 对数据划分敏感 | 大数据集或快速评估 |
| **K折交叉验证** | 较高 | 中等 (随K值增长) | 稳定性较高 | 中小数据集 |
| **留一法交叉验证** | 最高 | 极高 | 可能存在过拟合 | 小数据集 |

### 第五章 重抽样方法 (补充)

#### 5.1 重抽样的核心目的

重抽样方法并非一种新的模型类型，而是一套不可或缺的**工具**，用于评估和选择模型。在实践中，我们通常没有一个足够大的、独立的“真实”测试集来评估模型性能。重抽样方法通过从原始训练数据中巧妙地、重复地抽取样本，来模拟“未知数据”上的表现。其主要有两个核心目的：

1.  **模型评估 (Model Assessment)**: 估计一个给定模型在未知数据上的**测试误差**。这告诉我们最终选定的模型性能到底有多好。
2.  **模型选择 (Model Selection)**: 在多个候选模型或一个模型的不同复杂度（由调节参数$\lambda$, K等控制）之间，选择那个具有最低**估计测试误差**的模型。

#### 5.2 交叉验证 (Cross-Validation)

交叉验证是估计测试误差最常用、最强大的方法。

##### 5.2.1 验证集方法 (Validation Set Approach)

这是最简单的交叉验证形式。

*   **概念定义**:
    将原始数据集**随机**地划分为两个不相交的部分：**训练集 (Training Set)** 和 **验证集 (Validation Set)**。

*   **执行步骤**:
    1.  **划分**: 将数据随机分成两半（或其他比例，如70%-30%）。
    2.  **训练**: 在**训练集**上拟合模型。
    3.  **验证**: 在**验证集**上计算模型的性能指标（如MSE或误分类率），并将此结果作为对真实测试误差的估计。

*   **图解**:
    ```
    原始数据: [1, 2, 3, ..., n]
         ↓ (随机划分)
    训练集: [7, 22, 13, ...] (例如，n/2个样本)
    验证集: [1, 91, 8, ...]  (例如，n/2个样本)
    ```

*   **详细分析缺点**:
    1.  **测试误差估计具有高方差**:
        *   **解释**: 验证集上的MSE估计值**对数据的随机划分非常敏感**。如果你换一个随机种子重新划分数据，得到的训练集和验证集会完全不同，计算出的MSE估计值也可能会有很大差异。这种不稳定性（即高方差）使得单次验证集的结果并不可靠。
    2.  **测试误差估计具有高偏差 (会高估测试误差)**:
        *   **解释**: 这个问题在QA笔记中被明确提出。假设我们将数据对半划分，那么模型只在50%的数据上进行训练。通常情况下，用更少的数据训练出的模型性能会更差（因为它没有学习到足够的信息，可能欠拟合）。因此，这个在“缩水”的训练集上得到的模型，在验证集上的表现会比“用全部数据训练的模型”在真实新数据上的表现要差。这意味着，验证集方法得到的MSE**系统性地高估了**我们最终想要了解的那个模型的真实测试误差。

##### 5.2.2 留一交叉验证 (Leave-One-Out Cross-Validation, LOOCV)

LOOCV试图解决验证集方法的缺点。

*   **概念定义**:
    对于一个包含 n 个样本的数据集，进行 n 次拟合。每次拟合时，选择**一个**样本作为验证集，其余 **n-1** 个样本作为训练集。

*   **执行步骤**:
    1.  对于 $i = 1, 2, ..., n$:
        *   将第 i 个观测点 $(x_i, y_i)$ 作为验证集。
        *   用剩下的 n-1 个观测点作为训练集来拟合模型。
        *   计算模型在第 i 个观测点上的预测误差，例如 $\text{MSE}_i = (y_i - \hat{y}_i)^2$。
    2.  最终的LOOCV测试误差估计是这 n 个误差的平均值。

*   **公式**:
    $$ CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{MSE}_i $$

*   **详细分析优缺点**:
    *   **优点**:
        1.  **低偏差**: 每次训练都使用了 n-1 个样本，这个数量非常接近于原始数据集的大小 n。因此，LOOCV对真实测试误差的估计是**近似无偏**的。这是它相对于验证集法和k-折CV的最大优势。
        2.  **无随机性**: 划分方式是固定的，没有随机抽样的过程。因此，重复运行LOOCV会得到完全相同的结果。

    *   **缺点**:
        1.  **计算成本高**: 需要拟合模型 n 次。如果 n 很大且模型复杂，这将非常耗时。
            *   **重要特例**: 对于**最小二乘线性回归**和**多项式回归**，存在一个计算捷径，使得LOOCV的计算成本约等于只拟合一次模型。该捷径公式为：
                $$ CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2 $$
                其中 $\hat{y}_i$ 是在**全部数据**上拟合得到的预测值，$h_i$ 是第 i 个观测的**杠杆值 (leverage)**。这个公式使得我们无需真的拟合n次模型。
        2.  **测试误差估计具有高方差**: 这一点非常重要且有些反直觉。虽然LOOCV的估计偏差很小，但其估计值本身的**波动性可能很大**。因为 n 个训练集之间高度相似（每个训练集都包含了几乎相同的 n-1 个点），所以它们训练出的模型也高度相似，导致计算出的 n 个 $\text{MSE}_i$ 是**高度正相关**的。对一堆高度相关的数求平均，并不能有效地降低方差。

##### 5.2.3 k-折交叉验证 (k-Fold Cross-Validation)

k-折CV是LOOCV和验证集方法之间的一个折中，也是实践中最常用的方法。

*   **概念定义**:
    将数据随机划分为 k 个大小基本相等的、不相交的子集（称为“折”，fold）。

*   **执行步骤**:
    1.  将数据随机分成 k 折。
    2.  对于 $j = 1, 2, ..., k$:
        *   将第 j 折作为验证集。
        *   用其余 k-1 折作为训练集来拟合模型。
        *   计算模型在第 j 折上的预测误差 $\text{MSE}_j$。
    3.  最终的k-折CV测试误差估计是这 k 个误差的平均值。

*   **公式**:
    $$ CV_{(k)} = \frac{1}{k} \sum_{j=1}^{k} \text{MSE}_j $$

*   **图解 (以 k=5 为例)**:
    ```
    Fold 1: [验证] [训练] [训练] [训练] [训练]  -> 得到 MSE₁
    Fold 2: [训练] [验证] [训练] [训练] [训练]  -> 得到 MSE₂
    Fold 3: [训练] [训练] [验证] [训练] [训练]  -> 得到 MSE₃
    Fold 4: [训练] [训练] [训练] [验证] [训练]  -> 得到 MSE₄
    Fold 5: [训练] [训练] [训练] [训练] [验证]  -> 得到 MSE₅
    
    最终 CV 误差 = (MSE₁ + ... + MSE₅) / 5
    ```

*   **详细分析优缺点 (偏差-方差权衡)**:
    k-折CV在偏差和方差之间取得了绝佳的平衡。
    *   **偏差**: k-折CV的训练集大小为 $(k-1)/k \times n$。这个大小比验证集法（通常是 $n/2$）大，但比LOOCV（$n-1$）小。因此，其测试误差估计的偏差介于两者之间，通常在一个可接受的水平。
    *   **方差**: k-折CV的 k 个训练集之间的重叠部分比LOOCV的 n 个训练集要少得多。这意味着 k 个模型之间的相关性较低，因此对它们的 k 个预测误差求平均能够**有效地降低方差**。
    *   **经验法则**: 在实践中，k=5 或 k=10 被证明在偏差-方差权衡上能提供很好的结果，因此被广泛使用。

##### 5.2.4 交叉验证用于分类问题

当用于分类问题时，交叉验证的流程完全相同，只是性能度量从MSE变为**误分类率**。例如，LOOCV的误差估计公式变为：
$$ CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{Err}_i \quad \text{其中 } \text{Err}_i = I(y_i \neq \hat{y}_i) $$

#### 5.3 自助法 (Bootstrap)

Bootstrap是一种非常不同但同样强大的重抽样方法。

*   **核心思想与步骤**:
    1.  **抽样**: 从大小为 n 的原始数据集中，进行 **n 次有放回的随机抽样**，创建一个大小同样为 n 的“自助样本集”。
    2.  **重复**: 重复上述抽样过程 B 次（例如 B=1000），得到 B 个不同的自助样本集。
    3.  **计算**: 在每个自助样本集上计算我们感兴趣的统计量（如某个系数的估计值、一个模型的预测值等）。

*   **主要应用：估计统计量的不确定性**
    Bootstrap最强大的应用是**估计几乎任何统计量的标准误**。交叉验证无法做到这一点。
    *   **示例**: 假设我们想估计某个回归系数 $\alpha$ 的标准误。
        1.  生成 B 个自助样本集。
        2.  在每个自助样本集上拟合模型，得到 B 个该系数的估计值: $\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, ..., \hat{\alpha}^{*B}$。
        3.  $\alpha$ 的标准误的Bootstrap估计就是这 B 个估计值的标准差：
            $$ SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1} \sum_{r=1}^{B} \left( \hat{\alpha}^{*r} - \bar{\alpha}^* \right)^2} $$
            其中 $\bar{\alpha}^*$ 是 B 个估计值的平均值。

*   **Bootstrap 与模型评估**
    *   **局限性**: Bootstrap**不适合**像交叉验证那样直接用于估计测试误差。因为每个自助样本与原始数据集有大量重叠（约63.2%），如果用原始数据集作为验证集，模型已经在训练中“偷看”了大部分数据，会导致严重低估真实测试误差。
    *   **OOB (Out-of-Bag) 估计**: 这是Bootstrap用于模型评估的巧妙方法。
        *   **概念**: 对于每个自助样本，大约有 1/3 的原始数据点没有被抽中。这些点被称为**袋外 (Out-of-Bag, OOB)** 样本。
        *   **应用**: 对于第 i 个原始数据点，我们可以找到所有**不包含**它的自助样本集（约有 B/3 个），用在这些样本集上训练出的模型来对第 i 个点进行预测，然后计算其误差。对所有 n 个数据点重复此过程并求平均，就得到了**OOB误差**。
        *   **结论**: OOB误差是对真实测试误差的一个有效的无偏估计，其效果类似于LOOCV，但计算成本要低得多。它在**装袋法 (Bagging)** 和 **随机森林 (Random Forest)** 等集成学习方法中被广泛使用。

---
#### 附：自助样本中约含 2/3 原始数据的推导

*   **问题**: 证明为什么在大小为 n 的数据集中进行 n 次有放回抽样，最终的自助样本约包含 2/3 的原始数据点。
*   **推导过程**:
    1.  **单次抽样**: 在一次有放回的抽样中，某个特定观测点 $x_i$ **不被**抽中的概率是 $\frac{n-1}{n} = 1 - \frac{1}{n}$。
    2.  **n次抽样**: 由于每次抽样都是独立的，该观测点 $x_i$ 在 **n 次**抽样中**都未被**抽中的概率是 $(1 - \frac{1}{n})^n$。
    3.  **极限**: 当 n 足够大时，我们需要求解极限 $\lim_{n \to \infty} (1 - \frac{1}{n})^n$。
        *   根据重要的极限公式 $\lim_{x \to \infty} (1 + \frac{1}{x})^x = e$，我们可以进行换元。
        *   令 $m = -n$，则当 $n \to \infty$ 时，$m \to -\infty$。
        *   原式变为 $\lim_{m \to -\infty} (1 + \frac{1}{m})^m = e^{-1}$。
    4.  **结论**:
        *   一个观测点**不**出现在自助样本中的概率约为 $e^{-1} \approx 0.368$。
        *   因此，一个观测点**出现**在自助样本中的概率约为 $1 - e^{-1} \approx 0.632$。
        *   这意味着，平均而言，每个自助样本包含了约 **63.2%** (约 2/3) 的原始数据点。

---

## 第六章 线性模型选择与正则化

目标：通过限制或筛选变量，提高模型的**预测准确率**和**模型解释力**。



### 6.1 子集选择

通过从 p 个预测变量中选择一个子集来构建模型。

1.  **最优子集选择**:
    *   **方法**: 遍历所有 $2^p$ 种可能的变量组合，为每种组合拟合一个模型，然后根据某个标准选出最优模型。
    *   **缺点**: 计算量巨大，当 p > 40 时基本不可行。
2.  **逐步选择**:
    *   **向前逐步选择**: 从空模型开始，每次添加一个能最大程度改善模型拟合的变量。
    *   **向后逐步选择**: 从包含所有变量的全模型开始，每次移除一个对模型最无益的变量。
    *   **优点**: 计算效率远高于最优子集选择。
    *   **缺点**: 不能保证找到全局最优解。

#### 选择最优模型的标准

当比较不同大小（不同变量数）的模型时，不能只用 RSS 或 R²，因为它们总会偏爱变量更多的模型。需要使用考虑了模型复杂度的惩罚项的指标：

*   **Cp, AIC**: 对模型复杂度进行惩罚。值越小越好。
*   **BIC**: 惩罚力度比 AIC 更大，倾向于选择更简单的模型。值越小越好。
*   **调整 R²**: 对 R² 进行惩罚。值越大越好。

### 6.2 压缩/正则化方法 (Shrinkage)

通过向最小二乘的目标函数中添加一个惩罚项，来“压缩”回归系数的大小。

#### 6.2.1 岭回归 (Ridge Regression, L2 正则化)

*   **目标函数**:
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\} = \text{RSS} + \lambda \sum \beta_j^2
    $$
*   **特点**:
    *   惩罚项是系数的**平方和** (L2 范数)。
    *   可以将系数**压缩**得非常接近 0，但**不会**使其恰好等于 0。
    *   能有效处理共线性问题。
    *   调节参数 $\lambda$ 控制压缩程度：$\lambda$ 越大，压缩越强，模型越简单（方差减小，偏差增大）。

#### 6.2.2 Lasso (L1 正则化)

*   **目标函数**:
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\} = \text{RSS} + \lambda \sum |\beta_j|
    $$
*   **特点**:
    *   惩罚项是系数的**绝对值之和** (L1 范数)。
    *   可以将某些系数**精确地压缩到 0**，从而实现**自动变量选择**。
    *   产生的模型更稀疏，更易于解释。

#### 几何解释



*   椭圆是 RSS 的等高线，中心是最小二乘解 $\hat{\beta}$。
*   目标是找到等高线与约束区域（灰色区域）的第一个接触点。
*   **Lasso (左图)**: 约束区域是菱形，有尖角。等高线很可能在坐标轴的角点处接触，导致某个系数为 0。
*   **岭回归 (右图)**: 约束区域是圆形，没有尖角。接触点通常不会在坐标轴上，所以系数不为 0。

### 6.3 降维方法

将 p 个预测变量投影到一个 M 维子空间（M < p），然后用这 M 个投影作为新的预测变量来拟合模型。

1.  **主成分回归 (PCR)**:
    *   **方法**: 通过**主成分分析 (PCA)** 构造 M 个主成分，然后用这 M 个主成分进行线性回归。
    *   **特点**: PCA 是**无监督**的，它在寻找主成分时只考虑了 X 的方差，没有利用 Y 的信息。
2.  **偏最小二乘 (PLS)**:
    *   **方法**: 同样构造 M 个新特征，但在构造时同时考虑了 X 和 Y 的信息。
    *   **特点**: PLS 是**有监督**的，它寻找的方向不仅能很好地解释 X，也能很好地解释 Y。

### R 语言代码

```R
library(leaps)
regfit.full <- regsubsets(Y ~ ., data = mydata, nvmax = 10)
library(glmnet)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid)
library(pls)
pcr.fit <- pcr(Y ~ ., data = mydata, scale = TRUE, validation = "CV")
pls.fit <- plsr(Y ~ ., data = mydata, scale = TRUE, validation = "CV")
```

### 第六章 线性模型选择与正则化 (补充)

本章的核心是解决标准最小二乘回归的两个主要局限性：
1.  **预测准确性**: 当预测变量数量 p 很大，尤其是接近或超过样本量 n 时，最小二乘模型极易过拟合，导致在测试集上表现很差（高方差）。此外，包含无关的“噪声”变量也会不必要地增加模型方差。
2.  **模型解释性**: 当包含大量预测变量时，模型变得难以解释。我们希望通过移除无关变量，只保留一个与响应有强相关的变量小子集，来构建一个更简洁、更易于理解的模型。

为了克服这些局限，本章介绍了三类核心方法：**子集选择**、**压缩（正则化）**和**降维**。

---

#### 一、 子集选择 (Subset Selection)

##### 1. 最优子集选择 (Best Subset Selection)

*   **目标**: 找到所有 $2^p$ 个可能模型中“最好”的那个。
*   **算法步骤**:
    1.  **生成各大小下的最优模型**:
        *   记 $M_0$ 为不含任何预测变量的零模型（只含截距）。
        *   对 $k = 1, 2, ..., p$：
            *   拟合所有 $C_p^k$ 个包含 k 个预测变量的模型。
            *   在这些模型中，根据**训练RSS**（越小越好）或**训练R²**（越大越好），选出最优的那个，记为 $M_k$。
    2.  **在不同大小的模型间选择**:
        *   现在我们有了 p+1 个候选模型：$M_0, M_1, ..., M_p$。
        *   由于训练RSS和R²会系统性地偏好变量更多的模型，我们不能用它们来比较大小不同的模型。
        *   必须使用一种能估计**测试误差**的标准，如**Cp、AIC、BIC、调整R²**或**交叉验证**，从这 p+1 个模型中选出最终的全局最优模型。
*   **优缺点**:
    *   **优点**: 算法简单直接，能保证找到每个大小 k 下的全局最优模型。
    *   **缺点**:
        *   **计算成本极高**: 当 p 较大时（如 p>40），需要评估的模型数量 $2^p$ 会呈指数级爆炸，计算上不可行。
        *   **统计上的问题**: 由于搜索空间巨大，模型容易拟合到训练数据中的偶然噪声，导致**过拟合**和**高方差**。

##### 2. 逐步选择 (Stepwise Selection)

为了解决最优子集选择的计算问题，逐步选择采用贪心算法，不搜索所有模型。

*   **向前逐步选择 (Forward Stepwise Selection)**
    *   **算法步骤**:
        1.  从零模型 $M_0$ 开始。
        2.  对 $k = 0, 1, ..., p-1$：
            *   考虑在当前模型 $M_k$ 的基础上，再加入一个变量的所有可能模型。
            *   选择那个能使 RSS 最小（或 R² 最大）的模型，作为新的最优模型 $M_{k+1}$。
        3.  像最优子集选择一样，最后使用 Cp、AIC、BIC 或交叉验证从 $M_0, ..., M_p$ 中选择最终模型。
    *   **特点**: 计算效率高（只评估约 $p^2/2$ 个模型），即使在 $p > n$ 的情况下也能使用。但因为它是一种贪心算法，不能保证找到全局最优解。

*   **向后逐步选择 (Backward Stepwise Selection)**
    *   **算法步骤**:
        1.  从包含所有 p 个变量的全模型 $M_p$ 开始。
        2.  对 $k = p, p-1, ..., 1$：
            *   考虑从当前模型 $M_k$ 中移除一个变量的所有可能模型。
            *   选择那个 RSS 最小（或 R² 最大）的模型，作为新的最优模型 $M_{k-1}$。
        3.  同样，最后使用测试误差估计标准在 $M_0, ..., M_p$ 中选择最终模型。
    *   **特点**: 计算成本比向前选择高，且要求样本量 $n > p$（因为第一步需要拟合全模型）。

##### 3. 选择最优模型的标准（公式与详解）

*   **Cp / AIC (赤池信息量准则)**
    *   **公式**:
        $$ C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2) $$
    *   **详解**: 该准则在训练误差（RSS）的基础上增加了一个与模型变量数 $d$ 成正比的惩罚项。$2d\hat{\sigma}^2$ 是对因过拟合而导致的训练误差偏低的惩罚调整。我们选择Cp值最小的模型。AIC在最小二乘回归的设定下与Cp是成比例的，因此效果相同。

*   **BIC (贝叶斯信息准则)**
    *   **公式**:
        $$ BIC = \frac{1}{n}(RSS + \log(n)d\hat{\sigma}^2) $$
    *   **详解**: BIC 的惩罚项是 $\log(n)d\hat{\sigma}^2$。当样本量 $n > e^2 \approx 7.4$ 时，$\log(n) > 2$，这意味着 BIC 对复杂模型的惩罚比 AIC 更重。因此，BIC 倾向于选择比 AIC 更简洁（变量更少）的模型。

*   **调整R² (Adjusted R²)**
    *   **公式**:
        $$ \text{Adjusted } R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)} $$
    *   **详解**: 普通的 R² 随变量增多只会单调不减。调整 R² 通过各自的自由度对 RSS 和 TSS 进行了调整。当模型中加入一个与响应无关的变量时，RSS 的减小会非常有限，不足以抵消分母中自由度 ($n-d-1$) 的减小，从而导致调整 R² 下降。我们选择调整 R² 最大的模型。

---

#### 二、 压缩/正则化方法 (Shrinkage/Regularization)

##### 1. 岭回归 (Ridge Regression, L2 正则化)

*   **目标函数**: 最小化以下表达式：
    $$ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^{p} \beta_j^2 $$
*   **核心思想**: 在最小二乘的目标函数上增加了一个“压缩惩罚项” $\lambda \sum \beta_j^2$（即系数向量的L2范数的平方）。这个惩罚项会使得系数的估计值向0“收缩”。
*   **调节参数 λ**:
    *   λ 是一个非负的调节参数，控制着压缩的强度，需要通过**交叉验证**来确定。
    *   当 λ = 0 时，惩罚项消失，岭回归等价于最小二乘。
    *   当 λ → ∞ 时，惩罚项的作用占主导，所有系数 $\beta_j$ (除截距$\beta_0$外) 都会趋近于0。
*   **为何要对变量进行标准化？**
    *   岭回归的L2惩罚项对所有系数“一视同仁”地进行惩罚。但如果预测变量的尺度（单位）不同，例如 `income` (单位：美元) 和 `age` (单位：岁)，它们的系数大小本身就不具可比性。`income` 的系数可能很小，`age` 的系数可能很大，但并不代表 `age` 更重要。此时，惩罚项会不成比例地压缩数值较大的系数。
    *   **标准化**（将每个预测变量都转换为均值为0，标准差为1）可以消除这种尺度效应，使得所有变量处于同一起跑线上，惩罚才变得公平。
*   **特点**: 岭回归可以有效地处理**共线性**问题，并且通过牺牲少量偏差来大幅降低方差，从而提高预测准确性。但它不会将任何系数精确地压缩到0，因此**不能用于变量选择**。

##### 2. Lasso (L1 正则化)

*   **目标函数**: 最小化以下表达式：
    $$ \sum_{i=1}^{n} (y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij})^2 + \lambda \sum_{j=1}^{p} |\beta_j| = \text{RSS} + \lambda \sum_{j=1}^{p} |\beta_j| $$
*   **核心思想**: 与岭回归类似，但惩罚项换成了系数向量的**L1范数** $\lambda \sum |\beta_j|$。
*   **关键特性**: L1惩罚的一个神奇特性是，当 λ 足够大时，它能够将某些系数**精确地压缩到0**。
*   **优点**: Lasso 同时实现了系数压缩和**自动变量选择**，从而能够产生**稀疏模型 (sparse model)**。稀疏模型只包含一部分预测变量，因此更易于解释。

##### 3. 岭回归 vs. Lasso 的深入对比

*   **几何解释**:
    
    *   **等价约束形式**:
        *   岭回归: $\text{minimize } RSS \quad \text{subject to} \quad \sum \beta_j^2 \le s$
        *   Lasso: $\text{minimize } RSS \quad \text{subject to} \quad \sum |\beta_j| \le s$
    *   **约束区域**:
        *   岭回归的约束区域是一个**圆形**（p=2时）或超球面，边界光滑。
        *   Lasso的约束区域是一个**菱形**（p=2时）或多面体，边界在坐标轴上有**尖角**。
    *   **稀疏性来源**: RSS的等高线是椭圆。当椭圆扩张并首次接触约束区域时，就找到了解。对于Lasso的菱形区域，接触点很可能发生在坐标轴上的**尖角处**，此时其中一个系数（如$\beta_1$）恰好为0。而对于岭回归的圆形区域，由于边界光滑，接触点几乎不可能恰好发生在坐标轴上，所以系数只会接近0而不会等于0。

*   **适用场景**:
    *   **岭回归**: 当你认为**大部分**预测变量都对响应有一定贡献，且它们的系数大小比较平均时，岭回归通常表现更好。
    *   **Lasso**: 当你认为只有**少数**预测变量是真正重要的，而其他大部分变量是无关噪声时（即真实模型是稀疏的），Lasso表现更好，因为它能有效地剔除这些噪声变量。

---

#### 三、 降维方法 (Dimension Reduction)

与前两种方法不同，降维方法不直接在原始的p个变量上操作，而是先将它们转化为一个M个新变量的低维集合（M < p），然后用这M个新变量来拟合模型。

##### 1. 主成分回归 (PCR)

*   **算法步骤**:
    1.  **降维**: 使用**主成分分析 (PCA)** 从原始的p个预测变量中，提取出前M个方差最大的、互不相关的主成分 $Z_1, Z_2, ..., Z_M$。
    2.  **回归**: 将这M个主成分作为新的预测变量，对响应变量Y进行标准的最小二乘回归。
*   **核心特点**: PCA是一个**无监督 (unsupervised)** 的降维技术。它在寻找主成分时，只考虑了预测变量X自身的结构（最大化方差），完全没有利用响应变量Y的信息。这可能导致一个问题：方差最大的方向不一定是与Y最相关的方向。
*   **数据预处理**: 在进行PCR之前，对原始变量进行**标准化**（零均值，单位方差）是标准操作。

##### 2. 偏最小二乘 (Partial Least Squares, PLS)

*   **与PCR的区别**: PLS是一种**有监督 (supervised)** 的降维技术。
*   **核心思想**: PLS在构建新的成分（方向）时，不仅考虑了要能很好地解释原始预测变量X（像PCA那样），还同时考虑了要与响应变量Y有很强的相关性。它寻找的方向是X和Y协方差最大的方向。
*   **结果**: 因为利用了Y的信息，PLS通常能用更少的成分获得比PCR更好的预测性能。它在偏差和方差之间找到了一个与PCR不同的权衡。

---

## 第七章 非线性模型

### 7.1 多项式回归

通过添加预测变量的多项式项（如 $X^2, X^3$）来扩展线性模型，以拟合非线性关系。
$$
y = \beta_0 + \beta_1 X + \beta_2 X^2 + \dots + \beta_d X^d + \epsilon
$$

### 7.2 回归样条 (Regression Splines)

*   **思想**: 将 X 的取值范围划分为多个区域，在每个区域内拟合一个独立的多项式函数。这些函数在分割点（称为**结点, knots**）处被约束，使其平滑连接。
*   **三次样条**: 最常用，它在结点处约束函数本身、一阶导数和二阶导数都连续。一个有 K 个结点的三次样条需要估计 K+4 个系数。

### 7.3 光滑样条 (Smoothing Splines)

*   **思想**: 在所有可能的函数中，寻找一个既能很好地拟合数据，又足够平滑的函数 g(x)。
*   **目标函数**:
    $$
    \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - g(x_i))^2 + \lambda \int (g''(t))^2 dt \right\}
    $$
    *   第一项是 RSS，衡量拟合优度。
    *   第二项是粗糙度惩罚项，衡量函数的“弯曲”程度。$\lambda$ 是调节参数，控制平滑度。

### 7.4 广义可加模型 (GAM)

将多元线性回归中的每个线性项替换为一个非线性函数，但保持模型的可加性。
$$
y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i
$$
*   **优点**:
    *   能自动建模非线性关系，比线性模型更灵活。
    *   由于模型是可加的，仍然可以分析每个变量对响应的独立影响，比其他完全非线性的模型（如随机森林）更具可解释性。

### R 语言代码

```R
fit.poly <- lm(Y ~ poly(X, 3), data = mydata)
library(splines)
fit.spline <- lm(Y ~ bs(X, knots = c(25, 50, 75)), data = mydata)
fit.smooth <- smooth.spline(x, y, df = 16)
library(gam)
gam.fit <- gam(Y ~ s(X1, 4) + s(X2, 5) + X3, data = mydata)
```

### 第七章 非线性模型 (补充)

本章的核心是突破第三章线性模型的局限，处理预测变量和响应变量之间的非线性关系。主要方法包括多项式回归、阶梯函数、样条和广义可加模型。

#### 1. 多项式回归 (Polynomial Regression)

*   **概念定义**:
    通过向简单线性模型中添加预测变量 $X$ 的高次项（如 $X^2, X^3, \dots, X^d$）来拟合非线性曲线。
*   **公式**:
    $$ y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \dots + \beta_d x_i^d + \epsilon_i $$
*   **详细讲解**:
    *   **本质**: 多项式回归本质上是**多元线性回归**的一个特例。我们可以创建新的预测变量 $X_1=x, X_2=x^2, \dots, X_d=x^d$，然后用这些新变量进行标准的多元线性回归。因此，所有线性回归的理论、假设检验（t检验、F检验）和评估指标（R²、RSE）都同样适用。
    *   **最高阶数d的选择**:
        *   **问题**: 阶数 `d` 应该设多高？
        *   **回答**: `d` 是一个重要的调节参数，控制着模型的灵活性。
            *   较小的 `d` (如2或3) 可以捕捉简单的曲线关系，如U形。
            *   非常大的 `d` (如10) 会导致拟合出的曲线**过于“扭曲”和不规则**，尤其是在数据范围的**边界处**，会产生剧烈的震荡。这种现象是**过拟合**的典型表现，模型在训练集上表现很好，但在测试集上表现很差。
        *   **实践准则**: 在实践中，阶数 `d` **一般不大于3或4**，因为更高阶的多项式很少能带来模型性能的显著提升，反而会大大增加不稳定性。`d` 的最优值通常通过交叉验证来确定。

#### 2. 回归样条 (Regression Splines)

多项式回归是**全局**的，一个点的变化会影响整条曲线。样条是**局部**的，提供了更灵活的拟合方式。

*   **概念定义**:
    通过在预测变量 $X$ 的取值范围内设定若干个**结点 (knots)**，将数据划分为多个连续的区域，然后在每个区域内拟合一个独立的低阶多项式（通常是三次多项式）。为了使整条曲线平滑，会在结点处施加**连续性约束**。
*   **自由度计算 (以三次样条为例)**:
    *   **问题**: 一个有 K 个结点的三次样条为什么有 K+4 个自由度？
    *   **推导过程**:
        1.  **无约束情况**: 如果我们在 K+1 个区域内分别拟合一个三次多项式，每个多项式有4个系数（$\beta_0, \beta_1, \beta_2, \beta_3$），总共需要 `(K+1) * 4` 个参数。
        2.  **施加约束**: 为了使曲线在结点处平滑连接，我们要求在每个结点处：
            *   函数本身是连续的 (曲线不能断开)。
            *   函数的一阶导数是连续的 (曲线不能有尖角)。
            *   函数的二阶导数是连续的 (曲线的弯曲率变化是平滑的)。
        3.  **计算自由度**:
            *   每个结点施加了3个约束。总共有 K 个结点，所以总约束数量为 `3 * K`。
            *   最终的自由度（即需要估计的参数个数） = 初始参数个数 - 总约束个数。
            *   **自由度 = `4 * (K+1) - 3 * K = 4K + 4 - 3K = K + 4`**。
*   **结点 (Knots) 的选择**:
    *   **位置**: 在哪里放置结点？一个简单有效的方法是将结点均匀地放置在数据的**分位数**上。例如，如果需要3个结点，可以把它们放在数据的25%、50%和75%分位数点上。
    *   **数量**: 放置多少个结点？结点的数量决定了样条的灵活性。过少的结点可能导致欠拟合，过多的结点则会导致过拟合。结点的最优数量（或等价地说，自由度）通常通过**交叉验证**来确定。
*   **自然样条 (Natural Splines)**:
    *   **问题**: 普通回归样条在数据边界处的方差通常很大，拟合不稳定。
    *   **解决方法**: 自然样条在普通三次样条的基础上，额外增加了**边界约束**：要求函数在边界结点之外的区域是**线性**的。
    *   **为什么有效**: 这个看似简单的线性约束，极大地减少了边界区域的不确定性，使得拟合结果更加稳定和可靠，通常在实践中表现更好。
*   **与多项式回归的比较**:
    *   **优点**: 样条通过增加结点而不是提高多项式阶数来增加灵活性。这使得样条在提供同等灵活性的同时，比高阶多项式回归**更加稳定**，并且能更好地捕捉数据的**局部变化**。

#### 3. 光滑样条 (Smoothing Splines)

光滑样条提供了一种更“自动化”的样条拟合方法，无需手动选择结点的数量和位置。

*   **概念定义**:
    在所有可能的函数 $g(x)$ 中，寻找一个既能**很好地拟合数据**（RSS小），又足够**平滑**的函数。
*   **目标函数与惩罚项的理解**:
    $$ \underset{g}{\text{minimize}} \left\{ \underbrace{\sum_{i=1}^{n} (y_i - g(x_i))^2}_{\text{损失项 (RSS)}} + \underbrace{\lambda \int (g''(x))^2 dx}_{\text{惩罚项}} \right\} $$
    *   **损失项**: 即残差平方和 (RSS)。这一项要求函数 $g(x)$ 尽可能地靠近所有数据点。如果没有惩罚项，最优解将是一条穿过所有数据点的、极度扭曲的插值曲线。
    *   **惩罚项**: 这是理解光滑样条的关键。
        *   $g''(x)$ 是函数的**二阶导数**，它衡量了函数的**曲率或“弯曲”程度**。一条直线的二阶导数为0，而一条剧烈波动的曲线则有很大的二阶导数。
        *   $\int (g''(x))^2 dx$ 将函数在整个范围内的“总弯曲度”进行积分。这个值越大，函数越“粗糙”。
        *   **调节参数 $\lambda$**: 这是控制偏差-方差权衡的“旋钮”。
            *   **当 $\lambda = 0$**: 惩罚项不起作用。模型会不惜一切代价减小RSS，导致一条穿过所有数据点的插值曲线，即**严重过拟合**。
            *   **当 $\lambda \to \infty$**: 惩罚项的作用变得无穷大。为了最小化目标函数，模型必须选择一个惩罚为0的函数，即 $g''(x)=0$。这意味着 $g(x)$ 必须是一条**直线**，此时光滑样条退化为简单的最小二乘线性回归，可能**欠拟合**。
*   **有效自由度 (Effective Degrees of Freedom)**:
    *   光滑样条在每个数据点 $x_i$ 处都有一个结点，名义上的自由度是 n。但惩罚项 $\lambda$ 限制了这些自由度。
    *   **有效自由度** $df_{\lambda}$ 是一个衡量光滑样条**实际灵活性**的指标。它是一个随 $\lambda$ 变化的数值，当 $\lambda$ 从 0 变化到 $\infty$ 时，$df_{\lambda}$ 从 n 平滑地减小到 2。它比 $\lambda$ 本身更直观，例如，选择 $df_{\lambda}=5$ 的光滑样条，其灵活性与一个有 $5-4=1$ 个结点的三次样条大致相当。

#### 4. 广义可加模型 (Generalized Additive Models, GAMs)

GAM 将非线性模型扩展到了多预测变量的场景，同时巧妙地保留了部分可解释性。

*   **概念定义**:
    GAM 假设响应变量 Y 可以通过每个预测变量的**非线性平滑函数之和**来建模。
*   **公式**:
    $$ y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \dots + f_p(x_{ip}) + \epsilon_i $$
*   **详细讲解**:
    *   **模型结构**: GAM 是多元线性回归的自然推广。它将线性项 $\beta_j x_{ij}$ 替换为更灵活的非线性函数项 $f_j(x_{ij})$。
    *   **构建模块**: 每一个函数 $f_j$ 都可以用本章介绍的任何非线性方法来拟合，最常用的就是**样条**（如自然样条或光滑样条）。例如，我们可以用一个自由度为4的自然样条来拟合 `age`，用一个自由度为5的光滑样条来拟合 `income`。
*   **优缺点**:
    *   **优点**:
        1.  **自动建模非线性**: 无需手动尝试各种变量变换（如log, sqrt, x²）。
        2.  **更灵活准确**: 因为每个变量都可以有自己的非线性关系，模型通常比纯线性模型拟合得更好，预测更准。
        3.  **保持可解释性**: 由于模型的**可加性**，我们可以像在线性模型中一样，**独立地**分析每个预测变量对响应的影响。我们可以绘制每个 $f_j(x_j)$ 的图形，直观地看出该变量与响应之间的非线性关系，同时“控制”了其他变量的影响。
    *   **缺点**:
        1.  **忽略交互作用**: GAM 的主要局限性在于其标准形式是纯可加的，它无法自动捕捉**变量之间的交互作用**。例如，`age` 对 `wage` 的影响可能取决于 `education` 的水平，这种关系无法被标准GAM模型 $f_1(\text{age}) + f_2(\text{education})$ 捕捉。需要手动向模型中添加交互项，如 $f_{12}(\text{age, education})$。

---

## 第八章 基于树的方法

### 8.1 决策树

#### 8.1.1 基本思想

通过一系列的“是/否”问题，将预测变量空间递归地划分为多个简单的区域。对于一个新观测，根据它落入的区域进行预测。

#### 8.1.2 回归树

*   **目标**: 最小化 RSS。
*   **划分准则**: 每次选择一个变量和一个切分点，使得划分后的两个区域的总体 RSS 最小。
*   **预测**: 一个区域内所有观测的预测值是该区域训练样本响应值的平均值。

(左图：对空间的划分。右图：对应的决策树结构。)

#### 8.1.3 分类树

*   **目标**: 使区域“纯度”最高。
*   **划分准则** (纯度度量):
    *   **分类错误率**: $1 - \max_k(\hat{p}_{mk})$
    *   **基尼指数 (Gini Index)**: $G = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})$
    *   **交叉熵 (Cross-Entropy)**: $D = -\sum_{k=1}^{K} \hat{p}_{mk} \log(\hat{p}_{mk})$
    基尼指数和交叉熵对节点纯度更敏感，在实践中更常用。
*   **预测**: 一个区域的预测类别是该区域训练样本中最多的类别。

#### 8.1.4 树的剪枝

*   **问题**: 未经修剪的树容易过拟合。
*   **方法**: **代价复杂性剪枝**。
    1.  先生成一棵很大的树 $T_0$。
    2.  引入一个调节参数 $\alpha$，最小化以下目标函数：
        $$
        \sum_{\text{终端节点m}} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|
        $$
        其中 $|T|$ 是终端节点的数量。$\alpha$ 越大，对复杂树的惩罚越重，剪枝越多。
    3.  通过交叉验证选择最优的 $\alpha$。

### 8.2 集成方法 (Ensemble Methods)

#### 8.2.1 装袋法 (Bagging)

*   **思想**: 通过自助法 (bootstrap) 从原始数据中生成 B 个不同的训练集。在每个训练集上建立一棵决策树，然后将 B 棵树的预测结果进行平均（回归）或投票（分类）。
*   **效果**: 单个决策树方差很大，通过平均可以显著**降低方差**。

#### 8.2.2 随机森林 (Random Forest)

*   **对 Bagging 的改进**: Bagging 中生成的树可能高度相关（因为每次分裂都可能选择同一个强预测变量）。随机森林通过在每次分裂时，**只从 p 个变量中随机选择 m 个 (通常 m ≈ √p) 作为候选变量**，来**去相关**这些树。
*   **效果**: 进一步降低方差，通常比 Bagging 性能更好。

#### 8.2.3 提升法 (Boosting)

*   **思想**: **顺序地**构建树，每棵新树都试图修正前面树的错误。
*   **算法**:
    1.  从一个简单的模型（如预测所有样本为均值）开始。
    2.  计算当前模型的**残差**。
    3.  在残差上拟合一棵新的（通常很小的）树。
    4.  将这棵新树以一个较小的学习率 $\lambda$ 加入到模型中。
    5.  重复 2-4 步 B 次。
*   **效果**: 是一种缓慢学习的方法，能产生高度准确的模型，但可能过拟合。

### R 语言代码

```R
library(tree)
tree.fit <- tree(Y ~ ., data = mydata)
cv.tree.fit <- cv.tree(tree.fit)
prune.fit <- prune.tree(tree.fit, best = 5)
library(randomForest)
rf.fit <- randomForest(Y ~ ., data = mydata, mtry = 6, ntree = 500)
library(gbm)
boost.fit <- gbm(Y ~ ., data = mydata, distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
```

### 第八章 基于树的方法 (补充)

本章的核心是理解决策树的构建、剪枝过程，以及为了克服单个决策树的缺点而发展出的三种强大的集成方法：装袋法、随机森林和提升法。

#### 8.1 决策树 (Decision Tree)

##### 8.1.1 核心思想：递归二叉分裂

决策树通过一种称为**递归二叉分裂 (Recursive Binary Splitting)** 的自上而下的贪婪算法来构建。

*   **递归 (Recursive)**: 该过程可以重复多次。在得到一个分裂后，我们可以在分裂出的子区域上继续进行分裂。
*   **二叉 (Binary)**: 每次分裂都将一个区域恰好分成两个子区域。
*   **分裂 (Splitting)**: 整个过程就是将预测变量空间（所有预测变量可能取值的集合）不断地分割成一系列简单、互不重叠的矩形区域。
*   **自上而下 (Top-down)**: 算法从包含所有训练样本的根节点开始，然后连续分裂，逐步向下生长。
*   **贪婪 (Greedy)**: 在每一步分裂时，算法只选择**当前步骤**能带来最大收益（例如，RSS下降最多）的分裂，而不考虑这个分裂是否会对后续的步骤产生更好的全局结果。

##### 8.1.2 回归树：构建与RSS计算

*   **目标**: 预测一个**定量**的响应变量。
*   **预测方式**: 对于落入某个叶子节点（区域 $R_j$）的测试样本，其预测值 $\hat{y}_{R_j}$ 是该区域内所有**训练样本**响应值的**算术平均值**。
*   **分裂标准：最小化RSS**
    *   **公式**: 树的总RSS是所有叶子节点RSS的和。
        $$ RSS = \sum_{j=1}^{J} \text{RSS}_j = \sum_{j=1}^{J} \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2 $$
    *   **详细计算过程**: 假设我们要在一个节点上进行分裂。算法会遍历**每一个预测变量**（如 $X_1, X_2, \dots, X_p$），并对每一个变量遍历其所有可能的**切分点 s**。对于每一个可能的组合（变量 $X_j$ 和切分点 s），算法都会计算分裂后的两个子区域的RSS之和。
        $$ \text{RSS}_{\text{split}} = \sum_{i: x_{ij} < s} (y_i - \hat{y}_{R_1})^2 + \sum_{i: x_{ij} \geq s} (y_i - \hat{y}_{R_2})^2 $$
        其中 $\hat{y}_{R_1}$ 和 $\hat{y}_{R_2}$ 分别是分裂后两个新区域内训练样本的均值。算法最终会选择那个能使 $\text{RSS}_{\text{split}}$ **最小**的变量和切分点组合作为本次的最佳分裂。

##### 8.1.3 分类树：分裂标准辨析

*   **目标**: 预测一个**定性**的响应变量。
*   **预测方式**: 对落入某个叶子节点的测试样本，其预测类别是该区域内**训练样本**中数量最多的那个类别。
*   **分裂标准**: 由于响应变量是定性的，不能使用RSS。我们使用衡量**节点纯度 (Node Purity)** 的指标。一个纯度高的节点，其内部的样本绝大多数属于同一个类别。

    *   **1. 分类错误率 (Classification Error Rate)**
        *   **公式**: $E = 1 - \max_k(\hat{p}_{mk})$
        *   **含义**: 第m个节点中，不属于数量最多类别的样本所占的比例。
        *   **缺点**: 对节点纯度的变化**不够敏感**，因此很少用于树的生长，主要用于剪枝或最终评估。例如，一个分裂将节点从 `(50% A, 50% B)` 分成 `(30% A, 70% B)` 和 `(70% A, 30% B)`，两个子节点的纯度都提高了，但分类错误率可能没有变化。

    *   **2. 基尼指数 (Gini Index)**
        *   **公式**: $G = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})$
        *   **含义**: 衡量节点的不纯度。其值可以被解释为：从该节点中随机抽取两个样本，它们类别**不一致**的概率。
        *   **特性**: Gini指数对纯度的变化比分类错误率更敏感。如果一个节点是纯的（所有样本都属于一类，某个 $\hat{p}_{mk}=1$），则G=0。如果各类样本均匀混合，G值最大。

    *   **3. 交叉熵 (Cross-Entropy)**
        *   **公式**: $D = -\sum_{k=1}^{K} \hat{p}_{mk} \log(\hat{p}_{mk})$
        *   **含义**: 源于信息论，也称为信息增益。它同样衡量节点的不纯度。
        *   **特性**: 如果节点是纯的，$\hat{p}_{mk}$ 为0或1，则D=0。如果各类均匀混合，D值最大。在数值上，它与基尼指数非常相似。

    **结论**: 在实践中，**基尼指数**和**交叉熵**是生长分类树时首选的分裂标准，因为它们更能区分出“好”的分裂。

##### 8.1.4 剪枝：代价复杂性剪枝

*   **为什么需要剪枝**: 未经限制生长的树会持续分裂，直到每个叶子节点都非常纯净（甚至只包含一个样本）。这样的树在训练集上表现完美（训练误差为0），但它对训练数据**严重过拟合**，具有很高的方差，在测试集上表现会很差。
*   **代价复杂性剪枝 (Cost Complexity Pruning)**
    *   **核心思想**: 在树的“拟合优度”和“复杂度”之间找到一个最佳平衡点。
    *   **目标函数**: 对于一个给定的子树 T，我们最小化一个带惩罚项的函数：
        $$ C_{\alpha}(T) = \sum_{m=1}^{|T|} \sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T| $$
        *   第一项是树的**总RSS**，衡量拟合优度。
        *   $|T|$ 是树的**终端节点数**，衡量树的复杂度。
        *   $\alpha \ge 0$ 是一个**调节参数**。它控制着对复杂度的惩罚力度。
    *   **算法步骤**:
        1.  先生成一棵足够大的树 $T_0$。
        2.  对于一系列的 $\alpha$ 值，找到每一个 $\alpha$ 值所对应的能最小化 $C_{\alpha}(T)$ 的最优子树 $T_{\alpha}$。当 $\alpha=0$ 时，最优子树就是 $T_0$；随着 $\alpha$ 增大，惩罚变重，最优子树的规模会越来越小。
        3.  使用 **K-折交叉验证** 来为这一系列子树评估测试误差。
        4.  选择那个使交叉验证误差最小的 $\alpha$ 值，其对应的子树 $T_{\alpha}$ 就是最终剪枝后的树。

#### 8.2 集成方法 (Ensemble Methods)

集成方法通过构建并组合多个模型来获得比单个模型更好的预测性能。

##### 8.2.1 装袋法 (Bagging)

*   **全称**: **B**ootstrap **agg**regat**ing**。
*   **核心思想**: **降低方差**。单个决策树具有高方差，通过对多个独立（或近似独立）模型的预测结果进行平均，可以有效降低方差。
*   **算法步骤**:
    1.  通过**自助法 (Bootstrap)** 从原始训练集中有放回地抽取 B 个大小相同的训练样本集。
    2.  在**每一个**自助样本集上，独立地构建一棵**未剪枝**的决策树。
    3.  对于一个新的测试样本，用这 B 棵树分别进行预测。
    4.  将 B 个预测结果进行**聚合 (Aggregate)**：
        *   **回归问题**: 取 B 个预测值的**平均值**。
        *   **分类问题**: 进行**多数投票 (Majority Vote)**。

##### 8.2.2 随机森林 (Random Forest)

*   **对Bagging的改进**: Bagging中的B棵树之间可能存在**相关性**。如果数据中存在一个非常强的预测变量，那么大多数自助样本集构建的树，其顶部分裂很可能都会选择这个强变量。这导致树的结构相似，从而限制了平均操作降低方差的效果。
*   **核心机制**: **特征随机化**。随机森林在Bagging的基础上增加了一个关键步骤：在构建每棵树的**每一次分裂**时，算法不再从全部p个预测变量中寻找最优分裂点，而是先**随机抽取一个包含m个预测变量的子集**，然后只在这个子集中寻找最优分裂点。
*   **参数m**:
    *   m是每次分裂时抽取的变量数，是随机森林的关键调节参数。
    *   通常的经验取值是：分类问题 $m \approx \sqrt{p}$，回归问题 $m \approx p/3$。
    *   当 $m=p$ 时，随机森林就退化成了Bagging。
*   **效果**: 通过强制每次分裂只考虑一部分随机选择的变量，随机森林有效地**降低了树之间的相关性**，使得集成模型更加稳健，通常能获得比Bagging更好的性能。

##### 8.2.3 提升法 (Boosting)

*   **核心思想**: **顺序学习，逐步求精**。Boosting不是像Bagging那样并行地构建独立的树，而是**顺序地**构建树，每一棵新树都致力于**修正**前面所有树共同犯下的错误。
*   **算法步骤 (以回归为例)**:
    1.  **初始化**: 将模型 $\hat{f}(x)$ 初始化为0，将残差 $r_i$ 初始化为真实值 $y_i$。
    2.  **迭代 B 次**: 对于 $b = 1, 2, \dots, B$:
        a.  使用当前的**残差**作为响应变量，在数据 $(X, r)$ 上拟合一棵决策树 $\hat{f}^b(x)$。这棵树通常是比较小的“树桩 (stump)”，其深度 d (交互深度) 是一个调节参数。
        b.  将这棵新树以一个较小的学习率 $\lambda$ **加入**到现有模型中，进行更新：
            $$ \hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x) $$
        c.  更新残差，为下一棵树的拟合做准备：
            $$ r_i \leftarrow r_i - \lambda \hat{f}^b(x_i) $$
    3.  **输出**: 最终的模型是所有“压缩”后的小树的加和：
        $$ \hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}^b(x) $$
*   **三个关键调节参数**:
    *   **B (树的数量)**: 树的总数。与Bagging和随机森林不同，B不是越大越好。因为Boosting是逐步拟合残差，B过大会导致模型开始拟合噪声，从而**过拟合**。需要通过交叉验证选择。
    *   **λ (学习率)**: 学习率，也叫压缩参数。它控制了每棵树对最终模型的贡献程度。较小的λ（如0.01）意味着学习过程更“舒缓”，需要更多的树B来达到好的效果，但通常能获得更稳健的模型。
    *   **d (交互深度)**: 每棵树的深度。它控制了模型中允许的变量间交互的阶数。d=1时，每棵树只有一个分裂点，模型是一个**加法模型**，因为每棵树只涉及一个变量，最终模型是各个变量函数的和。

---

## 第九章 支持向量机 (SVM)

### 9.1 最大间隔分类器

*   **适用场景**: 数据线性可分。
*   **思想**: 在所有能将数据完美分开的超平面中，找到那个离两侧数据点**间隔 (margin)** 最大的超平面。
*   **支持向量 (Support Vectors)**: 位于间隔边界上的数据点，它们“支撑”着最大间隔超平面。移动这些点会改变超平面，而移动其他点则不会。


### 9.2 支持向量分类器 (软间隔)

*   **对最大间隔分类器的推广**: 允许一些观测点被错误分类或落在间隔内，以换取更好的鲁棒性。
*   **思想**: 引入**软间隔**，允许分类器“犯一些错误”。
*   **目标函数**: 最大化间隔，同时限制被“违反”的程度。
    $$
    y_i(\beta_0 + \beta_1 x_{i1} + \dots) \ge M(1-\epsilon_i)
    $$
    其中 $\epsilon_i$ 是松弛变量，C 是一个调节成本参数，控制着对“违反”的容忍度。
    *   **小 C**: 间隔宽，容忍度高，允许更多违反。**高偏差，低方差**。
    *   **大 C**: 间隔窄，容忍度低，不允许违反。**低偏差，高方差**。


### 9.3 支持向量机 (SVM) 与核函数

*   **适用场景**: 非线性决策边界。
*   **思想**: **核技巧 (Kernel Trick)**。通过一个称为**核函数**的函数，将数据从原始特征空间隐式地映射到一个更高维的空间，在这个高维空间中数据可能变得线性可分，然后在这个高维空间里应用支持向量分类器。
*   **常用核函数**:
    *   **多项式核**: $K(x_i, x_{i'}) = (1 + \sum_{j=1}^p x_{ij}x_{i'j})^d$
    *   **径向基函数 (RBF) 核**: $K(x_i, x_{i'}) = \exp(-\gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2)$


### 9.4 多分类 SVM

SVM 本质上是二分类器。处理多于两个类别的问题时，常用策略有：

*   **一对一 (One-vs-One, OVO)**: 为每对类别训练一个二分类器，共 $K(K-1)/2$ 个。预测时，让所有分类器投票。
*   **一对余 (One-vs-All, OVA)**: 为每个类别训练一个“该类 vs. 其他所有类”的二分类器，共 K 个。预测时，选择置信度最高的那个分类器。



### R 语言代码

```R
library(e1071)
svm.fit.linear <- svm(Y ~ ., data = mydata, kernel = "linear", cost = 10)
svm.fit.radial <- svm(Y ~ ., data = mydata, kernel = "radial", gamma = 1, cost = 10)
tune.out <- tune(svm, Y ~ ., data = mydata, kernel = "radial",
                 ranges = list(cost = c(0.1, 1, 10), gamma = c(0.5, 1, 2)))
```

### 第九章 支持向量机 (SVM) (补充)

本章介绍一种强大而灵活的分类方法。其核心思想是从最简单的线性可分情况出发，逐步扩展到更复杂的现实场景。

#### 9.1 基础：最大间隔分类器 (Maximal Margin Classifier)

这是SVM最原始、最理想化的形态，适用于**数据完全线性可分**的情况。

*   **1. 什么是超平面 (Hyperplane)?**
    *   **定义**: 在一个 p 维空间中，超平面是一个 p-1 维的平坦仿射子空间。
        *   p=2 (二维空间): 超平面是一条直线。
        *   p=3 (三维空间): 超平面是一个平面。
    *   **公式**: 一个超平面由以下方程定义：
        $$ \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p = 0 $$
    *   **分类功能**: 超平面将p维空间“一分为二”。
        *   如果一个点 $X$ 代入方程，结果 $> 0$，则它在超平面的一侧。
        *   如果结果 $< 0$，则它在超平面的另一侧。
        *   我们可以用 $y_i \in \{-1, 1\}$ 来标记类别。一个完美分割数据的超平面满足：对于所有数据点 $i$，都有 $y_i(\beta_0 + \beta_1 X_{i1} + \dots) > 0$。

*   **2. 什么是最大间隔 (Maximal Margin)?**
    *   **问题**: 对于线性可分的数据，存在无数个可以将其分开的超平面。哪一个是最好的？
    *   **思想**: “最好”的超平面应该是离两侧数据点都尽可能远的那个，因为它对新数据的扰动最不敏感，泛化能力最强。
    *   **定义**:
        *   **间隔 (Margin)**: 位于超平面两侧，平行于超平面且刚好接触到最近数据点的两条“边界线”之间的距离。可以想象成一条尽可能宽的“街道”。
        *   **最大间隔超平面**: 就是能使这个“街道”宽度最大的那个超平面。

*   **3. 谁是支持向量 (Support Vectors)?**
    *   **定义**: 那些**恰好位于间隔边界上**的数据点。
    *   **核心作用**: 它们是唯一对最大间隔超平面的位置起决定性作用的点。可以想象它们像“桩子”一样“支撑”着间隔边界。移动任何一个支持向量，超平面和间隔都会随之改变；而移动非支持向量（远离间隔边界的点）则对模型没有任何影响。

*   **4. 优化问题 (硬间隔)**
    *   **目标**: 找到参数 $\beta_0, \beta_1, \dots, \beta_p$ 来最大化间隔宽度 M。
    *   **公式**:
        $$ \underset{\beta_0, \dots, \beta_p, M}{\text{maximize}} \quad M $$
        **约束条件**:
        1.  $\sum_{j=1}^p \beta_j^2 = 1$ (这是一个归一化约束，用于唯一定义M)
        2.  $y_i(\beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ip}) \ge M$  对于所有 $i=1, \dots, n$
    *   **解释**: 第二个约束条件强制要求**所有**数据点都必须在间隔边界之外，并且在其正确的分类一侧。

*   **缺点**: 这种“硬间隔”分类器过于理想化。现实数据中只要有一个离群点导致数据线性不可分，它就无法工作。

#### 9.2 推广：支持向量分类器 (Support Vector Classifier, SVC)

这是对最大间隔分类器的改进，允许数据存在一定程度的线性不可分，使其更具鲁棒性。

*   **1. “软间隔” (Soft Margin) 的思想**
    *   **核心**: 与其追求完美分割，不如允许一些点“犯规”，即允许它们进入间隔甚至被错误分类，以换取一个更宽、更稳健的间隔。

*   **2. 关键参数：松弛变量 $\epsilon_i$ 与 成本参数 C**
    *   **松弛变量 (Slack Variable) $\epsilon_i$**:
        *   **定义**: 为每个数据点 $i$ 引入一个松弛变量 $\epsilon_i \ge 0$。它衡量了该点“违反”间隔的程度。
        *   **含义**:
            *   **$\epsilon_i = 0$**: 该点位于正确的间隔边界上或之外（表现良好）。
            *   **$0 < \epsilon_i \le 1$**: 该点位于间隔之内，但在超平面的正确一侧（犯规，但未被错分）。
            *   **$\epsilon_i > 1$**: 该点位于超平面的错误一侧（被错分）。
    *   **成本/调节参数 (Cost Parameter) C**:
        *   **定义**: 一个非负的调节参数，可以看作是所有松弛变量总和 $\sum \epsilon_i$ 的“预算上限”。它控制着模型对“犯规”的容忍度，是**偏差-方差权衡**的关键。
        *   **含义**:
            *   **小 C**: 预算充足，容忍度高。模型允许较多的点违反间隔，以换取一个更**宽**的间隔。这会产生一个更简单的模型，**偏差较高，方差较低**。
            *   **大 C**: 预算紧张，容忍度低。模型会尽力避免点违反间隔，导致间隔变**窄**，决策边界更曲折以适应训练数据。这会产生一个更复杂的模型，**偏差较低，方差较高**，容易过拟合。

*   **3. 优化问题 (软间隔)**
    *   **公式**:
        $$ \underset{\beta_0, \dots, \beta_p, \epsilon_1, \dots, \epsilon_n, M}{\text{maximize}} \quad M $$
        **约束条件**:
        1.  $\sum_{j=1}^p \beta_j^2 = 1$
        2.  $y_i(\beta_0 + \dots + \beta_p X_{ip}) \ge M(1 - \epsilon_i)$
        3.  $\epsilon_i \ge 0, \quad \sum_{i=1}^n \epsilon_i \le C$

*   **4. 支持向量 (软间隔情况)**
    *   **定义**: 在软间隔下，支持向量是所有**恰好在间隔边界上**以及**所有违反间隔**（即 $\epsilon_i > 0$）的点。

#### 9.3 最终形态：支持向量机 (Support Vector Machine, SVM)

这是对支持向量分类器的进一步推广，通过**核技巧**使其能够处理**非线性**决策边界。

*   **1. 核心思想：核技巧 (The Kernel Trick)**
    *   **直观理解**: 如果数据在当前维度是线性不可分的，我们可以将其映射到一个更高维度的空间，在这个新空间里它可能就变得线性可分了。例如，一维上无法用一个点分开的数据，映射到二维后可能可以用一条线分开。
    *   **计算问题**: 显式地计算高维映射非常耗时甚至不可行。
    *   **“技巧”所在**: 支持向量分类器的求解过程只依赖于数据点之间的**内积 (inner product)** $\langle x_i, x_{i'} \rangle$。核技巧的核心就是使用一个**核函数 $K(x_i, x_{i'})$** 来直接计算数据点在高维空间中的内积，而**无需实际进行高维映射**。
        $$ K(x_i, x_{i'}) \approx \langle \phi(x_i), \phi(x_{i'}) \rangle $$
        其中 $\phi(x)$ 是那个（我们从未计算过的）高维映射函数。

*   **2. 常用核函数及其参数**
    *   **多项式核 (Polynomial Kernel)**:
        $$ K(x_i, x_{i'}) = (1 + \sum_{j=1}^p x_{ij}x_{i'j})^d $$
        *   **参数 `d`**: 多项式的次数。`d` 越大，决策边界越灵活。
    *   **径向基核函数 (Radial Basis Function, RBF Kernel)**: 最常用、最强大的核函数之一。
        $$ K(x_i, x_{i'}) = \exp(-\gamma \|x_i - x_{i'}\|^2) $$
        *   **原理**: 它的值介于0和1之间，可以看作是两个数据点 $x_i$ 和 $x_{i'}$ 之间的“相似度”度量。如果两点距离很远，核函数值接近0；如果距离很近，核函数值接近1。
        *   **参数 `γ` (gamma)**: 控制单个训练样本的“影响范围”。这是另一个控制偏差-方差权衡的关键参数。
            *   **小 γ**: 影响范围大。决策边界非常**平滑**。模型简单，**偏差高，方差低**。
            *   **大 γ**: 影响范围小（仅影响近邻点）。决策边界非常**曲折**，试图适应每个局部区域。模型复杂，**偏差低，方差高**，容易过拟合。

*   **3. 术语梳理**
    *   **最大间隔分类器**: 理论模型，用于线性可分数据，使用硬间隔。
    *   **支持向量分类器 (SVC)**: 实用模型，用于近似线性可分的数据，使用软间隔，决策边界是线性的。
    *   **支持向量机 (SVM)**: SVC与核技巧的结合，用于非线性数据，决策边界是非线性的。在口语中，SVM常被用来统称这三者。

#### 9.4 实践应用：多分类SVM

SVM本身是一个二分类器。要处理 K > 2 个类别的问题，主要有两种策略：

*   **1. 一对余 (One-vs-All, OVA or OVR)**
    *   **方法**: 针对每个类别，都训练一个“该类 vs. 其他所有类”的二分类器。总共需要训练 **K** 个分类器。
    *   **预测**: 对于一个新样本，K 个分类器都会给出一个分数，选择分数最高的那个分类器对应的类别作为最终预测。

*   **2. 一对一 (One-vs-One, OVO)**
    *   **方法**: 为每**一对**类别训练一个二分类器。总共需要训练 **$K(K-1)/2$** 个分类器。
    *   **预测**: 对于一个新样本，所有 $K(K-1)/2$ 个分类器都进行一次“投票”，得票最多的那个类别成为最终预测。
    *   **优劣**: 当 K 较大时，OVO需要训练的分类器数量多，但每个分类器只在两个类别的数据子集上训练，速度较快。通常 OVO 的表现更稳健，是更常用的策略。
    
---

## 第十章 无监督学习

### 10.1 主成分分析 (PCA)

*   **目标**: **降维**。找到一个新的坐标系，使得数据在新坐标系下的方差得到最大程度的保留。
*   **主成分**: 新坐标系的坐标轴，是原始变量的线性组合，且彼此正交（不相关）。第一主成分是方差最大的方向，第二主成分是与第一主成分正交且方差次大的方向，以此类推。
*   **重要性**: **进行 PCA 前必须对变量进行标准化** (均值为0，方差为1)，否则方差大的变量会主导主成分。

(右图未标准化，Assault变量方差大，主导了第一主成分。左图标准化后，各变量贡献更均衡。)

### 10.2 聚类分析

#### 10.2.1 K-均值聚类 (K-Means)

*   **目标**: 将数据划分为 K 个预先指定的簇。
*   **算法**:
    1.  随机选择 K 个点作为初始的簇中心。
    2.  **分配步骤**: 将每个数据点分配到离它最近的簇中心。
    3.  **更新步骤**: 重新计算每个簇的中心（该簇所有点的均值）。
    4.  重复 2-3 步，直到簇的分配不再变化。
*   **特点**: 算法结果依赖于初始中心的选择，通常需要多次运行并选择最好的结果。

#### 10.2.2 层次聚类 (Hierarchical Clustering)

*   **目标**: 创建一个嵌套的聚类结构，可以用**谱系图 (Dendrogram)** 来可视化。
*   **算法 (凝聚型)**:
    1.  开始时，每个数据点自成一簇。
    2.  找到最相似（距离最近）的两个簇，将它们合并。
    3.  重复第 2 步，直到所有点合并为一个簇。
*   **连接方法 (Linkage)**: 定义簇间距离的方式。
    *   **最长距离法 (Complete)**: 两个簇之间所有点对距离的最大值。
    *   **最短距离法 (Single)**: 两个簇之间所有点对距离的最小值。
    *   **类平均法 (Average)**: 两个簇之间所有点对距离的平均值。
    *   **重心法 (Centroid)**: 两个簇的中心点之间的距离。


### R 语言代码

```R
pr.out <- prcomp(mydata, scale = TRUE)
km.out <- kmeans(mydata, centers = 3, nstart = 20)
hc.complete <- hclust(dist(mydata), method = "complete")
plot(hc.complete)
```

### 第十章 无监督学习 (补充)

无监督学习与有监督学习的核心区别在于，我们处理的数据集没有预先定义的响应变量 Y。我们的目标不是预测一个输出，而是从数据 X 本身中发现有趣的结构或模式。本章主要关注两种核心的无监督学习任务：主成分分析 (降维) 和聚类分析 (发现子群)。

#### 10.1 主成分分析 (Principal Component Analysis, PCA)

PCA 是一种广泛应用的降维技术，旨在用一组较少的不相关变量（主成分）来概括数据中的主要信息。

##### 10.1.1 PCA 的核心思想与两种解释

*   **问题**: PCA 的目标是什么？
*   **详细讲解**:
    1.  **解释一：最大方差方法**
        *   **定义**: 主成分是原始 p 个预测变量的**线性组合**。第一主成分 ($Z_1$) 是所有可能的线性组合中，样本方差最大的那一个。第二主成分 ($Z_2$) 是在与第一主成分正交（不相关）的约束下，方差最大的线性组合，以此类推。
        *   **直观理解**: PCA 试图在高维数据空间中找到一个新的坐标系。新坐标系的第一个轴 ($Z_1$ 的方向) 对准数据点分布最分散的方向，第二个轴 ($Z_2$ 的方向) 对准与第一个轴垂直且数据第二分散的方向。这样，大部分数据信息（方差）就被集中到了前几个主成分上。
    2.  **解释二：最小重构误差方法**
        *   **定义**: 前 M 个主成分构成的 M 维超平面，是所有 M 维超平面中，与原始 n 个数据点**最接近**的一个。
        *   **直观理解**: 想象在三维空间中有一团点云，PCA 找到的最佳二维平面（由前两个主成分定义）是这样一个平面：所有数据点到这个平面的垂直投影距离的平方和最小。这意味着这个平面是数据点的最佳低维“近似”。

##### 10.1.2 关键概念与计算

*   **载荷 (Loadings)**
    *   **定义**: 第 m 个主成分 $Z_m = \sum_{j=1}^{p} \phi_{jm} X_j$ 中，系数 $\phi_{jm}$ 称为第 j 个变量在第 m 个主成分上的**载荷**。
    *   **解释**: 载荷 $\phi_{jm}$ 的绝对值大小，反映了原始变量 $X_j$ 对主成分 $Z_m$ 的贡献程度或重要性。所有载荷构成的**载荷向量** $\phi_m = (\phi_{1m}, ..., \phi_{pm})^T$ 描述了第 m 个主成分的方向。

*   **得分 (Scores)**
    *   **定义**: 将第 i 个观测数据 $x_i = (x_{i1}, ..., x_{ip})$ 代入主成分公式计算得到的值 $z_{im}$，称为第 i 个观测在第 m 个主成分上的**得分**。
    *   **解释**: 得分是原始数据点在新的主成分坐标系下的坐标，是降维后的数据表示。

*   **方差解释比率 (Proportion of Variance Explained, PVE)**
    *   **定义**: 第 m 个主成分的方差占所有原始变量总方差的比例。
    *   **公式**:
        $$ \text{PVE}_m = \frac{\text{Var}(Z_m)}{\sum_{j=1}^{p} \text{Var}(X_j)} = \frac{\frac{1}{n}\sum_{i=1}^{n} z_{im}^2}{\sum_{j=1}^{p} (\frac{1}{n}\sum_{i=1}^{n} x_{ij}^2)} $$
    *   **解释**: PVE 告诉我们每个主成分捕捉了多少原始数据的信息量。

##### 10.1.3 PCA 的关键实践问题

*   **问题1**: PCA 是否需要对数据进行标准化？
    *   **答案**: **必须进行标准化**。
    *   **详细讲解**: PCA 完全依赖于变量的方差来确定主成分。如果变量的单位或尺度不同（例如，一个变量是身高(cm)，另一个是体重(kg)），那么数值范围更大（因而方差也更大）的变量将会主导第一主成分，这使得PCA的结果变得毫无意义。通过将每个变量都**标准化**（使其均值为0，标准差为1），可以消除尺度影响，确保所有变量在PCA分析中处于同等地位。

*   **问题2**: 如何确定保留多少个主成分？
    *   **答案**: 没有唯一的正确答案，但常用的启发式方法是**碎石图 (Scree Plot)**。
    *   **详细讲解**:
        1.  **绘制碎石图**: 将每个主成分的 PVE (或其对应的方差，即特征值) 从大到小依次绘制成折线图。
        2.  **寻找“肘点” (Elbow)**: 观察图形，找到一个点，在该点之后，PVE 的下降趋势明显变缓，形成一个类似“手肘”的拐点。
        3.  **选择**: 通常保留“肘点”之前的所有主成分。这背后的逻辑是，肘点之后的主成分解释的方差很小，更可能是由噪声引起的。

#### 10.2 聚类分析 (Clustering)

聚类是将数据集划分为若干个子群（簇）的过程，目标是使簇内的观测彼此相似，而簇间的观测彼此相异。

##### 10.2.1 PCA 与聚类的区别

*   **问题**: PCA 和聚类有什么不同？
*   **详细讲解**:
    *   **目标不同**: PCA 的目标是为数据找到一个最优的**低维表示**，它关注的是整个数据集的结构。聚类的目标是**发现**数据中的**分组**或**子群**。
    *   **过程不同**: PCA 通过线性变换找到新的坐标轴。聚类通过特定的算法（如K-均值、层次聚类）将每个数据点分配到一个簇中。
    *   **应用场景**: PCA 常用于数据可视化和作为其他学习方法的预处理步骤。聚类本身就是一种探索性数据分析，用于市场细分、图像分割等。

##### 10.2.2 K-均值聚类 (K-Means Clustering)

*   **算法详解**:
    1.  **初始化**: 随机从数据点中选择 K 个作为初始的簇中心点 (centroids)。
    2.  **迭代分配与更新**:
        *   **(a) 分配步**: 对每一个数据点，计算它到 K 个簇中心的距离（通常是欧氏距离），并将其分配给距离最近的那个簇。
        *   **(b) 更新步**: 在所有点都分配完毕后，重新计算每个簇的中心点，新的中心点是该簇内所有数据点的**均值**。
    3.  **终止**: 重复步骤 (a) 和 (b)，直到簇的分配不再发生变化，算法收敛。
*   **注意事项**:
    *   K-均值的结果可能陷入**局部最优**，因为它对初始中心点的选择很敏感。实践中通常会多次随机初始化，然后选择总簇内平方和 (WCSS) 最小的那次结果。
    *   需要**预先指定**簇的数量 K。

##### 10.2.3 层次聚类 (Hierarchical Clustering)

*   **谱系图 (Dendrogram) 的解读**
    *   **高度**: 谱系图中，两个分支合并点的高度代表了这两个被合并的簇在合并时的**相异度 (dissimilarity)**。高度越高，表示合并的两个簇之间的差异越大。
    *   **切割**: 通过在谱系图的某个高度水平切割，可以得到特定数量的簇。切割线穿过的垂直线条数就是最终的簇数。

*   **连接方法 (Linkage) 对比**

| 连接方法 | 定义簇间距离 | 特点 |
| :--- | :--- | :--- |
| **最短距离法 (Single)** | 两簇间所有点对距离的**最小值** | 倾向于产生“链式”效果，善于发现非球形簇，但对异常值敏感。 |
| **最长距离法 (Complete)** | 两簇间所有点对距离的**最大**值 | 倾向于产生紧凑、大小相近的球形簇，对异常值较不敏感。 |
| **类平均法 (Average)** | 两簇间所有点对距离的**平均**值 | 是前两者的折中，较为稳健。 |
| **重心法 (Centroid)** | 两簇**中心点**之间的距离 | 算法简单，但可能出现**倒置现象**。 |

*   **倒置现象 (Inversion) 详解**
    *   **问题**: 什么是倒置现象，为什么会发生？
    *   **详细讲解**: 倒置现象是指在谱系图中，某次合并的高度**低于**其子分支的合并高度。这使得谱系图的解释变得困难。
    *   **原因**: 这种现象几乎只在**重心法**中发生。假设簇 A 和簇 B 合并成簇 AB，其中心为 $C_{AB}$。然后，簇 C 可能与簇 D 合并，其中心为 $C_{CD}$。在下一步，如果簇 AB 和簇 C 合并，新中心 $C_{ABC}$ 可能会比 $C_{CD}$ 更靠近簇 D。在计算簇 (ABC) 和簇 D 的距离时，可能会小于簇 C 和簇 D 的距离，从而导致倒置。

##### 10.2.4 聚类的实践问题

*   **问题**: 如何选择合适的相异度指标？
*   **详细讲解**:
    1.  **欧氏距离**: 这是最常用的度量，它衡量数据点在 p 维空间中的**绝对几何距离**。它假设所有维度都是同等重要的。
    2.  **基于相关性的距离 (Correlation-based Distance)**:
        *   **定义**: 通常定义为 `1 - a`，其中 `a` 是两个观测向量之间的皮尔逊相关系数。
        *   **适用场景**: 当我们更关心观测的**模式或形状**，而不是它们的绝对大小时。例如，在基因表达数据中，我们可能认为两个基因的表达模式（随时间上升或下降的趋势）相似比它们的绝对表达量相似更重要。如果两个顾客的购买清单（评分向量）高度正相关，即使一个顾客总是给高分，另一个总是给低分，他们也可能属于同一类偏好群体。
        *   **与欧氏距离的对比**: 两个形状完全相同但基线不同的时间序列，它们的欧氏距离可能很大，但相关性距离会很小。
    3.  **标准化**: 无论使用哪种距离度量，如果变量的尺度不同，都**强烈建议先进行标准化**，以避免距离计算被尺度大的变量主导。

***
## 2020年真题

### 一、单项选择题

**原题 1.1**
数组 A 有 10 行 10 列，下面哪一个 R 命令可以删除数组 A 的最后 3 列？
A: `A[-c(1:3),]`
B: `A[,-c(1:3)]`
C: `A[,-1:3]`
D: `A[,1:7]`

**答案 1.1**
**D: `A[,1:7]`**

**解题思路:**
*   在 R 中，对矩阵或数据框进行索引的格式是 `[行, 列]`。
*   要删除列，我们需要对列索引进行操作。逗号前的部分留空表示选择所有行。
*   A 共有 10 列。删除最后 3 列（第 8, 9, 10 列）等价于保留前 7 列（第 1 到 7 列）。
*   `A[,1:7]` 的语法正是选择所有行和第 1 到 7 列。
*   **选项分析:**
    *   A: `A[-c(1:3),]` 是删除第 1 到 3 **行**。
    *   B: `A[,-c(1:3)]` 是删除第 1 到 3 **列**。
    *   C: `A[,-1:3]` 语法错误，应使用 `c()` 函数包裹序列。

---

**原题 1.2**
哪位统计学家提出了线性判别分析？
A: 高斯
B: 费舍尔
C: 南丁格尔
D: 贝叶斯

**答案 1.2**
**B: 费舍尔**

**解题思路:**
这是一个统计学史实问题。线性判别分析（Linear Discriminant Analysis, LDA）是由罗纳德·费舍尔（Ronald Fisher）在1936年提出的，也常被称为费舍尔判别分析。

---

**原题 1.3**
下面哪个模型**不是**在标准线性模型基础上改进的线性模型？
A: 岭回归
B: 偏最小二乘
C: 支持向量机
D: 主成分分析

**答案 1.3**
**C: 支持向量机**

**解题思路:**
*   标准线性模型的目标是最小化残差平方和 (RSS)。
*   **A (岭回归)**: 在最小二乘的目标函数上增加了 L2 正则化惩罚项，是对线性模型的改进。
*   **B (偏最小二乘, PLS)**: 是一种利用 Y 信息的降维回归方法，其基础仍然是线性模型。
*   **D (主成分分析, PCA)**: 主成分回归 (PCR) 是先用 PCA 对自变量降维，然后进行线性回归，是线性模型的扩展应用。
*   **C (支持向量机, SVM)**: 其基本思想是寻找最大间隔超平面来划分数据，目标函数是最大化间隔（或最小化 hinge loss 和正则项），这与最小二乘法的原理有本质区别。

---

**原题 1.4**
岭回归的算法中，参数 lambda 从 0 开始增加的话，训练集 RSS 如何变化？
A: 最初减小，然后开始增加，图像是呈现一个 U 形
B: 稳定增长
C: 稳定减少
D: 保持不变

**答案 1.4**
**B: 稳定增长**

**解题思路:**
*   岭回归的目标函数是 `RSS + λ * (系数平方和)`。
*   当 `λ = 0` 时，岭回归等价于普通最小二乘法，此时在训练集上的 RSS 达到最小值。
*   当 `λ` 从 0 开始增加，模型对系数的惩罚变大，系数估计值会被“压缩”并偏离使其 RSS 最小的最小二乘解。
*   因此，随着 `λ` 的增加，模型对训练数据的拟合程度会下降，导致训练集的 RSS 单调（稳定）增长。

---

**原题 1.5**
基尼系数 G 定义如下：
$$ G = \sum_{k=1}^{K} p_k(1-p_k) $$
其中 `pk` 代表样本中第 k 类所占比例。含义是：样本集中随机抽取两个样本，其类别标记不一致的概率。所以，在决定如何分叉时，利用基尼系数衡量样本的什么？
A: 准确度
B: 自由度
C: RSS
D: 纯度

**答案 1.5**
**D: 纯度**

**解题思路:**
基尼系数（Gini Index）是衡量数据不确定性或不纯度（impurity）的指标。
*   如果一个节点中的所有样本都属于同一类别（即完全纯净），那么某个 `pk=1`，其余为 0，此时 G = 0。
*   如果样本均匀分布在各个类别中（即最不纯），基尼系数达到最大值。
*   决策树在分裂时，会选择一个切分点，使得分裂后子节点的基尼系数之和（通常是加权和）最小，也就是使得子节点的纯度最高。

---

**原题 1.6**
K 均值聚类的思想是，一个好的聚类算法应该使______。
A: 类内差异尽可能小
B: 类间差异尽可能小
C: 类的数量尽可能多
D: 以上都不对

**答案 1.6**
**A: 类内差异尽可能小**

**解题思路:**
K-均值聚类的优化目标是最小化所有簇的**类内平方和 (Within-Cluster Sum of Squares, WCSS)**。这意味着它试图让同一个簇内的数据点尽可能地紧密，即类内差异尽可能小。同时，一个好的聚类也应该使类间差异尽可能大，但这不是 K-均值算法直接优化的目标。

---

**原题 1.7**
在估计线性回归系数时，通常利用最小二乘估计使残差平方和 RSS 最小，其中 RSS 的计算方法是：
A: Σ(Yᵢ - Ȳ)²
B: Σ(Yᵢ - Ŷᵢ)²
C: Σ(Ŷᵢ - Ȳ)²
D: Σ(Yᵢ - β₁)²

**答案 1.7**
**B: Σ(Yᵢ - Ŷᵢ)²**

**解题思路:**
RSS (Residual Sum of Squares) 定义为**实际观测值**与**模型预测值**之差的平方和。
*   `Yᵢ` 是第 i 个观测的实际值。
*   `Ŷᵢ` 是第 i 个观测的模型预测值。
*   `Ȳ` 是所有观测 Y 的均值。
*   `Σ(Yᵢ - Ŷᵢ)²` 正是残差平方和的定义。
*   `Σ(Yᵢ - Ȳ)²` 是总平方和 (TSS)。
*   `Σ(Ŷᵢ - Ȳ)²` 是回归平方和 (ESS)。

---

**原题 1.8**
多元线性回归模型中，Y = β₀ + β₁X₁ + β₂X₂ + ... + βₚXₚ + ε。其中 βⱼ 可解释为：
A: 在所有其他预测变量动态变化时，Xⱼ 增加一个单位对 Y 产生的平均效果
B: 在所有其他预测变量动态变化时，Xⱼ 增加一个单位对 Y 产生的总体效果
C: 在所有其他预测变量保持不变时，Xⱼ 增加一个单位对 Y 产生的平均效果
D: 在所有其他预测变量保持不变时，Xⱼ 增加一个单位对 Y 产生的总体效果

**答案 1.8**
**C: 在所有其他预测变量保持不变时，Xⱼ 增加一个单位对 Y 产生的平均效果**

**解题思路:**
多元回归系数 `βⱼ` 的标准解释是**边际效应**。它衡量的是在控制（即保持）所有其他自变量不变的情况下，自变量 `Xⱼ` 每变化一个单位，因变量 Y 的期望（平均）变化量。

---

**原题 1.9**
在利用 ROC 曲线评价分类模型时，ROC 曲线下面的面积取值越大，下面哪一个说法正确？
A: 分类器效果越好
B: 分类器效果越差
C: 分类器效果相同
D: 该面积不适用于分类模型的评价

**答案 1.9**
**A: 分类器效果越好**

**解题思路:**
ROC 曲线下的面积被称为 AUC (Area Under the Curve)。
*   AUC 的取值范围在 0 到 1 之间。
*   AUC = 1 代表一个完美的分类器。
*   AUC = 0.5 代表一个无预测能力的分类器（相当于随机猜测）。
*   AUC 越接近 1，表示分类器在所有可能阈值下的综合性能越好，即区分正负样本的能力越强。

---

**原题 1.10**
在进行主成分分析时，通常需要对预测变量进行哪一项操作？
A: 正则化
B: 离散化
C: 标准化
D: 矢量化

**答案 1.10**
**C: 标准化**

**解题思路:**
主成分分析 (PCA) 是通过寻找数据方差最大的方向来进行降维。如果各个预测变量的量纲（尺度）不同，那么方差较大的变量（通常是数值本身较大的变量）将在主成分中占据主导地位，这会掩盖其他变量的真实贡献。**标准化**（即使得每个变量的均值为 0，标准差为 1）可以消除量纲的影响，使得每个变量在分析中处于同等地位。

---

**原题 1.11**
现有购物平台上，不同购物者的购买历史数据，网站希望基于相似购物者的购买历史为每个购物者优先展示他们最可能感兴趣的商品，以下哪种统计学习方法适合此需求？
A: 简单线性回归
B: 逻辑斯蒂回归
C: 线性判别分析
D: 聚类分析

**答案 1.11**
**D: 聚类分析**

**解题思路:**
这个场景描述的是一种协同过滤推荐系统。其核心思想是“物以类聚，人以群分”。
1.  首先需要根据用户的购买历史将他们划分为不同的群体或“簇”。这就是**聚类分析**的任务。
2.  然后，可以向某个用户推荐他所在簇的其他用户喜欢但他还没买过的商品。
其他选项都不适用：A、B、C 都是有监督学习方法，需要一个明确的因变量，而这里的任务是发现用户群体这个内在结构，是无监督的。

---

**原题 1.12**
考虑下图中软间隔分类器的分割超平面（实线，其左侧为 x 分类，右侧为 o 分类），请检查有几个支持向量？
（图中，虚线为间隔边界，实线为超平面。有2个x点和1个o点落在虚线边界上。）
A: 1
B: 2
C: 3
D: 4

**答案 1.12**
**C: 3**

**解题思路:**
支持向量是指那些对决定超平面位置起到关键作用的数据点。在软间隔分类器中，支持向量包括：
1.  **恰好在间隔边界上**的点。
2.  **在间隔内部**的点。
3.  **被错误分类**的点。
根据图示，有 2 个 'x' 类别和 1 个 'o' 类别的数据点恰好落在间隔边界（虚线）上。因此，共有 2 + 1 = 3 个支持向量。

---

**原题 1.13**
哪个方法**不可以**减少预测变量的个数？
A: 向后逐步选择法
B: LASSO
C: 岭回归
D: PCA

**答案 1.13**
**C: 岭回归**

**解题思路:**
*   **A (向后逐步选择法)**: 是一种直接的特征选择方法，通过移除变量来减少变量个数。
*   **B (LASSO)**: L1 正则化方法，可以将某些变量的系数精确地压缩到 0，从而实现变量选择，减少了变量个数。
*   **D (PCA)**: 主成分分析是一种降维方法。它通过创建少数几个新的、能代表大部分原始信息的主成分来替代原始的大量变量，从而减少了变量个数。
*   **C (岭回归)**: L2 正则化方法，它只会将变量的系数向 0 压缩，使其变得很小，但**不会**使其精确地等于 0。因此，它保留了所有原始变量，没有减少变量个数。

---

**原题 1.14**
与传统的线性回归、逻辑回归、LDA 等方法比较，哪一项**不是**决策树的优点？
A: 解释性强，可画图
B: 更接近人的决策模式
C: 处理定性预测变量无需创建哑变量
D: 预测准确性更高

**答案 1.14**
**D: 预测准确性更高**

**解题思路:**
决策树的主要优点在于其**高度的可解释性**。
*   **A, B, C** 都是决策树的公认优点。它可以被直观地可视化，其分层决策的逻辑类似人类思考过程，并且能自然地处理分类型变量。
*   **D**: 单个决策树的预测准确性通常**不高**，因为它容易过拟合，且对数据的微小变动很敏感（方差大）。相比之下，线性模型在数据关系确实是线性时可能更准确。虽然通过 Bagging、随机森林、Boosting 等集成方法可以极大提升树模型的准确性，但单个决策树本身并不以高预测精度著称。

---

**原题 1.15**
在 Default 数据集上，结合 balance, income, student 三个预测变量建立预测 Default (是否违约) 的多元逻辑斯蒂回归模型，其中学生身份用一个哑变量 student[Yes] 编码，1 代表学生，0 代表非学生。结果如下：
| 系数 | 估计值 | 标准误 | z 统计量 | p 值 |
| :--- | :--- | :--- | :--- | :--- |
| 截距 | -10.8690 | 0.4923 | -22.08 | <0.0001 |
| balance | 0.0057 | 0.0002 | 24.74 | <0.0001 |
| income | 0.0030 | 0.0082 | 0.37 | 0.7115 |
| student[Yes] | -0.6468 | 0.2362 | -2.74 | 0.0062 |
下列哪句话是**错误**的？
A: balance 和 student 对 default 概率是有影响的
B: 学生比非学生更不易违约
C: 其它值固定时，balance 每增加一个单位，default 的对数发生比增加 0.0057 个单位
D: income 对 default 概率有显著影响

**答案 1.15**
**D: income 对 default 概率有显著影响**

**解题思路:**
在假设检验中，我们通过 p 值来判断一个变量是否显著。通常，如果 p < 0.05，我们认为该变量是统计显著的。
*   **balance** 的 p 值 < 0.0001，非常小，说明它对违约概率有显著影响。
*   **student[Yes]** 的 p 值 = 0.0062，小于 0.05，说明学生身份对违约概率有显著影响。
*   **income** 的 p 值 = 0.7115，非常大，远大于 0.05，说明没有足够的证据表明收入对违约概率有显著影响。
*   **分析选项:**
    *   A: 正确，balance 和 student 的 p 值都显著。
    *   B: student[Yes] 的系数是 -0.6468 (负数)，意味着在其他变量不变时，学生身份会降低违约的对数发生比，即学生更不易违约。正确。
    *   C: 这是对逻辑回归系数的标准解释。正确。
    *   D: 错误，因为 income 的 p 值非常大，表明其影响不显著。

---

### 二、多项选择题

**原题 2.1**
下面哪些是非参数的方法？
A: 线性回归模型
B: 自然样条曲线
C: 逻辑斯蒂回归
D: 决策树

**答案 2.1**
**B, D**

**解题思路:**
*   **参数方法**预先假设了函数 f 的具体形式（如线性）。**非参数方法**不对函数形式做假设，模型结构由数据驱动。
*   A (线性回归) 和 C (逻辑斯蒂回归) 都假设了响应变量与自变量之间存在线性关系（对逻辑回归来说是对数发生比的线性关系），是典型的参数方法。
*   B (自然样条曲线) 和 D (决策树) 不对函数形式做预设，它们的模型复杂度和形状完全由数据决定，是典型的非参数方法。

---

**原题 2.2**
下面哪些方法增加了模型的光滑度？
A: 增加新的预测变量
B: 引入高阶的多项式表达
C: LASSO 模型中，增大多数参数 lambda 的值
D: 样条曲线中，增加结点(knot)个数

**答案 2.2**
**A, B, D**

**解题思路:**
增加模型的光滑度（也称灵活性或复杂度）意味着让模型有能力拟合更复杂的、更“弯曲”的函数关系。
*   **A (增加新变量)**: 给了模型更多的维度去拟合数据，增加了模型的复杂度。
*   **B (引入高阶项)**: 允许模型拟合非线性曲线，增加了光滑度。
*   **D (增加结点数)**: 允许样条在更多地方改变其多项式形式，从而能拟合更复杂的局部变化，增加了光滑度。
*   **C (增大 LASSO 的 lambda)**: 增大 `λ` 会**增加**对系数的惩罚，使得更多系数变为 0，从而**降低**了模型的复杂度/光滑度，使模型更简单。

---

**原题 2.3**
下列哪种情况下光滑度高的模型表现会更好？
A: 样本量 n 非常大，预测变量 p 很小
B: 样本量 n 很小，而预测变量 p 很大
C: 误差项的方差 σ² = var(ε) 极其大
D: 预测变量和响应变量之间的关系是非线性的

**答案 2.3**
**A, D**

**解题思路:**
光滑度高的模型（低偏差，高方差）适合在有足够数据支撑的情况下捕捉复杂关系。
*   **A**: 样本量 n 很大，为估计复杂的非参数模型提供了充足的信息，可以有效降低高方差模型过拟合的风险。p 很小也避免了维度灾难。
*   **D**: 当真实关系是非线性时，简单的线性模型（光滑度低）会有很高的偏差。光滑度高的模型能够更好地拟合这种非线性关系，从而降低偏差。
*   **B**: n 小 p 大是典型的“维度灾难”场景，光滑度高的模型极易过拟合，此时需要光滑度低的方法（如岭回归或 LASSO）。
*   **C**: 误差项方差大意味着数据噪声多，光滑度高的模型会去拟合这些噪声，导致过拟合。

---

**原题 2.4**
以下哪些可以作为系统聚类法中的距离度量方法？
A: 最长距离法
B: 最短距离法
C: 类平均法
D: 重心法

**答案 2.4**
**A, B, C, D**

**解题思路:**
系统（层次）聚类在合并簇时需要定义“簇与簇之间的距离”。题中列出的四种方法都是标准的簇间距离度量方法（也称为连接方法, Linkage Method）。
*   A (Complete Linkage)
*   B (Single Linkage)
*   C (Average Linkage)
*   D (Centroid Linkage)

---

**原题 2.5**
关于 k 折交叉验证，下列说法正确的是？
A: k 值并非越大越好，k 值过大，会降低运算速度
B: 选择更大的 k 值，会使偏差更小，因为 k 值越大，训练集越接近整个训练样本
C: 选择合适的 k 值，能减小方差
D: 我们一般选择 k 为 3 的倍数

**答案 2.5**
**A, B, C**

**解题思路:**
*   **A**: 正确。k 值越大，需要训练的模型次数就越多（k 次），运算成本越高。
*   **B**: 正确。k 值增大，每次用于训练的数据集就越大（(k-1)/k * n），越接近于整个数据集。用更多数据训练的模型，其偏差通常会更小。当 k=n 时（LOOCV），偏差达到最小。
*   **C**: 正确。k 折交叉验证相比于简单的验证集方法，通过多次划分和平均，有效地降低了测试误差估计的方差（不稳定性）。选择合适的 k 是在偏差和方差之间做权衡。
*   **D**: 错误。k 的选择没有“3 的倍数”这个规则。实践中最常用的是 k=5 或 k=10。

---

**原题 2.6**
对于分类问题，线性回归的回归方法不适用的原因是：
A: 通常不能将一个定性的响应变量自然地转化为两个水平以上的定量变量的模型
B: 响应变量的估值可能会在 [0, 1] 范围外，预测结果很难当作概率来解释
C: 无法使用最小二乘建立线性回归模型
D: 无法对多个预测变量进行同时建模

**答案 2.6**
**A, B** (根据手写答案) / **A, B, D** (更全面的理解)

**解题思路:**
*   **A**: 正确。如果分类超过两类（如A, B, C），如何用数值编码（如1, 2, 3）会引入不存在的序数关系，影响模型结果。
*   **B**: 正确。线性回归的预测值范围是 (-∞, +∞)，而分类问题我们希望得到的是概率，应该在 [0, 1] 区间内。
*   **C**: 错误。技术上，我们可以对 0/1 编码的响应变量使用最小二乘法，只是得到的模型解释性和效果都很差。
*   **D**: 错误。线性回归本身就可以对多个预测变量建模（即多元线性回归）。

**注意**: 手写答案为 AB。这可能是因为 D 选项的表述可能被理解为“线性回归不能处理多变量”，这是错误的。但如果从多分类的角度看，线性回归对多变量的同时建模会受到编码的影响，所以从这个角度看它不适用。但 A 和 B 是最主要、最直接的原因。

---

### 三、判断题

**原题 3.1**
通过自助法采样得到的袋外 (Out-Of-Bag) 观测可以作为测试集。

**答案 3.1**
**正确 (√)**

**解题思路:**
在 Bagging 和随机森林中，每个自助样本大约包含 2/3 的原始数据。那些没有被抽到用于训练某棵树的样本（约 1/3）被称为该树的**袋外 (OOB) 样本**。这些 OOB 样本可以被用来评估这棵树的性能，而将所有树的 OOB 预测汇总起来，可以得到一个对整个集成模型测试误差的无偏估计。因此，OOB 样本起到了类似测试集的作用。

---

**原题 3.2**
与多项式回归不同，样条函数不使用较高次幂来获得光滑的拟合效果，而是通过增加结点个数来使结果变得光滑。

**答案 3.2**
**正确 (√)**

**解题思路:**
这个说法的核心思想是正确的。多项式回归通过提高全局多项式的次数（高次幂）来增加模型的灵活性。而回归样条是在不同区间使用低阶多项式（如三次多项式），并通过增加**结点 (knots)** 的数量来提高模型的灵活性和光滑度。它通过拼接多个简单的函数来构建复杂的函数，而不是用一个全局的复杂函数。

---

**原题 3.3**
线性回归在拟合定性预测变量时，构造哑变量的编码方式是唯一的。

**答案 3.3**
**错误 (×)**

**解题思路:**
编码方式不唯一。对于一个有 k 个水平的定性变量，我们通常创建 k-1 个哑变量。选择哪个水平作为基准（参照）水平是任意的，不同的选择会得到不同的哑变量编码方案。虽然最终模型的拟合值和整体显著性不变，但各个系数的估计值和解释会依赖于所选的基准水平。

---

**原题 3.4**
线性判别分析模型的结果由先验概率和密度函数共同决定。

**答案 3.4**
**正确 (√)**

**解题思路:**
LDA 是基于贝叶斯定理来计算后验概率 $P(Y=k|X=x)$ 的。根据贝叶斯公式：
$$ P(Y=k|X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^{K} \pi_l f_l(x)} $$
其中：
*   $\pi_k$ 是类别 k 的**先验概率**。
*   $f_k(x)$ 是类别 k 的**类条件概率密度函数**（LDA 假设其为高斯分布）。
分类决策是选择使后验概率最大的类别 k。因此，结果确实由先验概率和密度函数共同决定。

---

### 四、简答题

**原题 4.1**
请描述使用最小二乘法的最优子集选择的算法，并指出其优缺点。

**答案 4.1**
**算法描述:**
最优子集选择旨在从 p 个预测变量中，为每个可能的子集大小 k (从 0 到 p)，都找到一个能使 RSS 最小（或 R² 最大）的模型。
1.  **初始化**: 拟合一个不含任何预测变量的零模型 M₀，该模型只包含截距，其预测值为所有样本的均值。
2.  **迭代搜索**: 对 `k = 1, 2, ..., p`:
    *   遍历所有包含 k 个预测变量的 $C_p^k$ 个可能模型。
    *   对每个模型，使用最小二乘法进行拟合，并计算其 RSS（或 R²）。
    *   在所有 $C_p^k$ 个模型中，选择 RSS 最小的那个作为该大小下的最优模型，记为 Mₖ。
3.  **最终选择**: 从得到的 p+1 个最优模型 (M₀, M₁, ..., Mₚ) 中，使用交叉验证错误率、Cp、AIC、BIC 或调整 R² 等标准，选出一个最终的全局最优模型。

**优缺点:**
*   **优点**:
    *   **全局最优**: 理论上，该方法能够保证在所有可能的变量组合中找到给定大小 k 下的全局最优模型。
*   **缺点**:
    *   **计算量巨大**: 当预测变量数量 p 较大时，需要评估的模型总数 ($2^p$) 会呈指数级增长，导致计算上不可行。例如，当 p=40 时，就需要评估超过一万亿个模型。
    *   **容易过拟合**: 由于搜索空间巨大，模型很容易拟合到训练数据中的噪声，导致在测试集上表现不佳，方差较高。

---

**原题 4.2**
请列出本课程中三个带有惩罚因子的模型及其公式。

**答案 4.2**
1.  **岭回归 (Ridge Regression)**
    *   **公式**:
        $$ \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\} $$
    *   **惩罚因子**: $\lambda \sum_{j=1}^{p} \beta_j^2$ (L2 范数惩罚)，惩罚的是回归系数的平方和。

2.  **Lasso 回归**
    *   **公式**:
        $$ \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\} $$
    *   **惩罚因子**: $\lambda \sum_{j=1}^{p} |\beta_j|$ (L1 范数惩罚)，惩罚的是回归系数的绝对值之和。

3.  **光滑样条 (Smoothing Spline)**
    *   **公式**:
        $$ \text{minimize} \left\{ \sum_{i=1}^{n} (y_i - f(x_i))^2 + \lambda \int (f''(x))^2 dx \right\} $$
    *   **惩罚因子**: $\lambda \int (f''(x))^2 dx$，惩罚的是函数 f(x) 的整体弯曲度或粗糙度（通过二阶导数的积分来衡量）。

---

**原题 4.3**
请画出如下样本分区图对应的决策树。矩形内的数字表示各区域中样本响应值的均值。
(图示：X轴为 Year(工作年数)，Y轴为 Pr(项目数)。第一个分割在 Year=2 处。Year<2 的区域均值为 3。Year>=2 的区域，又在 Pr=2 处分割。Year>=2 且 Pr<2 的区域均值为 10；Year>=2 且 Pr>=2 的区域均值为 15。)

**答案 4.3**

```
          +-----------------+
          |   Year < 2 ?    |
          +-----------------+
                /     \
               /       \
             是 /         \ 否
             /           \
        +-------+     +-----------------+
        |   3   |     |    Pr < 2 ?     |
        +-------+     +-----------------+
                            /     \
                           /       \
                         是 /         \ 否
                         /           \
                    +-------+     +-------+
                    |  10   |     |  15   |
                    +-------+     +-------+
```

**文字描述:**
1.  根节点的分裂规则是 **Year < 2**。
2.  如果回答“是”，则进入左侧的叶子节点，该节点的预测值为 **3**。
3.  如果回答“否”，则进入右侧的中间节点，该节点的分裂规则是 **Pr < 2**。
4.  对于右侧的中间节点，如果回答“是”，则进入其左侧的叶子节点，预测值为 **10**。
5.  如果回答“否”，则进入其右侧的叶子节点，预测值为 **15**。

---

### 五、综合应用题

**原题 5.1**
(基于 R 代码输出进行分析)

```R
# ... (加载数据，划分训练集)
8  > glm.fit = glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial, subset=train)
# ... (预测和混淆矩阵)
   glm.pred   Down  Up
     Down       77  34
     Up         92  49
# ...
13 > glm.fit = glm(Direction~Lag1+Lag2, family=binomial, data=Smarket, subset=train)
# ... (预测和混淆矩阵)
   glm.pred   Down  Up
     Down       35  35
     Up         76 106
# ...
18 > lda.fit = lda(Direction~Lag1+Lag2, data=Smarket, subset=train)
19 > qda.fit = qda(Direction~Lag1+Lag2, data=Smarket, subset=train)
```

**(1) 代码中第 8、18、19 行分别使用哪种模型进行数据拟合？**
**(2) 请根据混淆矩阵计算第 8 行代码所对应模型在测试集上的正确率（正确预测的比例）。**
**(3) 对比第 8 行和第 13 行的模型的正确率，后者用了少量的预测变量却得到了更高的预测正确率，其原因可能是什么？**
**(4) 如果该预测的贝叶斯决策边界是线性的，那么在训练集上第 18 行和第 19 行所对应的模型，通常来说，哪一个模型拟合效果会更好？在测试集上呢？**

**答案 5.1**
**(1) 模型识别:**
*   **第 8 行**: `glm(..., family=binomial)` 是**逻辑斯蒂回归**模型。
*   **第 18 行**: `lda(...)` 是**线性判别分析 (LDA)** 模型。
*   **第 19 行**: `qda(...)` 是**二次判别分析 (QDA)** 模型。

**(2) 正确率计算:**
根据第 8 行模型输出的混淆矩阵：
*   正确预测的样本数 = TP + TN = 49 (Up-Up) + 77 (Down-Down) = 126
*   总样本数 = 77 + 34 + 92 + 49 = 252
*   正确率 (Accuracy) = 正确预测数 / 总样本数 = 126 / 252 = **0.5 (或 50%)**

**(3) 准确率提高原因:**
第 13 行的模型 (只用 `Lag1` 和 `Lag2`) 在测试集上的正确率是 `(35+106)/(35+35+76+106) = 141/252 ≈ 0.56`，高于第 8 行模型的 0.5。原因可能包括：
1.  **降低过拟合**: 第 8 行的模型包含了 6 个预测变量，可能过于复杂，拟合了训练数据中的噪声。减少变量数量可以简化模型，提高其在测试集上的泛化能力。
2.  **移除了不相关的预测变量**: 变量 `Lag3`, `Lag4`, `Lag5`, `Volume` 可能与 `Direction` 关系不大（即它们的真实系数接近于 0）。将这些噪声变量包含在模型中会增加模型的方差，从而降低预测性能。
3.  **多重共线性**: 多个 `Lag` 变量之间可能存在相关性。去除一些相关的变量可以使模型更稳定。

**(4) LDA vs. QDA 效果比较:**
*   **背景**: LDA 假设决策边界是线性的，而 QDA 允许决策边界是二次的，因此 QDA 更灵活。
*   **在训练集上**: QDA 模型更灵活，它能够拟合更复杂的模式。因此，在训练集上，**QDA 的拟合效果通常会更好**（或至少不比 LDA 差）。
*   **在测试集上**: 题目假设真实的贝叶斯决策边界是**线性**的。在这种情况下，LDA 模型的假设与真实情况完全相符，而 QDA 的额外灵活性可能会导致它过拟合训练数据中的噪声。因此，在测试集上，**LDA 模型的表现通常会更好**，因为它有更低的方差。

---

**原题 5.2**
(基于 Carseats 数据集和决策树的 R 代码分析)

```R
# ①
train = sample(1:dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train,]
Carseats.test = Carseats[-train,]
# ②
library(tree)
tree.carseats = tree(Sales~., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats); text(tree.carseats, pretty=0)
# ③
pred.carseats = predict(tree.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.carseats)^2)
## [1] 4.149
# ④
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
# ⑤
pruned.carseats = prune.tree(tree.carseats, best = 9)
plot(pruned.carseats); text(pruned.carseats, pretty=0)
pred.pruned = predict(pruned.carseats, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
## [1] 3.xxx (手写答案推测)
```

**(1) 请简述代码块①-⑤分别在做什么。**
**(2) 解释为什么剪枝后的模型在测试集上的 MSE 与剪枝前不同。**

**答案 5.2**
**(1) 代码块功能解释:**
*   **代码块 ① (数据划分)**:
    *   `sample()`: 从 Carseats 数据集的行索引中随机抽取一半作为训练集的索引。
    *   `Carseats.train`, `Carseats.test`: 将数据集 `Carseats` 按照抽取的索引划分为训练集和测试集。

*   **代码块 ② (构建和可视化决策树)**:
    *   `library(tree)`: 加载 `tree` 程序包。
    *   `tree(Sales~., ...)`: 使用训练集 `Carseats.train` 构建一棵回归树，其中 `Sales` 是响应变量，`~.` 表示使用所有其他变量作为预测变量。
    *   `summary()` 和 `plot()`/`text()`: 显示树的摘要信息（如使用的变量、终端节点数）并对树进行可视化。

*   **代码块 ③ (评估未剪枝的树)**:
    *   `predict()`: 使用在训练集上构建的树 `tree.carseats` 对测试集 `Carseats.test` 进行预测。
    *   `mean(...)`: 计算模型在测试集上的均方误差 (MSE)，结果为 4.149。

*   **代码块 ④ (交叉验证选择剪枝参数)**:
    *   `cv.tree()`: 对未剪枝的树进行交叉验证，以评估不同大小（通过代价复杂性参数 `k` 控制）的子树的性能。`FUN = prune.tree` 指定剪枝方法。
    *   `plot()`: 绘制两张图。第一张图展示了树的大小（终端节点数）与交叉验证误差（`dev`）的关系；第二张图展示了代价复杂性参数 `k` 与交叉验证误差的关系。这些图用于帮助选择最佳的子树大小。

*   **代码块 ⑤ (剪枝并评估剪枝后的树)**:
    *   `prune.tree(..., best = 9)`: 根据交叉验证的结果，将原始树剪枝为一个包含 9 个终端节点的最优子树。这个 `best=9` 是通过观察代码块④中的图得到的，即在终端节点数为 9 时交叉验证误差最小。
    *   `plot()`/`text()`: 可视化剪枝后的新树。
    *   `predict()` 和 `mean()`: 使用剪枝后的树 `pruned.carseats` 对测试集进行预测，并计算其在测试集上的 MSE。

**(2) MSE 变化原因:**
剪枝后的模型在测试集上的 MSE (手写答案推测为 3.xxx) 小于剪枝前的 MSE (4.149)。这是因为：
*   **剪枝旨在解决过拟合问题**。未剪枝的大树（代码块②中构建的）可能过于复杂，它不仅学习了数据中真实的信号，还学习了训练集特有的**噪声**。这导致它在训练集上表现很好，但在新的、未见过的测试集上泛化能力差，表现为较高的测试 MSE。
*   **剪枝通过简化模型来提高泛化能力**。代价复杂性剪枝移除了那些对降低训练误差贡献很小但会增加模型复杂度的分支。这使得模型变得更简单、更稳健，减少了对训练集噪声的拟合。
*   **结果是偏差-方差权衡的体现**。剪枝增加了模型的**偏差**（因为它不再完美拟合训练数据），但显著降低了模型的**方差**（因为它对不同数据集的波动不那么敏感）。当方差的降低程度超过偏差的增加程度时，总体的测试误差（MSE）就会下降。

***

### 模擬試題

#### 一、單項選擇題 (共30分，每題2分)

**原題 1.1**
在R中，有一個名為`my_matrix`的10x10矩陣。哪個命令可以選取第2、5、8行以及所有列？
A: `my_matrix[c(2,5,8),]`
B: `my_matrix[,c(2,5,8)]`
C: `my_matrix[2,5,8,]`
D: `my_matrix[-c(2,5,8),]`

**答案 1.1**
**A: `my_matrix[c(2,5,8),]`**

**解題思路:**
*   R中矩陣索引的格式是 `[行索引, 列索引]`。
*   要選取特定的多行，需要將行號組合在一個向量中，使用 `c()` 函數。
*   列索引留空表示選擇所有列。
*   因此，`my_matrix[c(2,5,8),]` 是正確的語法。
*   **選項分析:**
    *   B: 這是選取第2、5、8**列**。
    *   C: 語法錯誤，多個索引必須用 `c()` 組合。
    *   D: 這是**排除**第2、5、8行。

---

**原題 1.2**
提升法 (Boosting) 算法，特別是梯度提升機 (Gradient Boosting Machine)，其早期關鍵思想和算法主要由哪位學者貢獻？
A: Leo Breiman
B: Jerome Friedman
C: Vladimir Vapnik
D: Ronald Fisher

**答案 1.2**
**B: Jerome Friedman**

**解題思路:**
這是一個關於統計學習發展史的問題。
*   Jerome Friedman 是梯度提升機 (GBM) 的主要提出者之一。
*   Leo Breiman 對 Bagging 和隨機森林有開創性貢獻。
*   Vladimir Vapnik 是支持向量機 (SVM) 的核心創始人之一。
*   Ronald Fisher 提出了線性判別分析 (LDA)。

---

**原題 1.3**
下列哪種方法在其標準形式下，內置了自動進行特徵選擇的功能？
A: 嶺回歸
B: 最小二乘回歸
C: Lasso回歸
D: 主成分回歸

**答案 1.3**
**C: Lasso回歸**

**解題思路:**
*   特徵選擇意味著將某些變量的影響完全剔除，即將其係數變為嚴格的0。
*   **Lasso回歸**使用L1正則化，其懲罰項 `λΣ|βⱼ|` 的一個關鍵特性就是能夠在λ足夠大時，將某些係數精確地壓縮到0，從而實現特徵選擇。
*   **嶺回歸**使用L2正則化，只會將係數向0收縮，但不會使其變為嚴格的0。
*   **最小二乘回歸**和**主成分回歸**在其標準形式下都不會自動將任何變量的係數置為0。

---

**原題 1.4**
在Lasso回歸中，當正則化參數 lambda 從一個很大的值開始減小時，模型中非零係數的個數會如何變化？
A: 穩定減少
B: 保持不變
C: 先增加後減少
D: 穩定增加

**答案 1.4**
**D: 穩定增加**

**解題思路:**
*   當 lambda 非常大時，Lasso的懲罰極強，會將所有係數都壓縮到0，模型中非零係數個數為0。
*   當 lambda 從大到小減小時，懲罰力度減弱，模型會逐漸允許一些重要的變量進入，其係數會從0變為非零值。
*   這個過程是單調的，lambda越小，進入模型的變量越多，非零係數的個數也越多。當lambda減小到0時，Lasso等價於最小二乘，所有係數通常都非零。

---

**原題 1.5**
在分類決策樹的生長過程中，交叉熵 (Cross-Entropy) 被用作分裂標準。一個節點的交叉熵值越小，表示該節點：
A: 樣本數量越少
B: 樣本數量越多
C: 數據越“不純” (各類別混合越均勻)
D: 數據越“純” (樣本主要來自單一類別)

**答案 1.5**
**D: 數據越“純” (樣本主要來自單一類別)**

**解題思路:**
交叉熵和基尼指數一樣，都是衡量節點不純度 (impurity) 的指標。
*   **純節點**: 如果一個節點中所有樣本都屬於同一個類別，其純度最高，此時交叉熵為0。
*   **不純節點**: 如果一個節點中各類別的樣本數量均勻分佈，其純度最低，此時交叉熵達到最大值。
*   決策樹在分裂時，目標是選擇一個分裂點，使得分裂後產生的子節點的總體交叉熵（加權和）最小，即子節點的總體純度最高。

---

**原題 1.6**
關於層次聚類 (Hierarchical Clustering) 的譜系圖 (Dendrogram)，下列說法**錯誤**的是？
A: 譜系圖的葉子節點代表單個觀測。
B: 兩個分支合併點的高度代表這兩個簇被合併時的相異度。
C: 在任何高度水平切割譜系圖，得到的簇的數量是唯一的。
D: 最長距離法 (Complete Linkage) 傾向於產生大小更均勻的簇。

**答案 1.6**
**B: 兩個分支合併點的高度代表這兩個簇被合併時的相異度。**

**解題思路:**
*   A, B, D 都是關於譜系圖和層次聚類的正確描述。譜系圖直觀地展示了凝聚過程，分支合併的高度是關鍵信息，而不同的連接方法（如最長距離法）確實會影響最終簇的形狀和大小。
*   C 說法錯誤。雖然在一個固定的高度切割，簇的數量是唯一的，但我們可以**在不同的高度進行切割**，從而得到不同數量的簇。如何選擇切割高度（即如何選擇簇的數量）是層次聚類中的一個關鍵決策，通常需要結合實際問題或一些啟發式規則來決定。

---

**原題 1.7**
在線性回歸中，殘差標準誤 (RSE) 的計算公式是 $\sqrt{RSS/(n-p-1)}$。RSE的單位是什麼？
A: 沒有單位
B: 與預測變量X的單位相同
C: 與響應變量Y的單位相同
D: 單位是Y的單位的平方

**答案 1.7**
**C: 與響應變量Y的單位相同**

**解題思路:**
*   RSS是殘差的平方和，其單位是Y的單位的平方。
*   RSE是對RSS取平均再開方得到的，因此其單位與Y的單位相同。
*   RSE衡量的是模型預測值偏離真實值的典型大小，所以它的大小和單位都應該能與響應變量直接比較。例如，如果Y是房價（單位：萬元），那麼RSE的單位也是萬元。

---

**原題 1.8**
在一個包含交互項的線性回歸模型 $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 X_2) + \epsilon$ 中，如何解釋 $X_1$ 對 Y 的影響？
A: $X_1$ 每增加一個單位，Y 平均增加 $\beta_1$。
B: $X_1$ 每增加一個單位，Y 平均增加 $\beta_1 + \beta_3$。
C: $X_1$ 對 Y 的影響是恆定的。
D: $X_1$ 每增加一個單位，Y 平均增加 $\beta_1 + \beta_3 X_2$。

**答案 1.8**
**D: $X_1$ 每增加一個單位，Y 平均增加 $\beta_1 + \beta_3 X_2$。**

**解題思路:**
*   在有交互項的模型中，一個變量的影響不再是孤立的，而是依賴於與之交互的另一個變量的值。
*   為了看 $X_1$ 的影響，我們對模型關於 $X_1$ 求偏導：$\frac{\partial Y}{\partial X_1} = \beta_1 + \beta_3 X_2$。
*   這意味著 $X_1$ 每增加一個單位，Y的平均變化量是 $\beta_1 + \beta_3 X_2$，這個變化量本身是 $X_2$ 的一個函數。

---

**原題 1.9**
在一個二分類問題中，如果我們非常關心“不能漏掉任何一個真正的病人”（即假陰性FN的代價極高），我們應該優先考慮哪個評估指標？
A: 準確率 (Accuracy)
B: 精確度 (Precision)
C: 召回率 (Recall) / 靈敏度 (Sensitivity)
D: 特異度 (Specificity)

**答案 1.9**
**C: 召回率 (Recall) / 靈敏度 (Sensitivity)**

**解題思路:**
*   召回率的定義是 `TP / (TP + FN)`，即在所有真正的病人中，我們成功識別出了多少比例。
*   要使假陰性FN盡可能小，等價於最大化召回率。
*   **選項分析**:
    *   A (準確率): 在樣本不均衡時（如病人很少）可能是個誤導性指標。
    *   B (精確度): 關心的是“被診斷為有病的人中，真正有病的比例”，它更關心假陽性FP。
    *   D (特異度): 關心的是“健康人被正確識別為健康的比例”。

---

**原題 1.10**
對於K-最近鄰 (KNN) 算法，如果預測變量的尺度差異很大，通常需要進行什麼預處理操作？
A: 移除離群點
B: 變量標準化
C: 增加樣本量
D: 減少K值

**答案 1.10**
**B: 變量標準化**

**解題思路:**
*   KNN算法完全基於觀測點之間的距離（如歐氏距離）來進行預測。
*   如果預測變量的尺度差異很大（例如，一個變量範圍是0-1，另一個是0-10000），那麼距離的計算將會被尺度大的變量完全主導，尺度小的變量的影響會被忽略。
*   **變量標準化**（將所有變量轉換到相同的尺度，如均值為0，標準差為1）可以確保每個變量在距離計算中都有同等的權重。

---

**原題 1.11**
在建立信用評分卡的場景中，目標是預測客戶是否會違約（是/否）。這是一個典型的：
A: 回歸問題
B: 聚類問題
C: 分類問題
D: 降維問題

**答案 1.11**
**C: 分類問題**

**解題思路:**
*   該問題的響應變量是“是否違約”，這是一個定性（離散）的變量，只有兩個類別。
*   預測一個定性響應變量的問題，根據定義，就是一個分類問題。

---

**原題 1.12**
在支持向量分類器 (SVC) 中，增大成本參數 C (Cost) 通常會導致什麼結果？
A: 間隔變寬，允許更多的點違反間隔
B: 偏差增大，方差減小
C: 訓練集上的誤分類點可能減少，決策邊界更曲折
D: 模型變得更簡單

**答案 1.12**
**C: 訓練集上的誤分類點可能減少，決策邊界更曲折**

**解題思路:**
*   參數 C 是對違反間隔的點的懲罰成本。
*   **增大 C** 意味著對誤分類的容忍度降低，模型會盡力將訓練樣本正確分類。
*   為了做到這一點，決策邊界需要變得更靈活、更曲折，以適應訓練數據。這會導致間隔變窄。
*   這種行為對應於**偏差減小，方差增大**，模型變得更複雜，更容易過擬合。
*   **選項分析**:
    *   A: 這是減小C的結果。
    *   B: 這是減小C的結果。
    *   D: 模型變得更複雜，不是更簡單。

---

**原題 1.13**
下列哪種方法是一種無監督學習的降維方法？
A: 線性判別分析 (LDA)
B: Lasso回歸
C: 向前逐步選擇
D: 主成分分析 (PCA)

**答案 1.13**
**D: 主成分分析 (PCA)**

**解題思路:**
*   **無監督學習**意味著方法在學習過程中不使用響應變量Y。
*   **PCA** 在尋找主成分（最大方差方向）時，只用到了預測變量X的數據，與Y無關，因此是無監督的。
*   **LDA** 是一種有監督的降維方法，它尋找的方向是能最好地區分類別的方向，需要用到Y的類別信息。
*   **Lasso** 和 **向前逐步選擇** 都是有監督的特徵選擇方法，它們的選擇標準（如RSS）都依賴於Y。

---

**原題 1.14**
關於提升法 (Boosting) 的描述，下列哪項是**錯誤**的？
A: Boosting 是一種順序學習的方法，每棵樹都基於之前的殘差進行擬合。
B: 較小的學習率 (shrinkage) 通常需要更多的樹 (n.trees) 來達到好的性能。
C: Boosting 不容易過擬合，樹的數量越多越好。
D: Boosting 組合的是一系列“弱學習器”（通常是小樹）。

**答案 1.14**
**C: Boosting 不容易過擬合，樹的數量越多越好。**

**解題思路:**
*   A, B, D 都是對 Boosting 的正確描述。它順序地擬合殘差，組合弱學習器，並且學習率和樹的數量是需要權衡的關鍵參數。
*   C 是錯誤的。與 Bagging 和隨機森林不同，Boosting **會過擬合**。因為它不斷地擬合殘差，如果樹的數量B過大，模型就會開始擬合訓練數據中的噪聲，導致在測試集上性能下降。因此，B是一個需要通過交叉驗證來仔細選擇的調節參數。

---

**原題 1.15**
在一個多元線性回歸的輸出中，某個變量的係數為正且其p值小於0.001。這意味著：
A: 該變量與響應變量之間存在顯著的正相關關係。
B: 在模型中包含其他變量的情況下，該變量與響應變量之間存在顯著的正向關係。
C: 該變量是模型中最重要的預測變量。
D: 移除該變量會使模型的R²顯著下降。

**答案 1.15**
**B: 在模型中包含其他變量的情況下，該變量與響應變量之間存在顯著的正向關係。**

**解題思路:**
*   多元回歸中的係數和p值解釋的是該變量在**控制了模型中所有其他變量之後**的**邊際效應**。
*   A: 這是簡單線性回歸的結論。在多元回歸中，一個變量可能與Y單獨正相關，但在控制其他變量後變為負相關（混淆效應）。
*   C: p值小只代表關係顯著，不代表其重要性（影響大小）最大。一個係數絕對值更大的變量可能更重要。
*   D: R²總會隨著變量移除而下降（或不變），調整R²才可能上升。顯著性也不直接等同於對R²的影響大小。
*   B 是最準確的描述，強調了這是在一個**多元模型**的背景下的結論。

---

### 二、多項選擇題 (共10分，每題2分)

**原題 2.1**
下列哪些模型或方法能夠有效地處理非線性關係？
A: 帶有二次項的線性回歸
B: K-最近鄰 (KNN)
C: 帶有徑向基核函數 (RBF Kernel) 的支持向量機
D: 決策樹

**答案 2.1**
**A, B, C, D**

**解題思路:**
*   A: 顯式地引入非線性項（如二次項、三次項）是處理非線性的直接方法。
*   B: KNN是一種非參數方法，其決策邊界可以高度非線性，以適應數據的局部結構。
*   C: RBF核SVM通過核技巧將數據映射到高維空間，能夠學習到非常複雜的非線性決策邊界。
*   D: 決策樹通過對預測變量空間的遞歸劃分，可以逼近任意形狀的非線性函數。

---

**原題 2.2**
下列哪些操作會**降低**模型的靈活性（或增加正則化）？
A: 在嶺回歸中，增大調節參數 lambda 的值。
B: 在K-最近鄰中，減小 K 的值。
C: 在決策樹剪枝中，增大成本複雜度參數 α 的值。
D: 在支持向量機中，減小成本參數 C 的值。

**答案 2.2**
**A, C, D**

**解題思路:**
降低靈活性意味著使模型更簡單，偏差更大，方差更小。
*   A: 增大 lambda 會增強對係數的懲罰，使係數更接近0，模型變得更簡單。
*   B: 減小 K 值會使KNN的決策邊界更曲折，模型更複雜，靈活性**增加**。
*   C: 增大 α 會增強對樹複雜度（葉節點數）的懲罰，導致更多的剪枝，模型變得更簡單。
*   D: 減小 C 值意味著降低對誤分類的懲罰，模型會容忍更多的訓練錯誤以換取一個更寬、更平滑的間隔，模型變得更簡單。

---

**原題 2.3**
關於留一交叉驗證 (LOOCV)，下列說法正確的是？
A: 它是K折交叉驗證在K=n時的特例。
B: 相比於K=5或10的交叉驗證，LOOCV對測試誤差的估計有更小的偏差。
C: 相比於K=5或10的交叉驗證，LOOCV對測試誤差的估計有更小的方差。
D: 對於任何模型，LOOCV的計算成本都非常高。

**答案 2.3**
**A, B**

**解題思路:**
*   A: 正確，這是LOOCV的定義。
*   B: 正確，LOOCV每次使用n-1個樣本訓練，最接近使用全部n個樣本，因此其對測試誤差的估計偏差非常小。
*   C: 錯誤，LOOCV的n個訓練集高度相似，導致n個誤差估計值高度相關，對它們求平均並不能有效降低方差。因此，LOOCV的方差通常比K折CV要**大**。
*   D: 錯誤，對於普通最小二乘線性回歸和多項式回歸，存在一個計算捷徑，使得LOOCV的計算成本約等於只擬合一次模型，並不高。

---

**原題 2.4**
關於支持向量機 (SVM) 的核函數，下列說法正確的是？
A: 核技巧的目的是在低維空間中直接計算高維空間中的內積，以避免顯式的高維映射。
B: 線性核函數適用於數據本身就近似線性可分的場景。
C: 多項式核的階數d越大，決策邊界越平滑。
D: 徑向基核函數 (RBF) 的參數γ越大，決策邊界越曲折，模型越容易過擬合。

**答案 2.4**
**A, B, D**

**解題思路:**
*   A: 正確，這是核技巧的核心思想。
*   B: 正確，線性核 $K(x_i, x_{i'}) = x_i^T x_{i'}$ 等價於在原始空間中尋找線性決策邊界的支持向量分類器。
*   C: 錯誤，多項式核的階數d越大，模型越靈活，能夠擬合更複雜的非線性關係，決策邊界會變得更**曲折**，而不是更平滑。
*   D: 正確，γ控制RBF核的影響範圍。γ越大，影響範圍越小（越局部），模型為了適應局部數據會變得非常靈活，決策邊界也因此更曲折，更容易過擬合。

---

**原題 2.5**
關於集成學習方法，下列說法正確的是？
A: Bagging和隨機森林都是通過並行構建多棵樹來降低方差。
B: Boosting通過順序構建多棵樹來逐步降低模型的偏差。
C: 隨機森林相對於Bagging的主要改進是引入了特徵的隨機抽樣。
D: Bagging中使用的決策樹通常是經過深度剪枝的“弱學習器”。

**答案 2.5**
**A, B, C**

**解題思路:**
*   A: 正確，Bagging和隨機森林都屬於並行集成方法，通過對多個獨立（或去相關）模型的預測進行平均/投票來降低方差。
*   B: 正確，Boosting順序地擬合前一輪的殘差，每一步都在修正錯誤，其主要作用是降低偏差。
*   C: 正確，這是隨機森林的核心創新，通過在每次分裂時隨機選擇特徵子集，來降低樹之間你的相關性。
*   D: 錯誤，Bagging中使用的決策樹通常是**未剪枝**的、完全生長的“強學習器”，以保證單個模型的低偏差，然後通過平均來降低其高方差。Boosting才使用“弱學習器”。

---

### 三、判斷題 (共8分，每題2分)

**原題 3.1**
在多元線性回歸中，如果模型的調整R²很高，那麼所有預測變量的p值一定都很小。

**答案 3.1**
**錯誤 (×)**

**解題思路:**
調整R²衡量的是模型整體的解釋能力。一個高的調整R²說明模型整體上對響應變量有很好的解釋力。但是，這可能是由模型中一部分非常強的預測變量驅動的，而模型中可能同時包含了一些與響應無關、p值很大的“噪聲”變量。共線性也可能導致某些重要變量的p值變大。

---

**原題 3.2**
Lasso回歸在任何情況下都優於嶺回歸，因為它能進行變量選擇。

**答案 3.2**
**錯誤 (×)**

**解題思路:**
沒有一種模型在所有情況下都是最優的（“天下沒有免費的午餐”定理）。
*   當真實模型是稀疏的（即只有少數變量是重要的），Lasso通常表現更好。
*   當真實模型中大部分變量都有貢獻，且係數大小比較平均時，嶺回歸通常表現更好，因為Lasso可能會錯誤地將一些有用的變量剔除。

---

**原題 3.3**
主成分分析 (PCA) 找到的主成分是原始變量的線性組合，這些主成分一定能很好地預測一個外部的響應變量Y。

**答案 3.3**
**錯誤 (×)**

**解題思路:**
PCA是一種**無監督**方法。它在尋找主成分時，只考慮了預測變量X自身的方差結構，完全沒有使用任何關於響應變量Y的信息。因此，方差最大的方向（主成分）不一定就是與Y最相關的方向。有可能與Y最相關的信息存在於方差較小的主成分中，而被PCA忽略了。

---

**原題 3.4**
在提升法 (Boosting) 算法中，將學習率 (shrinkage) 設得非常大可以加快模型的收斂速度，從而得到更好的模型。

**答案 3.4**
**錯誤 (×)**

**解題思路:**
Boosting是一種“慢學習”算法。將學習率設得很大會導致每一步都過度修正殘差，模型會很快地在訓練數據上過擬合，導致泛化能力變差。通常，較小的學習率配合較多的樹數量，能讓模型更穩健地收斂到一個好的解。

---

### 四、簡答題 (共12分，每題4分)

**原題 4.1**
請簡述K-均值聚類 (K-Means Clustering) 的算法步驟。

**答案 4.1**
K-均值聚類旨在將n個觀測劃分到預先指定的K個簇中，其目標是最小化簇內平方和（即讓每個簇盡可能緊湊）。
**算法步驟如下：**
1.  **初始化**: 隨機選擇K個觀測點作為初始的K個簇的中心點（centroids）。
2.  **重複迭代直至收斂**:
    *   **(a) 分配步驟**: 對於每一個觀測點，計算它到K個簇中心的距離（通常使用歐氏距離），並將該觀測點分配給距離最近的那個簇。
    *   **(b) 更新步驟**: 在所有觀測點都完成分配後，重新計算每個簇的中心點。新的中心點是該簇內所有觀測點的均值向量。
3.  **終止**: 當一次完整的迭代（分配步+更新步）沒有導致任何一個觀測點的簇分配發生改變時，算法收斂，聚類完成。

---

**原題 4.2**
請詳細比較嶺回歸 (Ridge Regression) 和 Lasso 的異同點。

**答案 4.2**
嶺回歸和Lasso都是在標準最小二乘回歸基礎上改進的正則化方法，旨在解決過擬合和共線性問題。
*   **相同點**:
    1.  **目標**: 都是通過在最小二乘的損失函數上增加一個懲罰項來壓縮回歸係數，以降低模型方差。
    2.  **偏差-方差權衡**: 兩者都引入了一個調節參數λ，λ越大，懲罰越重，模型越簡單，偏差越大，方差越小。
    3.  **處理共線性**: 都能有效處理預測變量間的共線性問題。

*   **不同點**:
    1.  **懲罰項不同 (核心區別)**:
        *   **嶺回歸**: 使用**L2範數**懲罰，即 $\lambda \sum \beta_j^2$（係數的平方和）。
        *   **Lasso**: 使用**L1範數**懲罰，即 $\lambda \sum |\beta_j|$（係數的絕對值之和）。
    2.  **變量選擇功能**:
        *   **嶺回歸**: L2懲罰只會將係數向0收縮，但**不能**使其精確等於0。因此，最終模型仍然包含所有p個變量。
        *   **Lasso**: L1懲罰可以在λ足夠大時，將某些不重要的變量係數**精確地壓縮到0**，從而實現自動的變量選擇，產生稀疏模型。
    3.  **適用場景**:
        *   **嶺回歸**: 適用於大部分預測變量都對結果有貢獻，且係數大小比較平均的場景。
        *   **Lasso**: 適用於只有少數預測變量是重要的（即真實模型是稀疏的）場景。

---

**原題 4.3**
給定如下的二維空間劃分，請畫出其對應的決策樹。矩形內的數字是該區域的預測值。
(圖示：X軸為`rm`，Y軸為`lstat`。第一次分裂在`rm < 6.5`。對於`rm >= 6.5`的區域，第二次分裂在`lstat < 15`。`rm < 6.5`的區域預測值為15。`rm >= 6.5`且`lstat < 15`的區域預測值為30。`rm >= 6.5`且`lstat >= 15`的區域預測值為22。)

**答案 4.3**

```
          +-----------------+
          |    rm < 6.5 ?   |
          +-----------------+
                /     \
               /       \
             是 /         \ 否
             /           \
        +-------+     +------------------+
        |  15   |     |   lstat < 15 ?   |
        +-------+     +------------------+
                            /      \
                           /        \
                         是 /          \ 否
                         /            \
                    +-------+      +-------+
                    |  30   |      |  22   |
                    +-------+      +-------+
```

**文字描述:**
1.  根節點的分裂規則是 **rm < 6.5**。
2.  如果回答“是”，則進入左側的葉子節點，預測值為 **15**。
3.  如果回答“否”，則進入右側的內部節點，該節點的分裂規則是 **lstat < 15**。
4.  對於右側的內部節點，如果回答“是”，則進入其左側的葉子節點，預測值為 **30**。
5.  如果回答“否”，則進入其右側的葉子節點，預測值為 **22**。

---

### 五、綜合應用題 (共40分)

**原題 5.1 (20分)**
下面是一段使用 `Hitters` 數據集比較嶺回歸、Lasso和主成分回歸 (PCR) 預測棒球運動員薪水 (Salary) 的R代碼及部分輸出。

```R
# 1. 數據加載與預處理
library(ISLR)
library(glmnet)
library(pls)
data(Hitters)
Hitters <- na.omit(Hitters) # 移除缺失值
x <- model.matrix(Salary ~ ., Hitters)[,-1] # 創建預測變量矩陣
y <- Hitters$Salary # 創建響應向量

# 2. 數據集劃分
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

# 3. 嶺回歸
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid)
cv.out.ridge <- cv.glmnet(x[train,], y[train], alpha = 0)
bestlam.ridge <- cv.out.ridge$lambda.min
# bestlam.ridge 的值為 212.1
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[test,])
mse.ridge <- mean((ridge.pred - y.test)^2)
# mse.ridge 的值為 96996

# 4. Lasso 回歸
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
cv.out.lasso <- cv.glmnet(x[train,], y[train], alpha = 1)
bestlam.lasso <- cv.out.lasso$lambda.min
# bestlam.lasso 的值為 0.01 (為方便展示，實際值可能不同)
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test,])
mse.lasso <- mean((lasso.pred - y.test)^2)
# mse.lasso 的值為 101736
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam.lasso)
# lasso.coef 的輸出顯示19個係數中有7個為0

# 5. 主成分回歸 (PCR)
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
# CV 結果顯示最佳主成分數為 M=6
pcr.pred <- predict(pcr.fit, x[test,], ncomp = 6)
mse.pcr <- mean((pcr.pred - y.test)^2)
# mse.pcr 的值為 97413
```

**(1)** 請解釋在 `glmnet()` 函數中，`alpha = 0` 和 `alpha = 1` 分別代表什麼模型？並指出第5部分使用的是什麼模型。
**(2)** 根據代碼輸出，嶺回歸、Lasso和PCR三種方法中，哪一種在測試集上表現最好？
**(3)** Lasso模型的輸出 `lasso.coef` 顯示19個係數中有7個為0。這個現象說明了Lasso的什麼關鍵特性？為什麼它能產生這種現象？
**(4)** 假設你事先知道，數據集中大部分預測變量都與薪水有關。在這種情況下，嶺回歸和Lasso哪一個理論上可能表現更好？為什麼？

**答案 5.1**
**(1) 模型識別:**
*   在 `glmnet()` 函數中，`alpha` 參數控制彈性網絡正則化的混合比例。
    *   `alpha = 0` 表示純L2正則化，對應的是**嶺回歸 (Ridge Regression)**。
    *   `alpha = 1` 表示純L1正則化，對應的是**Lasso回歸**。
*   第5部分使用的是 `pcr()` 函數，代表**主成分回歸 (Principal Component Regression)**。

**(2) 模型表現比較:**
*   根據代碼中的計算結果：
    *   嶺回歸的測試MSE (`mse.ridge`) 為 96996。
    *   Lasso回歸的測試MSE (`mse.lasso`) 為 101736。
    *   主成分回歸的測試MSE (`mse.pcr`) 為 97413。
*   比較這三個測試MSE，值越小代表模型在測試集上的預測越準確。因此，**嶺回歸**的表現最好。

**(3) Lasso的特性及原因:**
*   **關鍵特性**: 這個現象說明了Lasso具有**自動進行變量選擇**並產生**稀疏模型**的關鍵特性。它能夠將一部分不重要的預測變量的係數精確地壓縮到0，從而將它們從模型中剔除。
*   **原因**: 這源於Lasso使用的**L1正則化**（懲罰項為係數絕對值之和）。從幾何角度看，L1正則化的約束區域是一個在坐標軸上有尖角的多面體（如二維時的菱形）。在優化過程中，損失函數的等高線很可能在這些尖角處與約束區域相切，而這些尖角恰好位於某個或某些係數為0的坐標軸上，從而導致稀疏解。

**(4) 理論模型選擇:**
*   **理論上嶺回歸可能表現更好**。
*   **原因**: Lasso的優勢在於處理“稀疏”問題，即當只有少數變量是真正重要的時候，它能有效地識別出這些變量並剔除噪聲。而嶺回歸的優勢在於，當大部分變量都對響應有貢獻時（即真實模型是“稠密”的），它會將所有變量的係數都向0進行一定程度的收縮，但保留所有變量在模型中。這種“溫和”的收縮方式在這種情況下能更好地降低整體方差，而不會因為錯誤地剔除了一些有用變量而引入偏差。

---

**原題 5.2 (20分)**
下面是一段使用 `Boston` 數據集比較Bagging、隨機森林和Boosting三種集成樹模型預測房價中位數 (`medv`) 的R代碼及部分輸出。

```R
# 1. 加載包和數據
library(MASS)
library(randomForest)
library(gbm)
data(Boston)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
test <- -train
Boston.test <- Boston[test, "medv"]

# 2. Bagging
# Bagging是隨機森林在mtry=p時的特例
# Boston數據集有13個預測變量
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE)
yhat.bag <- predict(bag.boston, newdata = Boston[test,])
mse.bag <- mean((yhat.bag - Boston.test)^2)
# mse.bag 的值為 13.47

# 3. 隨機森林
rf.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 4, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[test,])
mse.rf <- mean((yhat.rf - Boston.test)^2)
# mse.rf 的值為 11.48

# 4. Boosting
boost.boston <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)
# 為了選擇最佳的樹數量，我們可以在一個序列上進行預測
n.trees.seq <- seq(from = 100, to = 5000, by = 100)
predmat <- predict(boost.boston, newdata = Boston[test,], n.trees = n.trees.seq)
test.mse.boost <- apply((predmat - Boston.test)^2, 2, mean)
best.iter <- n.trees.seq[which.min(test.mse.boost)]
# best.iter 的值為 600
# 使用最佳迭代次數計算最終MSE
yhat.boost <- predict(boost.boston, newdata = Boston[test,], n.trees = best.iter)
mse.boost <- mean((yhat.boost - Boston.test)^2)
# mse.boost 的值為 11.35

# 5. 變量重要性
importance(rf.boston)
#              %IncMSE IncNodePurity
# lstat         32.99     1057.8
# rm            30.89     1020.5
# ... (其他變量)
summary(boost.boston)
#             var    rel.inf
# lstat     lstat      45.96
# rm           rm      31.22
# ... (其他變量)
```

**(1)** 在 `randomForest()` 函數中，Bagging 和隨機森林是如何通過 `mtry` 參數來區分的？請解釋代碼中 `mtry = 13` 和 `mtry = 4` 的設置依據。
**(2)** 根據三種模型的測試MSE，哪一個模型在本問題中表現最好？
**(3)** 請解釋Boosting模型中 `shrinkage` 和 `interaction.depth` 這兩個參數的含義。
**(4)** 根據 `importance()` 和 `summary()` 函數的輸出，哪兩個變量在這兩種模型中都被認為是最重要的？

**答案 5.2**
**(1) `mtry` 參數的區分與依據:**
*   **區分**: `mtry` 參數指定了在構建決策樹的**每一次分裂**時，可以隨機抽樣作為候選的預測變量的數量。
    *   **Bagging**: Bagging在每次分裂時會考慮**所有**可用的預測變量。因此，將 `mtry` 設置為預測變量的總數 `p`，即可實現Bagging。
    *   **隨機森林**: 隨機森林為了降低樹之間的相關性，在每次分裂時只考慮一個隨機的變量子集。因此，`mtry` 會被設置為一個小於 `p` 的值。
*   **設置依據**:
    *   `mtry = 13`: `Boston` 數據集共有13個預測變量（p=13）。將 `mtry` 設為13，意味著每次分裂都考慮全部13個變量，這正是Bagging的定義。
    *   `mtry = 4`: 這是隨機森林的設置。對於回歸問題，`mtry` 的一個常用經驗推薦值是 `p/3`。這裡 $13/3 \approx 4.33$，所以選擇4是一個合理的默認值。

**(2) 最佳模型:**
*   根據代碼輸出的測試集MSE：
    *   Bagging (`mse.bag`): 13.47
    *   隨機森林 (`mse.rf`): 11.48
    *   Boosting (`mse.boost`): 11.35
*   比較這三個值，**Boosting** 的測試MSE最低，因此在本問題中表現最好。

**(3) Boosting參數含義:**
*   **`shrinkage` (學習率)**: 這是一個很小的正數（如此處的0.01），用於控制Boosting算法的學習速度。在每一次迭代中，新生成的樹對最終模型的貢獻會被這個學習率“壓縮”。較小的`shrinkage`值意味著學習過程更慢、更穩健，通常需要更多的樹 (`n.trees`) 來達到最佳性能，但有助於防止過擬合。
*   **`interaction.depth` (交互深度)**: 這個參數限制了集成中每棵樹的最大深度。它控制了模型能夠捕捉的變量間的交互作用的階數。例如，`interaction.depth = 4` 意味著模型可以考慮最多4個變量之間的交互作用。較小的值（如1或2）會產生更簡單、可解釋性更好的模型，但可能會忽略高階交互。

**(4) 最重要的變量:**
*   根據 `randomForest` 的 `importance()` 輸出（基於 `%IncMSE` 或 `IncNodePurity` 指標）和 `gbm` 的 `summary()` 輸出（基於 `rel.inf` 相對影響力指標），兩種模型都顯示 **`lstat`** (低收入人群比例) 和 **`rm`** (每戶平均房間數) 是預測房價中位數 (`medv`) 最重要的兩個變量。